{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c8e3aa7",
   "metadata": {},
   "source": [
    "# `AML — Task 2:` Heart rhythm classification from raw ECG signals\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51a018a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For TQDM :\n",
    "#! python3.6 -m pip install ipywidgets\n",
    "#! python3.6 -m pip install --upgrade jupyter\n",
    "#! jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f126d6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import biosppy.signals.ecg as ecg\n",
    "import biosppy.signals.tools as tools\n",
    "import neurokit2 as nk\n",
    "from biosppy.plotting import plot_ecg\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6dc16cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_selection import SelectFpr, SelectFdr, SelectFwe, f_classif, chi2, mutual_info_classif\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.linear_model import RidgeClassifier, LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier,\\\n",
    "                             AdaBoostClassifier, VotingClassifier, ExtraTreesClassifier, IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from pywt import wavedec\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b29de4",
   "metadata": {},
   "source": [
    "---\n",
    "## Dataset import and export to `csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acdf2170",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_from_csv(extension=\"\", drop_id = True):\n",
    "    X_train = pd.read_csv('data/X_train' + extension + '.csv')\n",
    "    y_train = pd.read_csv('data/y_train' + extension + '.csv')\n",
    "    X_test  = pd.read_csv('data/X_test' + extension + '.csv')\n",
    "    \n",
    "    if drop_id:\n",
    "        X_train = X_train.drop(columns=['id'])\n",
    "        y_train = y_train.drop(columns=['id'])\n",
    "        X_test  = X_test.drop(columns=['id'])\n",
    "     \n",
    "    return X_train, y_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "910612cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_to_csv(X_train, y_train, X_test, extension=\"_cleaned\"):\n",
    "    X_train.to_csv('data/X_train' + extension + '.csv', index=False)\n",
    "    y_train.to_csv('data/y_train' + extension + '.csv', index=False)\n",
    "    X_test.to_csv('data/X_test' + extension + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a707ea",
   "metadata": {},
   "source": [
    "## Submission export to `csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3acf5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_submission(sub_id, pred, basepath='submissions/task2-sub'):\n",
    "    result = pred.copy().rename(columns={0: 'y'})\n",
    "    result['id'] = range(0, len(result))\n",
    "    result = result[['id', 'y']]\n",
    "    result.to_csv(basepath + str(sub_id) + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9dc4ed0",
   "metadata": {},
   "source": [
    "## Plot helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e61ab7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_raw_signal_sample(raw_signal):\n",
    "    # Some matplotlib setting \n",
    "    plt.rcParams[\"figure.figsize\"] = (30, 5)\n",
    "    plt.rcParams['lines.linewidth'] = 5\n",
    "    plt.rcParams['xtick.labelsize'] = 24\n",
    "    plt.rcParams['ytick.labelsize'] = 32\n",
    "    plt.rcParams['axes.labelsize'] = 48\n",
    "    plt.rcParams['axes.titlesize'] = 48\n",
    "\n",
    "    fig, axs = plt.subplots(1, 1)\n",
    "\n",
    "    seconds = np.arange(0, 600) / 30 \n",
    "    x_labels = [0, 5, 10, 15, 20]\n",
    "\n",
    "    ax = axs\n",
    "    \n",
    "    measurements = raw_signal.dropna().to_numpy(dtype='float32')\n",
    "    # Get a subsequence of a signal and downsample it for visualization purposes\n",
    "    measurements = measurements[1000:7000:10] \n",
    "    # convert volts to millivolts\n",
    "    measurements /= 1000\n",
    "    ax.plot(seconds, measurements, color='k')\n",
    "    ax.set_xticks(x_labels)\n",
    "\n",
    "    fig.tight_layout()\n",
    "\n",
    "    # Display x- and y-labels for the whole plot\n",
    "    ax = fig.add_subplot(111, frameon=False)\n",
    "    # hide tick and tick label of the big axes\n",
    "    plt.tick_params(labelcolor='none', top=False, bottom=False, left=False, right=False)\n",
    "    ax.yaxis.set_label_coords(-0.05, 0.5)\n",
    "    plt.xlabel('Time [s]')\n",
    "    plt.ylabel('Amplitude [mV]')            \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a838977",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_templates(templates):\n",
    "    pd.DataFrame(np.mean(templates, axis=0)).plot()\n",
    "    pd.DataFrame(templates.transpose()).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ffbae4",
   "metadata": {},
   "source": [
    "---\n",
    "## Data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809894cd",
   "metadata": {},
   "source": [
    "### Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fdd8ab55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_stats_from(features_series: list) -> pd.Series:\n",
    "    # pd.describe().drop(count) returns 7 interesting features (min, max, std...)\n",
    "    stats = pd.concat([pd.Series(s, dtype=np.float64).describe().drop([\"count\"]) for s in features_series], ignore_index=True)\n",
    "    stats = stats.append(pd.Series([pd.Series(s, dtype=np.float64).skew() for s in features_series]), ignore_index=True)\n",
    "    stats = stats.append(pd.Series([pd.Series(s, dtype=np.float64).kurtosis() for s in features_series]), ignore_index=True)\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b98a4871",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_qpeaks_features(filtered: np.array, rpeaks: np.array, window_size=50):\n",
    "    qpeaks = [rpeak - window_size + np.argmin(filtered[rpeak-window_size:rpeak]) for rpeak in rpeaks]\n",
    "    qpeaks_amplitudes = [filtered[qpeak] for qpeak in qpeaks]\n",
    "    return qpeaks, qpeaks_amplitudes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11deacd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_speaks_features(filtered: np.array, rpeaks: np.array, window_size=50):\n",
    "    speaks = [rpeak + np.argmin(filtered[rpeak:rpeak+window_size]) for rpeak in rpeaks]\n",
    "    speaks_amplitudes = [filtered[speak] for speak in speaks]\n",
    "    return speaks, speaks_amplitudes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c9f3587",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_euclidean_Rpeak_specific_points_features(templates):\n",
    "    beats = templates\n",
    "    \n",
    "    beat = np.mean(beats, axis=0) #Might be improved\n",
    "    rpeak = np.argmax(beat) #Might be improved\n",
    "    \n",
    "    key_points = np.zeros((4, 2))\n",
    "    results = np.full(4, np.nan)\n",
    "    \n",
    "    key_points[0, :] = max(enumerate(beat[0:40]), key=itemgetter(1))\n",
    "    key_points[1, :] = min(enumerate(beat[75:85]), key=itemgetter(1))\n",
    "    key_points[2, :] = min(enumerate(beat[95:105]), key=itemgetter(1))\n",
    "    key_points[3, :] = max(enumerate(beat[150:180]), key=itemgetter(1))\n",
    "    \n",
    "    key_points[1][0] += 75\n",
    "    key_points[2][0] += 95\n",
    "    key_points[3][0] += 150\n",
    "    \n",
    "    rvalue= beat[rpeak]\n",
    "    \n",
    "    max_x = max(np.append(key_points[:, 0], rpeak))\n",
    "    min_x = min(np.append(key_points[:, 0], rpeak))\n",
    "    max_y = max(np.append(key_points[:, 1], rvalue))\n",
    "    min_y = min(np.append(key_points[:, 1], rvalue))\n",
    "    \n",
    "    rpeak_normalized = (rpeak-min_x)/(max_x-min_x)\n",
    "    rvalue_normalized = (rvalue-min_y)/(max_y-min_y)\n",
    "    \n",
    "    for i in range(key_points.shape[0]):\n",
    "        x = key_points[i][0]\n",
    "        y = key_points[i][1]\n",
    "        x_normalized = (x-min_x)/(max_x-min_x)\n",
    "        y_normalized = (y-min_y)/(max_y-min_y)\n",
    "        \n",
    "        results[i] = np.linalg.norm([rpeak_normalized-x_normalized, rvalue_normalized-y_normalized])\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bbd7adb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ECG_features_index(no_nans, sampling_rate=300, method='dwt'):\n",
    "    cleaned_signal = nk.ecg_clean(no_nans, sampling_rate=sampling_rate)\n",
    "    rpeaks = nk.ecg_findpeaks(cleaned_signal, sampling_rate=sampling_rate)[\"ECG_R_Peaks\"]\n",
    "\n",
    "    PQRST_time_dic = {}\n",
    "    PQRST_time_dic[\"ECG_R_Peaks\"] = rpeaks\n",
    "\n",
    "    try:\n",
    "        _, waves = nk.ecg_delineate(cleaned_signal, rpeaks, sampling_rate=300, method=method, show=False)\n",
    "        PQRST_time_dic.update({column_name: pd.DataFrame(waves[column_name]).dropna().to_numpy(dtype=int).reshape(-1) for column_name in list(waves.keys())})\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(\"No PQST-peak could be found, problem with nk.ecg_delineate.\")         \n",
    "\n",
    "    return PQRST_time_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c53a8c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ECG_amplitudes(no_nans, ECG_feature_index_dic):\n",
    "    peaks_columns=[\"ECG_R_Peaks\", \"ECG_P_Peaks\", \"ECG_Q_Peaks\", \"ECG_S_Peaks\", \"ECG_T_Peaks\"]\n",
    "    amplitudes = []\n",
    "    for col_name in peaks_columns:\n",
    "        amplitudes.append([no_nans[index] for index in ECG_feature_index_dic.get(col_name, [])])\n",
    "    return amplitudes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1d8d7ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ECG_offset_onset_diff(ECG_feature_index_dic):\n",
    "    peaks_having_onsets = [\"P\", \"R\", \"T\"]\n",
    "\n",
    "    peaks_duration =  [[] for i in range(len(peaks_having_onsets)) ]\n",
    "    for i, p in enumerate(peaks_having_onsets):\n",
    "        onset_column_name = \"ECG_\" + p + \"_Onsets\"\n",
    "        offset_column_name = \"ECG_\" + p + \"_Offsets\"\n",
    "        peaks_duration[i] = [offset-onset for offset, onset in zip(ECG_feature_index_dic.get(offset_column_name, []), ECG_feature_index_dic.get(onset_column_name, []))]\n",
    "\n",
    "    return peaks_duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "814e3bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To be continued to extract all the other features such as indexes distances\n",
    "def extract_ECG_features(no_nans, sampling_rate=300, method='dwt'):\n",
    "    ECG_feature_index_dic = extract_ECG_features_index(no_nans, sampling_rate=sampling_rate, method=method)\n",
    "    amplitudes = extract_ECG_amplitudes(no_nans, ECG_feature_index_dic)\n",
    "    onset_offset_diff = extract_ECG_offset_onset_diff(ECG_feature_index_dic)\n",
    "    return amplitudes, onset_offset_diff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08f32d8",
   "metadata": {},
   "source": [
    "### Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4d47da7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(time_series: pd.Series, sampling_rate=300) -> pd.Series:\n",
    "    # Drop nan values in the time series\n",
    "    no_nans = time_series.dropna()\n",
    "    \n",
    "    # Extract main features from ECG\n",
    "    ts, filtered, rpeaks, _, templates, _, heart_rate = ecg.ecg(no_nans, sampling_rate, show=False)\n",
    "    assert len(rpeaks) > 1, 'ECG cannot have a single R peak'\n",
    "    assert len(templates) > 1, 'ECG cannot have a single heartbeat'\n",
    "    \n",
    "    # Extract Q,R,S peak features\n",
    "    rpeaks_amplitude = [filtered[rpeak] for rpeak in rpeaks]\n",
    "    qpeaks, qpeaks_amplitude = extract_qpeaks_features(filtered, rpeaks)\n",
    "    speaks, speaks_amplitude = extract_speaks_features(filtered, rpeaks)\n",
    "    \n",
    "    # Extract RR, QRS durations features\n",
    "    rr_durations = [r2 - r1 for r1, r2 in zip(rpeaks, rpeaks[1:])]\n",
    "    qrs_durations = [speak - qpeak for qpeak, speak in zip(qpeaks, speaks)]\n",
    "    \n",
    "    # Extract RR differences\n",
    "    rr_differences = [rr2 - rr1 for rr1, rr2 in zip(rr_durations, rr_durations[1:])]\n",
    "    \n",
    "    # Extract QRS direction features\n",
    "    qrs_direction = [q + r + s for q, r, s in zip(qpeaks, rpeaks, speaks)]\n",
    "    \n",
    "    # Extract Q/R ratio features\n",
    "    qr_ratio = [q / r for q, r in zip(qpeaks, rpeaks)]\n",
    "    \n",
    "    #TODO: Extract SNR ratio (http://www.cinc.org/archives/2011/pdf/0609.pdf)\n",
    "    snr = np.quantile(np.std(templates, axis=0), 0.35)\n",
    "    \n",
    "    # Use this to go from index differences to seconds\n",
    "    index_to_time = ts[-1] / len(filtered)\n",
    "    # Extract pNNx (https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1767394/)\n",
    "    pNNs = []\n",
    "    for i in [28, 50]:#np.arange(5, 55, 5):\n",
    "        pNNs.append((np.array(rr_durations) * index_to_time > i * 0.001).sum() / len(rr_durations))\n",
    "    pNNs = pd.Series(pNNs)\n",
    "    \n",
    "    # Extract log(RMSSD) (log of root mean square successive differences, linked to heart rate variability (HRV))\n",
    "    logRMSSD = np.log(np.sqrt(np.mean(np.array(rr_differences) ** 2)))\n",
    "    \n",
    "    # Extract power features\n",
    "    freqs, power = tools.power_spectrum(filtered, sampling_rate, decibel=True)\n",
    "    total_power = tools.band_power(freqs, power, [0, 0.4], decibel=True)[0]\n",
    "    vlf_power = tools.band_power(freqs, power, [0, 0.04], decibel=True)[0]\n",
    "    lf_power = tools.band_power(freqs, power, [0.04, 0.15], decibel=True)[0]\n",
    "    hf_power = tools.band_power(freqs, power, [0.15, 0.4], decibel=True)[0]\n",
    "    lfhf_ratio = lf_power / hf_power\n",
    "    lf_power_norm = lf_power / (total_power - vlf_power)\n",
    "    hf_power_norm = hf_power / (total_power - vlf_power)\n",
    "    \n",
    "    # Extract beats skew and kurtosis (https://paperswithcode.com/paper/heartbeat-classification-fusing-temporal-and)\n",
    "    skews = pd.DataFrame(templates).skew(axis=1)\n",
    "    kurtosiss = pd.DataFrame(templates).kurtosis(axis=1)\n",
    "    \n",
    "    #Extract ECG features from neurokit\n",
    "    ecg_amplitude, onset_offset_diff = extract_ECG_features(no_nans)\n",
    "    ppeaks_amplitudes = ecg_amplitude[1]\n",
    "    tpeaks_amplitudes = ecg_amplitude[4]\n",
    "    #ponset_offset_diff = onset_offset_diff[0]\n",
    "    #ronset_offset_diff = onset_offset_diff[1]\n",
    "    #tonset_offset_diff = onset_offset_diff[2]\n",
    "    \n",
    "    # Extract statistics\n",
    "    ecg_features = extract_stats_from([\n",
    "        rpeaks_amplitude, qpeaks_amplitude, speaks_amplitude,\n",
    "        ppeaks_amplitudes, tpeaks_amplitudes, \n",
    "        #ponset_offset_diff, ronset_offset_diff, tonset_offset_diff,\n",
    "        rr_durations, qrs_durations, rr_differences, qrs_direction, \n",
    "        qr_ratio, skews, kurtosiss\n",
    "    ])\n",
    "    \n",
    "    # Extract different standard deviations\n",
    "    sdsd = np.std(rr_differences) # could be optimized, already computed\n",
    "    sdnn = np.std(rr_durations) # could be optimized, already computed\n",
    "    sd1 = (1 / np.sqrt(2)) * sdsd\n",
    "    sd2 = 2 * sdnn**2 - sd1**2\n",
    "    sd1sd2ratio = sd1 / sd2\n",
    "    s = 3.14159 * sd1 * sd2\n",
    "    \n",
    "    \n",
    "    #Extract wavelets\n",
    "    coeffs = np.mean([wavedec(template, 'db1', level=3) for template in templates], axis=0, dtype=object)\n",
    "    wavelet = coeffs[0]\n",
    "    wavelet_features = pd.Series(wavelet)\n",
    "    \n",
    "\n",
    "    euclidean_dist_features = pd.Series(extract_euclidean_Rpeak_specific_points_features(templates))\n",
    "\n",
    "    \n",
    "    \n",
    "    return ecg_features.append(pd.Series([snr, \n",
    "                                          logRMSSD, \n",
    "                                          total_power, \n",
    "                                          vlf_power,\n",
    "                                          lf_power, \n",
    "                                          hf_power,\n",
    "                                          lfhf_ratio, \n",
    "                                          lf_power_norm, \n",
    "                                          hf_power_norm,\n",
    "                                          sd1,\n",
    "                                          sd2, \n",
    "                                          sd1sd2ratio, \n",
    "                                          s]), ignore_index=True).append(pNNs, ignore_index=True).append(wavelet_features, ignore_index=True).append(euclidean_dist_features, ignore_index=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b98812",
   "metadata": {},
   "source": [
    "---\n",
    "## Data standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7e31de0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_data(X_train, X_test):\n",
    "    scaler = StandardScaler().fit(X_train)\n",
    "    X_train_scaled = scaler.transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    return pd.DataFrame(X_train_scaled), pd.DataFrame(X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a616b1",
   "metadata": {},
   "source": [
    "---\n",
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6178196e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_constant_features(X_train, X_test, verbose=True):\n",
    "    non_constant_features_mask = X_train.apply(pd.Series.nunique) != 1\n",
    "    X_train_selected_features = X_train.loc[:, non_constant_features_mask]\n",
    "    X_test_selected_features = X_test.loc[:, non_constant_features_mask]\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"{X_train.shape[1]-X_train_selected_features.shape[1]} features removed because of constant values ({100*(X_train.shape[1]-X_train_selected_features.shape[1])/X_train.shape[1]:.2f}%).\")\n",
    "    \n",
    "    return X_train_selected_features, X_test_selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fd321209",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_too_correlated_features(X_train, X_test, threshold=0.98, verbose=True):\n",
    "    X_train_corr_ = X_train.corr()\n",
    "\n",
    "    X_train_too_correlated = (X_train_corr_.mask(\n",
    "        np.tril(np.ones([len(X_train_corr_)]*2, dtype=bool))).abs() > threshold).any()\n",
    "    \n",
    "    X_train_selected_features = X_train.loc[:, (~X_train_too_correlated)]\n",
    "    X_test_selected_features = X_test.loc[:, (~X_train_too_correlated)]\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"{X_train.shape[1]-X_train_selected_features.shape[1]} features removed because of correlation with another feature > {threshold} ({100*(X_train.shape[1]-X_train_selected_features.shape[1])/X_train.shape[1]:.2f}%).\")\n",
    "\n",
    "    return X_train_selected_features, X_test_selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fcae7ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_useless_features(X_train, y_train, X_test, selector_type, stat, verbose=1):\n",
    "    # See https://stats.stackexchange.com/questions/328358/fpr-fdr-and-fwe-for-feature-selection\n",
    "    # and https://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection\n",
    "    \n",
    "    assert selector_type in [\"fpr\", \"fdr\", \"fwe\"], \"Unrecognised selector type\"\n",
    "    assert stat in [f_classif, chi2, mutual_info_classif], \"Unrecognised stat\"\n",
    "    selector = None\n",
    "    if selector_type == \"fpr\":\n",
    "        selector = SelectFpr(stat)\n",
    "    elif selector_type == \"fdr\":\n",
    "        selector = SelectFdr(stat)\n",
    "    elif selector_type == \"fwe\":\n",
    "        selector = SelectFwe(stat)\n",
    "    \n",
    "    selector.fit(X_train, y_train)\n",
    "    X_train_selected_features = pd.DataFrame(selector.transform(X_train))\n",
    "    X_test_selected_features = pd.DataFrame(selector.transform(X_test))\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"{X_train.shape[1]-X_train_selected_features.shape[1]} features removed because of complicated p-value stuff we don't understand ({100*(X_train.shape[1]-X_train_selected_features.shape[1])/X_train.shape[1]:.2f}%).\")\n",
    "        \n",
    "    return X_train_selected_features, X_test_selected_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e834e645",
   "metadata": {},
   "source": [
    "---\n",
    "## Outlier detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b8a29d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers(X_train, y_train, contamination='auto', verbose=1, method=\"LocalOutlierFactor\"):\n",
    "    clf = None\n",
    "    if method == \"LocalOutlierFactor\":\n",
    "        clf = LocalOutlierFactor(contamination=contamination)\n",
    "    elif method == \"IsolationForest\":\n",
    "        clf = IsolationForest(contamination=contamination, random_state=0, verbose=verbose)\n",
    "    else:\n",
    "        raise AttributeError(f\"Unvalid argument for method, must be 'LocalOutlierFactor' or 'IsolationForest', not '{method}'\")\n",
    "        \n",
    "    outliers_mask = pd.Series(clf.fit_predict(X_train)).map({1:1, -1:0}) #Mask with 0 for outliers and 1 for non outliers\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Detected {(outliers_mask == 0).sum()} outliers with method {method}, out of {outliers_mask.shape[0]} samples ({100 * (outliers_mask == 0).sum() / outliers_mask.shape[0]:.2f}%).\")\n",
    "    \n",
    "    # Remove outliers from the training set\n",
    "    X_train = np.array(X_train)[outliers_mask == 1, :]\n",
    "    y_train = np.array(y_train)[outliers_mask == 1]\n",
    "    \n",
    "    return pd.DataFrame(X_train), pd.DataFrame(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfed9a4c",
   "metadata": {},
   "source": [
    "---\n",
    "## Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4891314a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(model, params, X_train, y_train, clear=False):\n",
    "    gs = GridSearchCV(model, params, cv=5, verbose=3, scoring='f1_micro', error_score='raise')\n",
    "    gs.fit(X_train, y_train)\n",
    "    \n",
    "    if clear:\n",
    "        clear_output(wait=True)\n",
    "    print(f\"{type(gs.best_estimator_).__name__} best validation score is {gs.best_score_:.5f} +- {gs.cv_results_['std_test_score'][gs.best_index_]:.5f},\\nobtained with {gs.best_params_}\")\n",
    "    \n",
    "    return gs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436814d8",
   "metadata": {},
   "source": [
    "---\n",
    "## Main Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358f9688",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2dbf8612",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_raw, y_train_raw, X_test_raw = load_from_csv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b44965",
   "metadata": {},
   "source": [
    "### Extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "080216a8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/loic/Enter/envs/InternetAnalytics/lib/python3.9/site-packages/numpy/core/_methods.py:163: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asanyarray(a)\n",
      "/home/loic/Enter/envs/InternetAnalytics/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3440: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/loic/Enter/envs/InternetAnalytics/lib/python3.9/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/home/loic/Enter/envs/InternetAnalytics/lib/python3.9/site-packages/neurokit2/signal/signal_period.py:60: NeuroKitWarning: Too few peaks detected to compute the rate. Returning empty vector.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No PQST-peak could be found, problem with nk.ecg_delineate.\n",
      "No PQST-peak could be found, problem with nk.ecg_delineate.\n",
      "No PQST-peak could be found, problem with nk.ecg_delineate.\n",
      "No PQST-peak could be found, problem with nk.ecg_delineate.\n",
      "No PQST-peak could be found, problem with nk.ecg_delineate.\n",
      "No PQST-peak could be found, problem with nk.ecg_delineate.\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train_raw.apply(extract_features, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e22045f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No PQST-peak could be found, problem with nk.ecg_delineate.\n",
      "No PQST-peak could be found, problem with nk.ecg_delineate.\n",
      "No PQST-peak could be found, problem with nk.ecg_delineate.\n"
     ]
    }
   ],
   "source": [
    "X_test = X_test_raw.apply(extract_features, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c5895c58",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train has 221 null values (0.00029%).\n",
      "X_test has 189 null values (0.00037%).\n"
     ]
    }
   ],
   "source": [
    "print(f\"X_train has {X_train.isna().sum().sum()} null values ({X_train.isna().sum().sum()/X_train.size:.5f}%).\")\n",
    "print(f\"X_test has {X_test.isna().sum().sum()} null values ({X_test.isna().sum().sum()/X_test.size:.5f}%).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ffc823",
   "metadata": {},
   "source": [
    "### Standardize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "49458a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_stand, X_test_stand = standardize_data(X_train, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7c7bf5",
   "metadata": {},
   "source": [
    "### Impute data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d263a775",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = KNNImputer(n_neighbors=6, weights='uniform').fit(X_train_stand)\n",
    "X_train_imp = pd.DataFrame(imputer.transform(X_train_stand))\n",
    "X_test_imp = pd.DataFrame(imputer.transform(X_test_stand))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a0c76fb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train has 0 null values.\n",
      "X_test has 0 null values.\n"
     ]
    }
   ],
   "source": [
    "print(f\"X_train has {X_train_imp.isna().sum().sum()} null values.\")\n",
    "print(f\"X_test has {X_test_imp.isna().sum().sum()} null values.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb48401e",
   "metadata": {},
   "source": [
    "### Select features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "60abe361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 features removed because of constant values (0.00%).\n"
     ]
    }
   ],
   "source": [
    "X_train_1, X_test_1 = remove_constant_features(X_train_imp, X_test_imp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "17548772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 features removed because of correlation with another feature > 0.98 (10.00%).\n"
     ]
    }
   ],
   "source": [
    "X_train_2, X_test_2 = remove_too_correlated_features(X_train_1, X_test_1, threshold=0.98)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "74ac5095",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 features removed because of complicated p-value stuff we don't understand (8.15%).\n"
     ]
    }
   ],
   "source": [
    "X_train_3, X_test_3 = remove_useless_features(X_train_2, np.array(y_train_raw).ravel(), X_test_2, selector_type=\"fpr\", stat=f_classif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2cb1f419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected 205 outliers with method LocalOutlierFactor, out of 5117 samples (4.01%).\n"
     ]
    }
   ],
   "source": [
    "X_train_4, y_train = remove_outliers(X_train_3, y_train_raw, method=\"LocalOutlierFactor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4c9e8af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(y_train).ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34218305",
   "metadata": {},
   "source": [
    "### Train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8aef3c96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC best validation score is 0.79296 +- 0.01245,\n",
      "obtained with {'C': 10, 'class_weight': None, 'kernel': 'rbf', 'random_state': 0}\n"
     ]
    }
   ],
   "source": [
    "gs_svc = grid_search(SVC(),\n",
    "                     {\n",
    "                         # Need to play with degree parameter for poly kernel\n",
    "                         \"kernel\": [\"rbf\", \"poly\"], #[\"rbf\", \"poly\", \"sigmoid\"],\n",
    "                         \"C\": [1, 10, 50, 100],\n",
    "                         \"class_weight\": [\"balanced\", None],\n",
    "                         \"random_state\": [0],\n",
    "                     },\n",
    "                     X_train_4,\n",
    "                     y_train,\n",
    "                     clear=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ff0ae041",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier best validation score is 0.83306 +- 0.01544,\n",
      "obtained with {'class_weight': None, 'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100, 'random_state': 0}\n"
     ]
    }
   ],
   "source": [
    "gs_random_forest = grid_search(RandomForestClassifier(),\n",
    "                               {\n",
    "                                   \"n_estimators\": [100], #np.arange(100, 300, 200),\n",
    "                                   \"max_depth\": [None], #np.arange(2, 8, 1),\n",
    "                                   \"min_samples_split\": [2, 4], #np.arange(2, 8, 1),\n",
    "                                   \"min_samples_leaf\": [1, 4], #np.arange(1, 9, 2),\n",
    "                                   \"class_weight\": [\"balanced\", None],\n",
    "                                   \"random_state\": [0], \n",
    "                               },\n",
    "                               X_train_4,\n",
    "                               y_train,\n",
    "                               clear=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4d4cce3d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoostingClassifier best validation score is 0.83306 +- 0.00931,\n",
      "obtained with {'criterion': 'mse', 'learning_rate': 0.1, 'loss': 'deviance', 'min_samples_leaf': 3, 'min_samples_split': 4, 'n_estimators': 200, 'n_iter_no_change': None, 'subsample': 0.7, 'tol': 0.0001}\n"
     ]
    }
   ],
   "source": [
    "gs_gbc = grid_search(GradientBoostingClassifier(),\n",
    "                     {\n",
    "                         \"loss\": [\"deviance\"],\n",
    "                         \"learning_rate\": [0.1], #[0.1, 1],\n",
    "                         \"n_estimators\": [200], #[50, 200],\n",
    "                         \"subsample\": [0.7], #[0.7, 1],\n",
    "                         \"criterion\": [\"mse\"], #[\"friedman_mse\", \"mse\"],\n",
    "                         \"min_samples_split\": [4], #[2, 4],\n",
    "                         \"min_samples_leaf\": [3], #[1, 3],\n",
    "                         \"n_iter_no_change\": [None],\n",
    "                         \"tol\": [1e-4],\n",
    "                     },\n",
    "                     X_train_4, \n",
    "                     y_train,\n",
    "                     clear=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "99dbe220",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier best validation score is 0.74552 +- 0.00664,\n",
      "obtained with {'n_neighbors': 7, 'weights': 'distance'}\n"
     ]
    }
   ],
   "source": [
    "# No predict_proba\n",
    "gs_knn = grid_search(KNeighborsClassifier(),\n",
    "                     {\n",
    "                         \"n_neighbors\": np.arange(2, 10, 1),\n",
    "                         \"weights\": [\"uniform\", \"distance\"],\n",
    "                     },\n",
    "                     X_train_4, \n",
    "                     y_train,\n",
    "                     clear=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c2f68e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaussianProcessClassifier best validation score is 0.71458 +- 0.00433,\n",
      "obtained with {'kernel': None, 'multi_class': 'one_vs_one', 'random_state': 0}\n"
     ]
    }
   ],
   "source": [
    "# Takes ~6min to train the 2*5 epochs, but scores of ~0.72\n",
    "gs_gp = grid_search(GaussianProcessClassifier(),\n",
    "                    {\n",
    "                        \"kernel\": [None],\n",
    "                        \"multi_class\": [\"one_vs_rest\", \"one_vs_one\"],\n",
    "                        \"random_state\": [0],\n",
    "                    },\n",
    "                    X_train_4, \n",
    "                    y_train,\n",
    "                    clear=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b160fba4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier best validation score is 0.73717 +- 0.01183,\n",
      "obtained with {'base_estimator': None, 'learning_rate': 0.1, 'n_estimators': 300, 'random_state': 0}\n"
     ]
    }
   ],
   "source": [
    "gs_ab = grid_search(AdaBoostClassifier(),\n",
    "                    {\n",
    "                        \"base_estimator\": [None],\n",
    "                        \"n_estimators\": [50, 300],\n",
    "                        \"learning_rate\": [0.1, 1.0],\n",
    "                        \"random_state\": [0],\n",
    "                    },\n",
    "                    X_train_4, \n",
    "                    y_train,\n",
    "                    clear=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ac8fc3ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExtraTreesClassifier best validation score is 0.82085 +- 0.01101,\n",
      "obtained with {'bootstrap': False, 'class_weight': 'balanced', 'criterion': 'gini', 'max_features': 'auto', 'min_samples_leaf': 1, 'min_samples_split': 7, 'n_estimators': 750, 'random_state': 0}\n"
     ]
    }
   ],
   "source": [
    "gs_etc = grid_search(ExtraTreesClassifier(),\n",
    "                    {\n",
    "                        \"n_estimators\": [10, 50, 100, 250, 500, 750],\n",
    "                        \"criterion\": [\"gini\", \"entropy\"],\n",
    "                        \"bootstrap\": [True, False],\n",
    "                        \"min_samples_split\" :[2,3,5,7],\n",
    "                        \"min_samples_leaf\": [1,2,3,5,7],\n",
    "                        \"max_features\": [\"auto\", \"log2\"],\n",
    "                        \"class_weight\": [\"balanced\", \"balanced_subsample\"],\n",
    "                        \"random_state\": [0],\n",
    "                    },\n",
    "                    X_train_4, \n",
    "                    y_train,\n",
    "                    clear=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0b183697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RidgeClassifier best validation score is 0.76222 +- 0.00770,\n",
      "obtained with {'alpha': 0.1, 'class_weight': None, 'random_state': 0}\n"
     ]
    }
   ],
   "source": [
    "# No predict_proba\n",
    "gs_ridge = grid_search(RidgeClassifier(),\n",
    "                       {\n",
    "                           \"alpha\": [0.1, 1, 5],\n",
    "                           \"class_weight\": [\"balanced\", None],\n",
    "                           \"random_state\": [0],\n",
    "                       },\n",
    "                       X_train_4,\n",
    "                       y_train,\n",
    "                       clear=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d0001a07",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression best validation score is 0.76771 +- 0.00640,\n",
      "obtained with {'C': 1, 'class_weight': None, 'max_iter': 2000, 'multi_class': 'ovr', 'penalty': 'l2', 'random_state': 0, 'solver': 'saga'}\n"
     ]
    }
   ],
   "source": [
    "gs_logreg = grid_search(LogisticRegression(),\n",
    "                        {\n",
    "                            \"penalty\": [\"l2\"],\n",
    "                            \"C\": [0.1, 0.5, 1, 5, 10],\n",
    "                            \"class_weight\": [\"balanced\", None],\n",
    "                            \"random_state\": [0],\n",
    "                            \"max_iter\": [1000],\n",
    "                            \"solver\": [\"newton-cg\", \"lbfgs\", \"sag\", \"saga\"],\n",
    "                            \"multi_class\": [\"auto\", \"ovr\", \"multinomial\"]\n",
    "                        },\n",
    "                        X_train_4,\n",
    "                        y_train,\n",
    "                        clear=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a8b0edf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier best validation score is 0.78705 +- 0.01327,\n",
      "obtained with {'activation': 'relu', 'alpha': 0.0001, 'early_stopping': True, 'hidden_layer_sizes': (500,), 'random_state': 0, 'solver': 'adam'}\n"
     ]
    }
   ],
   "source": [
    "gs_mlp = grid_search(MLPClassifier(),\n",
    "                     {\n",
    "                         \"hidden_layer_sizes\": [(500, ), (100,), (40, 40, 30, 10), (40, 40), (100, 30, 10), (100, 100, 100), (15, 15, 15, 15, 15, 7)],\n",
    "                         \"alpha\": [0.0001],\n",
    "                         \"activation\": ['relu'],#['logistic', 'tanh', 'relu'],\n",
    "                         \"solver\": [\"adam\"],\n",
    "                         \"early_stopping\": [True],\n",
    "                         \"random_state\": [0],\n",
    "                     },\n",
    "                     X_train_4,\n",
    "                     y_train,\n",
    "                     clear=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "86a2ec25",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV 1/5] END .......................voting=soft;, score=0.840 total time= 1.4min\n",
      "[CV 2/5] END .......................voting=soft;, score=0.835 total time= 1.4min\n",
      "[CV 3/5] END .......................voting=soft;, score=0.844 total time= 1.4min\n",
      "[CV 4/5] END .......................voting=soft;, score=0.818 total time= 1.4min\n",
      "[CV 5/5] END .......................voting=soft;, score=0.840 total time= 1.4min\n",
      "VotingClassifier best validation score is 0.83550 +- 0.00934,\n",
      "obtained with {'voting': 'soft'}\n"
     ]
    }
   ],
   "source": [
    "# Only kept models with a predict proba function for soft voting + good val score\n",
    "gs_ensemble = grid_search(VotingClassifier([('svc', SVC(probability=True, **gs_svc.best_params_)),\n",
    "                                            ('rf', RandomForestClassifier(**gs_random_forest.best_params_)),\n",
    "                                            ('gbc', GradientBoostingClassifier(**gs_gbc.best_params_)),\n",
    "                                            ('etc', ExtraTreesClassifier(**gs_etc.best_params_))]),\n",
    "                          {\n",
    "                              \"voting\": [\"soft\"], #[\"hard\", \"soft\"],\n",
    "                          },\n",
    "                          X_train_4, \n",
    "                          y_train,\n",
    "                          clear=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3fa2da40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV 1/5] END .......................voting=soft;, score=0.837 total time= 1.3min\n",
      "[CV 2/5] END .......................voting=soft;, score=0.838 total time= 1.3min\n",
      "[CV 3/5] END .......................voting=soft;, score=0.846 total time= 1.3min\n",
      "[CV 4/5] END .......................voting=soft;, score=0.816 total time= 1.3min\n",
      "[CV 5/5] END .......................voting=soft;, score=0.845 total time= 1.3min\n",
      "VotingClassifier best validation score is 0.83652 +- 0.01102,\n",
      "obtained with {'voting': 'soft'}\n"
     ]
    }
   ],
   "source": [
    "# Only kept models with a predict proba function for soft voting + good val score\n",
    "gs_ensemble_2 = grid_search(VotingClassifier([('rf', RandomForestClassifier(**gs_random_forest.best_params_)),\n",
    "                                              ('gbc', GradientBoostingClassifier(**gs_gbc.best_params_)),\n",
    "                                              ('etc', ExtraTreesClassifier(**gs_etc.best_params_))]),\n",
    "                            {\n",
    "                                \"voting\": [\"soft\"], #[\"hard\", \"soft\"],\n",
    "                            },\n",
    "                            X_train_4, \n",
    "                            y_train,\n",
    "                            clear=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9476a765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV 1/5] END .......................voting=soft;, score=0.830 total time= 1.4min\n",
      "[CV 2/5] END .......................voting=soft;, score=0.829 total time= 1.4min\n",
      "[CV 3/5] END .......................voting=soft;, score=0.844 total time= 1.4min\n",
      "[CV 4/5] END .......................voting=soft;, score=0.819 total time= 1.4min\n",
      "[CV 5/5] END .......................voting=soft;, score=0.827 total time= 1.4min\n",
      "VotingClassifier best validation score is 0.82980 +- 0.00823,\n",
      "obtained with {'voting': 'soft'}\n"
     ]
    }
   ],
   "source": [
    "# Only kept models with a predict proba function for soft voting + good val score\n",
    "gs_ensemble_3 = grid_search(VotingClassifier([('rf', RandomForestClassifier(**gs_random_forest.best_params_)),\n",
    "                                              ('gbc', GradientBoostingClassifier(**gs_gbc.best_params_)),\n",
    "                                              ('etc', ExtraTreesClassifier(**gs_etc.best_params_)),\n",
    "                                              ('svc', SVC(probability=True, **gs_svc.best_params_)),\n",
    "                                              ('mlp', MLPClassifier(**gs_mlp.best_params_))]),\n",
    "                            {\n",
    "                                \"voting\": [\"soft\"], #[\"hard\", \"soft\"],\n",
    "                            },\n",
    "                            X_train_4, \n",
    "                            y_train,\n",
    "                            clear=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7bf4a38a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV 1/5] END .......................voting=soft;, score=0.838 total time= 1.2min\n",
      "[CV 2/5] END .......................voting=soft;, score=0.836 total time= 1.2min\n",
      "[CV 3/5] END .......................voting=soft;, score=0.847 total time= 1.2min\n",
      "[CV 4/5] END .......................voting=soft;, score=0.813 total time= 1.2min\n",
      "[CV 5/5] END .......................voting=soft;, score=0.841 total time= 1.2min\n",
      "VotingClassifier best validation score is 0.83510 +- 0.01183,\n",
      "obtained with {'voting': 'soft'}\n"
     ]
    }
   ],
   "source": [
    "# Only kept models with a predict proba function for soft voting + good val score\n",
    "gs_ensemble_4 = grid_search(VotingClassifier([('rf', RandomForestClassifier(**gs_random_forest.best_params_)),\n",
    "                                              ('gbc', GradientBoostingClassifier(**gs_gbc.best_params_))\n",
    "                                             ]),\n",
    "                            {\n",
    "                                \"voting\": [\"soft\"], #[\"hard\", \"soft\"],\n",
    "                            },\n",
    "                            X_train_4, \n",
    "                            y_train,\n",
    "                            clear=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77cbffc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All models => hard voting\n",
    "gs_ensemble_5 = grid_search(VotingClassifier([('rf', RandomForestClassifier(**gs_random_forest.best_params_)),\n",
    "                                              ('gbc', GradientBoostingClassifier(**gs_gbc.best_params_)),\n",
    "                                              ('etc', ExtraTreesClassifier(**gs_etc.best_params_)),\n",
    "                                              ('svc', SVC(probability=True, **gs_svc.best_params_)),\n",
    "                                              ('mlp', MLPClassifier(**gs_mlp.best_params_)),\n",
    "                                              ('knn', KNeighborsClassifier(**gs_knn.best_params_)),\n",
    "                                              ('gp', GaussianProcessClassifier(**gs_gp.best_params_)),\n",
    "                                              ('ada', AdaBoostClassifier(**gs_ab.best_params_)),\n",
    "                                              ('ridge', RidgeClassifier(**gs_ridge.best_params_)),\n",
    "                                              ('logreg', LogisticRegression(**gs_logreg.best_params_)),\n",
    "                                             ]),\n",
    "                                              \n",
    "                            {\n",
    "                                \"voting\": [\"hard\"], #[\"hard\", \"soft\"],\n",
    "                            },\n",
    "                            X_train_4, \n",
    "                            y_train,\n",
    "                            clear=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd4f19a",
   "metadata": {},
   "source": [
    "---\n",
    "## Generate new submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "957c172a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gs_ensemble_4\n",
    "sub_id = 40\n",
    "prediction = pd.DataFrame(model.predict(X_test_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "221af8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_submission(sub_id, prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95891ad7",
   "metadata": {},
   "source": [
    "**Solutions must be submitted on the [project website](https://aml.ise.inf.ethz.ch/task2/#submission).**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c8e3aa7",
   "metadata": {},
   "source": [
    "# `AML — Task 2:` Heart rhythm classification from raw ECG signals\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a018a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For TQDM :\n",
    "#! python3.6 -m pip install ipywidgets\n",
    "#! python3.6 -m pip install --upgrade jupyter\n",
    "#! jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f126d6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import biosppy.signals.ecg as ecg\n",
    "import biosppy.signals.tools as tools\n",
    "from biosppy.plotting import plot_ecg\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6dc16cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_selection import SelectFpr, SelectFdr, SelectFwe, f_classif, chi2, mutual_info_classif\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.linear_model import RidgeClassifier, LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier,\\\n",
    "                             AdaBoostClassifier, VotingClassifier, ExtraTreesClassifier, IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b29de4",
   "metadata": {},
   "source": [
    "---\n",
    "## Dataset import and export to `csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acdf2170",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_from_csv(extension=\"\", drop_id = True):\n",
    "    X_train = pd.read_csv('data/X_train' + extension + '.csv')\n",
    "    y_train = pd.read_csv('data/y_train' + extension + '.csv')\n",
    "    X_test  = pd.read_csv('data/X_test' + extension + '.csv')\n",
    "    \n",
    "    if drop_id:\n",
    "        X_train = X_train.drop(columns=['id'])\n",
    "        y_train = y_train.drop(columns=['id'])\n",
    "        X_test  = X_test.drop(columns=['id'])\n",
    "     \n",
    "    return X_train, y_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "910612cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_to_csv(X_train, y_train, X_test, extension=\"_cleaned\"):\n",
    "    X_train.to_csv('data/X_train' + extension + '.csv', index=False)\n",
    "    y_train.to_csv('data/y_train' + extension + '.csv', index=False)\n",
    "    X_test.to_csv('data/X_test' + extension + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a707ea",
   "metadata": {},
   "source": [
    "## Submission export to `csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3acf5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_submission(sub_id, pred, basepath='submissions/task2-sub'):\n",
    "    result = pred.copy().rename(columns={0: 'y'})\n",
    "    result['id'] = range(0, len(result))\n",
    "    result = result[['id', 'y']]\n",
    "    result.to_csv(basepath + str(sub_id) + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ffbae4",
   "metadata": {},
   "source": [
    "---\n",
    "## Data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809894cd",
   "metadata": {},
   "source": [
    "### Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "fdd8ab55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_stats_from(features_series: list) -> pd.Series:\n",
    "    # pd.describe().drop(count) returns 7 interesting features (min, max, std...)\n",
    "    stats = pd.concat([pd.Series(s).describe().drop([\"count\"]) for s in features_series], ignore_index=True)\n",
    "    stats = stats.append(pd.Series([pd.Series(s).skew() for s in features_series]), ignore_index=True)\n",
    "    stats = stats.append(pd.Series([pd.Series(s).kurtosis() for s in features_series]), ignore_index=True)\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b98a4871",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_qpeaks_features(filtered: np.array, rpeaks: np.array, window_size=50):\n",
    "    qpeaks = [rpeak - window_size + np.argmin(filtered[rpeak-window_size:rpeak]) for rpeak in rpeaks]\n",
    "    qpeaks_amplitudes = [filtered[qpeak] for qpeak in qpeaks]\n",
    "    return qpeaks, qpeaks_amplitudes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11deacd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_speaks_features(filtered: np.array, rpeaks: np.array, window_size=50):\n",
    "    speaks = [rpeak + np.argmin(filtered[rpeak:rpeak+window_size]) for rpeak in rpeaks]\n",
    "    speaks_amplitudes = [filtered[speak] for speak in speaks]\n",
    "    return speaks, speaks_amplitudes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08f32d8",
   "metadata": {},
   "source": [
    "### Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "4d47da7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(time_series: pd.Series, sampling_rate=300) -> pd.Series:\n",
    "    # Drop nan values in the time series\n",
    "    no_nans = time_series.dropna()\n",
    "    \n",
    "    # Extract main features from ECG\n",
    "    ts, filtered, rpeaks, _, templates, _, heart_rate = ecg.ecg(no_nans, sampling_rate, show=False)\n",
    "    assert len(rpeaks) > 1, 'ECG cannot have a single R peak'\n",
    "    assert len(templates) > 1, 'ECG cannot have a single heartbeat'\n",
    "    \n",
    "    # Extract Q,R,S peak features\n",
    "    rpeaks_amplitude = [filtered[rpeak] for rpeak in rpeaks]\n",
    "    qpeaks, qpeaks_amplitude = extract_qpeaks_features(filtered, rpeaks)\n",
    "    speaks, speaks_amplitude = extract_speaks_features(filtered, rpeaks)\n",
    "    \n",
    "    # Extract RR, QRS durations features\n",
    "    rr_durations = [r2 - r1 for r1, r2 in zip(rpeaks, rpeaks[1:])]\n",
    "    qrs_durations = [speak - qpeak for qpeak, speak in zip(qpeaks, speaks)]\n",
    "    \n",
    "    # Extract RR differences\n",
    "    rr_differences = [rr2 - rr1 for rr1, rr2 in zip(rr_durations, rr_durations[1:])]\n",
    "    \n",
    "    # Extract QRS direction features\n",
    "    qrs_direction = [q + r + s for q, r, s in zip(qpeaks, rpeaks, speaks)]\n",
    "    \n",
    "    # Extract Q/R ratio features\n",
    "    qr_ratio = [q / r for q, r in zip(qpeaks, rpeaks)]\n",
    "    \n",
    "    #TODO: Extract SNR ratio (http://www.cinc.org/archives/2011/pdf/0609.pdf)\n",
    "    snr = np.quantile(np.std(templates, axis=0), 0.35)\n",
    "    \n",
    "    # Use this to go from index differences to seconds\n",
    "    index_to_time = ts[-1] / len(filtered)\n",
    "    # Extract pNNx (https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1767394/)\n",
    "    pNNs = []\n",
    "    for i in np.arange(5, 55, 5):\n",
    "        pNNs.append((np.array(rr_durations) * index_to_time > i * 0.001).sum() / len(rr_durations))\n",
    "    pNNs = pd.Series(pNNs)\n",
    "    \n",
    "    # Extract log(RMSSD) (log of root mean square successive differences, linked to heart rate variability (HRV))\n",
    "    logRMSSD = np.log(np.sqrt(np.mean(np.array(rr_differences) ** 2)))\n",
    "    \n",
    "    # Extract power features\n",
    "    freqs, power = tools.power_spectrum(filtered, sampling_rate, decibel=True)\n",
    "    total_power = tools.band_power(freqs, power, [0, 0.4], decibel=True)[0]\n",
    "    vlf_power = tools.band_power(freqs, power, [0, 0.04], decibel=True)[0]\n",
    "    lf_power = tools.band_power(freqs, power, [0.04, 0.15], decibel=True)[0]\n",
    "    hf_power = tools.band_power(freqs, power, [0.15, 0.4], decibel=True)[0]\n",
    "    lfhf_ratio = lf_power / hf_power\n",
    "    lf_power_norm = lf_power / (total_power - vlf_power)\n",
    "    hf_power_norm = hf_power / (total_power - vlf_power)\n",
    "    \n",
    "    # Extract beats skew and kurtosis (https://paperswithcode.com/paper/heartbeat-classification-fusing-temporal-and)\n",
    "    skews = pd.DataFrame(templates).skew(axis=1)\n",
    "    kurtosiss = pd.DataFrame(templates).kurtosis(axis=1)\n",
    "    \n",
    "    # Extract statistics\n",
    "    ecg_features = extract_stats_from([\n",
    "        rpeaks_amplitude, qpeaks_amplitude, speaks_amplitude,\n",
    "        rr_durations, qrs_durations, rr_differences, qrs_direction, \n",
    "        qr_ratio, skews, kurtosiss\n",
    "    ])\n",
    "    \n",
    "    # Extract different standard deviations\n",
    "    sdsd = np.std(rr_differences) # could be optimized, already computed\n",
    "    sdnn = np.std(rr_durations) # could be optimized, already computed\n",
    "    sd1 = (1 / np.sqrt(2)) * sdsd\n",
    "    sd2 = 2 * sdnn**2 - sd1**2\n",
    "    sd1sd2ratio = sd1 / sd2\n",
    "    s = 3.14159 * sd1 * sd2\n",
    "    \n",
    "    return ecg_features.append(pd.Series([snr, \n",
    "                                          logRMSSD, \n",
    "                                          total_power, \n",
    "                                          vlf_power,\n",
    "                                          lf_power, \n",
    "                                          hf_power,\n",
    "                                          lfhf_ratio, \n",
    "                                          lf_power_norm, \n",
    "                                          hf_power_norm,\n",
    "                                          sd1,\n",
    "                                          sd2, \n",
    "                                          sd1sd2ratio, \n",
    "                                          s]), ignore_index=True).append(pNNs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b98812",
   "metadata": {},
   "source": [
    "---\n",
    "## Data standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e31de0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_data(X_train, X_test):\n",
    "    scaler = StandardScaler().fit(X_train)\n",
    "    X_train_scaled = scaler.transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    return pd.DataFrame(X_train_scaled), pd.DataFrame(X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a616b1",
   "metadata": {},
   "source": [
    "---\n",
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6178196e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_constant_features(X_train, X_test, verbose=True):\n",
    "    non_constant_features_mask = X_train.apply(pd.Series.nunique) != 1\n",
    "    X_train_selected_features = X_train.loc[:, non_constant_features_mask]\n",
    "    X_test_selected_features = X_test.loc[:, non_constant_features_mask]\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"{X_train.shape[1]-X_train_selected_features.shape[1]} features removed because of constant values ({100*(X_train.shape[1]-X_train_selected_features.shape[1])/X_train.shape[1]:.2f}%).\")\n",
    "    \n",
    "    return X_train_selected_features, X_test_selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd321209",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_too_correlated_features(X_train, X_test, threshold=0.98, verbose=True):\n",
    "    X_train_corr_ = X_train.corr()\n",
    "\n",
    "    X_train_too_correlated = (X_train_corr_.mask(\n",
    "        np.tril(np.ones([len(X_train_corr_)]*2, dtype=bool))).abs() > threshold).any()\n",
    "    \n",
    "    X_train_selected_features = X_train.loc[:, (~X_train_too_correlated)]\n",
    "    X_test_selected_features = X_test.loc[:, (~X_train_too_correlated)]\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"{X_train.shape[1]-X_train_selected_features.shape[1]} features removed because of correlation with another feature > {threshold} ({100*(X_train.shape[1]-X_train_selected_features.shape[1])/X_train.shape[1]:.2f}%).\")\n",
    "\n",
    "    return X_train_selected_features, X_test_selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fcae7ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_useless_features(X_train, y_train, X_test, selector_type, stat, verbose=1):\n",
    "    # See https://stats.stackexchange.com/questions/328358/fpr-fdr-and-fwe-for-feature-selection\n",
    "    # and https://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection\n",
    "    \n",
    "    assert selector_type in [\"fpr\", \"fdr\", \"fwe\"], \"Unrecognised selector type\"\n",
    "    assert stat in [f_classif, chi2, mutual_info_classif], \"Unrecognised stat\"\n",
    "    selector = None\n",
    "    if selector_type == \"fpr\":\n",
    "        selector = SelectFpr(stat)\n",
    "    elif selector_type == \"fdr\":\n",
    "        selector = SelectFdr(stat)\n",
    "    elif selector_type == \"fwe\":\n",
    "        selector = SelectFwe(stat)\n",
    "    \n",
    "    selector.fit(X_train, y_train)\n",
    "    X_train_selected_features = pd.DataFrame(selector.transform(X_train))\n",
    "    X_test_selected_features = pd.DataFrame(selector.transform(X_test))\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"{X_train.shape[1]-X_train_selected_features.shape[1]} features removed because of complicated p-value stuff we don't understand ({100*(X_train.shape[1]-X_train_selected_features.shape[1])/X_train.shape[1]:.2f}%).\")\n",
    "        \n",
    "    return X_train_selected_features, X_test_selected_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e834e645",
   "metadata": {},
   "source": [
    "---\n",
    "## Outlier detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b8a29d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers(X_train, y_train, contamination='auto', verbose=1, method=\"LocalOutlierFactor\"):\n",
    "    clf = None\n",
    "    if method == \"LocalOutlierFactor\":\n",
    "        clf = LocalOutlierFactor(contamination=contamination)\n",
    "    elif method == \"IsolationForest\":\n",
    "        clf = IsolationForest(contamination=contamination, random_state=0, verbose=verbose)\n",
    "    else:\n",
    "        raise AttributeError(f\"Unvalid argument for method, must be 'LocalOutlierFactor' or 'IsolationForest', not '{method}'\")\n",
    "        \n",
    "    outliers_mask = pd.Series(clf.fit_predict(X_train)).map({1:1, -1:0}) #Mask with 0 for outliers and 1 for non outliers\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Detected {(outliers_mask == 0).sum()} outliers with method {method}, out of {outliers_mask.shape[0]} samples ({100 * (outliers_mask == 0).sum() / outliers_mask.shape[0]:.2f}%).\")\n",
    "    \n",
    "    # Remove outliers from the training set\n",
    "    X_train = np.array(X_train)[outliers_mask == 1, :]\n",
    "    y_train = np.array(y_train)[outliers_mask == 1]\n",
    "    \n",
    "    return pd.DataFrame(X_train), pd.DataFrame(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfed9a4c",
   "metadata": {},
   "source": [
    "---\n",
    "## Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4891314a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(model, params, X_train, y_train, clear=False):\n",
    "    gs = GridSearchCV(model, params, cv=5, verbose=3, scoring='f1_micro', error_score='raise')\n",
    "    gs.fit(X_train, y_train)\n",
    "    \n",
    "    if clear:\n",
    "        clear_output(wait=True)\n",
    "    print(f\"{type(gs.best_estimator_).__name__} best validation score is {gs.best_score_:.5f} +- {gs.cv_results_['std_test_score'][gs.best_index_]:.5f},\\nobtained with {gs.best_params_}\")\n",
    "    \n",
    "    return gs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436814d8",
   "metadata": {},
   "source": [
    "---\n",
    "## Main Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358f9688",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2dbf8612",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_raw, y_train_raw, X_test_raw = load_from_csv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b44965",
   "metadata": {},
   "source": [
    "### Extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "080216a8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d08240d3ee149e6b5b13f97407a8183",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5117 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_train = X_train_raw.progress_apply(extract_features, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "e22045f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76fe5b7572b14864a525addb61f5ad76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3411 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_test = X_test_raw.progress_apply(extract_features, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "c5895c58",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train has 0 null values.\n",
      "X_test has 0 null values.\n"
     ]
    }
   ],
   "source": [
    "print(f\"X_train has {X_train.isna().sum().sum()} null values.\")\n",
    "print(f\"X_test has {X_test.isna().sum().sum()} null values.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ffc823",
   "metadata": {},
   "source": [
    "### Standardize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "49458a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = standardize_data(X_train, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb48401e",
   "metadata": {},
   "source": [
    "### Select features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "60abe361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 features removed because of constant values (0.00%).\n"
     ]
    }
   ],
   "source": [
    "X_train_1, X_test_1 = remove_constant_features(X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "17548772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 features removed because of correlation with another feature > 0.98 (14.16%).\n"
     ]
    }
   ],
   "source": [
    "X_train_2, X_test_2 = remove_too_correlated_features(X_train_1, X_test_1, threshold=0.98)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "74ac5095",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 features removed because of complicated p-value stuff we don't understand (10.31%).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/antoine/miniconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:985: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "X_train_3, X_test_3 = remove_useless_features(X_train_2, y_train_raw, X_test_2, selector_type=\"fpr\", stat=f_classif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "2cb1f419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected 161 outliers with method LocalOutlierFactor, out of 5117 samples (3.15%).\n"
     ]
    }
   ],
   "source": [
    "X_train_4, y_train = remove_outliers(X_train_3, y_train_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "4c9e8af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(y_train).ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34218305",
   "metadata": {},
   "source": [
    "### Train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "8aef3c96",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC best validation score is 0.78511 +- 0.00451,\n",
      "obtained with {'C': 1, 'class_weight': None, 'kernel': 'rbf', 'random_state': 0}\n"
     ]
    }
   ],
   "source": [
    "gs_svc = grid_search(SVC(),\n",
    "                     {\n",
    "                         # Need to play with degree parameter for poly kernel\n",
    "                         \"kernel\": [\"rbf\", \"poly\"], #[\"rbf\", \"poly\", \"sigmoid\"],\n",
    "                         \"C\": [1, 10, 50, 100],\n",
    "                         \"class_weight\": [\"balanced\", None],\n",
    "                         \"random_state\": [0],\n",
    "                     },\n",
    "                     X_train_4,\n",
    "                     y_train,\n",
    "                     clear=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "ff0ae041",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier best validation score is 0.82244 +- 0.01382,\n",
      "obtained with {'class_weight': None, 'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100, 'random_state': 0}\n"
     ]
    }
   ],
   "source": [
    "gs_random_forest = grid_search(RandomForestClassifier(),\n",
    "                               {\n",
    "                                   \"n_estimators\": [100], #np.arange(100, 300, 200),\n",
    "                                   \"max_depth\": [None], #np.arange(2, 8, 1),\n",
    "                                   \"min_samples_split\": [2, 4], #np.arange(2, 8, 1),\n",
    "                                   \"min_samples_leaf\": [1, 4], #np.arange(1, 9, 2),\n",
    "                                   \"class_weight\": [\"balanced\", None],\n",
    "                                   \"random_state\": [0], \n",
    "                               },\n",
    "                               X_train_4,\n",
    "                               y_train,\n",
    "                               clear=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "4d4cce3d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoostingClassifier best validation score is 0.82789 +- 0.00497,\n",
      "obtained with {'criterion': 'squared_error', 'learning_rate': 0.1, 'loss': 'deviance', 'min_samples_leaf': 3, 'min_samples_split': 4, 'n_estimators': 200, 'n_iter_no_change': None, 'subsample': 0.7, 'tol': 0.0001}\n"
     ]
    }
   ],
   "source": [
    "gs_gbc = grid_search(GradientBoostingClassifier(),\n",
    "                     {\n",
    "                         \"loss\": [\"deviance\"],\n",
    "                         \"learning_rate\": [0.1], #[0.1, 1],\n",
    "                         \"n_estimators\": [200], #[50, 200],\n",
    "                         \"subsample\": [0.7], #[0.7, 1],\n",
    "                         \"criterion\": [\"squared_error\"], #[\"friedman_mse\", \"mse\"],\n",
    "                         \"min_samples_split\": [4], #[2, 4],\n",
    "                         \"min_samples_leaf\": [3], #[1, 3],\n",
    "                         \"n_iter_no_change\": [None],\n",
    "                         \"tol\": [1e-4],\n",
    "                     },\n",
    "                     X_train_4, \n",
    "                     y_train,\n",
    "                     clear=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "99dbe220",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier best validation score is 0.75103 +- 0.01143,\n",
      "obtained with {'n_neighbors': 8, 'weights': 'distance'}\n"
     ]
    }
   ],
   "source": [
    "# No predict_proba\n",
    "gs_knn = grid_search(KNeighborsClassifier(),\n",
    "                     {\n",
    "                         \"n_neighbors\": np.arange(2, 10, 1),\n",
    "                         \"weights\": [\"uniform\", \"distance\"],\n",
    "                     },\n",
    "                     X_train_4, \n",
    "                     y_train,\n",
    "                     clear=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f68e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes 1min to train a single instance, but scores of ~0.7...\n",
    "gs_gp = grid_search(GaussianProcessClassifier(),\n",
    "                    {\n",
    "                        \"kernel\": [None],\n",
    "                        \"multi_class\": [\"one_vs_rest\", \"one_vs_one\"],\n",
    "                        \"random_state\": [0],\n",
    "                    },\n",
    "                    X_train_4, \n",
    "                    y_train,\n",
    "                    clear=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b160fba4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier best validation score is 0.72210 +- 0.00570,\n",
      "obtained with {'base_estimator': None, 'learning_rate': 0.1, 'n_estimators': 300, 'random_state': 0}\n"
     ]
    }
   ],
   "source": [
    "gs_ab = grid_search(AdaBoostClassifier(),\n",
    "                    {\n",
    "                        \"base_estimator\": [None],\n",
    "                        \"n_estimators\": [50, 300],\n",
    "                        \"learning_rate\": [0.1, 1.0],\n",
    "                        \"random_state\": [0],\n",
    "                    },\n",
    "                    X_train_4, \n",
    "                    y_train,\n",
    "                    clear=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "ac8fc3ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExtraTreesClassifier best validation score is 0.81618 +- 0.01358,\n",
      "obtained with {'bootstrap': False, 'class_weight': 'balanced', 'criterion': 'gini', 'n_estimators': 500, 'random_state': 0}\n"
     ]
    }
   ],
   "source": [
    "gs_etc = grid_search(ExtraTreesClassifier(),\n",
    "                    {\n",
    "                        \"n_estimators\": [500], #[50, 100, 500],\n",
    "                        \"criterion\": ['gini'], #[\"gini\", \"entropy\"],\n",
    "                        \"bootstrap\": [True, False],\n",
    "                        \"class_weight\": [\"balanced\", \"balanced_subsample\"],\n",
    "                        \"random_state\": [0],\n",
    "                    },\n",
    "                    X_train_4, \n",
    "                    y_train,\n",
    "                    clear=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0b183697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RidgeClassifier best validation score is 0.72152 +- 0.00717,\n",
      "obtained with {'alpha': 1, 'class_weight': None, 'random_state': 0}\n"
     ]
    }
   ],
   "source": [
    "# No predict_proba\n",
    "gs_ridge = grid_search(RidgeClassifier(),\n",
    "                       {\n",
    "                           \"alpha\": [0.1, 1, 5],\n",
    "                           \"class_weight\": [\"balanced\", None],\n",
    "                           \"random_state\": [0],\n",
    "                       },\n",
    "                       X_train_4,\n",
    "                       y_train,\n",
    "                       clear=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d0001a07",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression best validation score is 0.75240 +- 0.00754,\n",
      "obtained with {'C': 10, 'class_weight': None, 'max_iter': 1000, 'penalty': 'l2', 'random_state': 0}\n"
     ]
    }
   ],
   "source": [
    "gs_logreg = grid_search(LogisticRegression(),\n",
    "                        {\n",
    "                            \"penalty\": [\"l2\"],\n",
    "                            \"C\": [0.1, 1, 10],\n",
    "                            \"class_weight\": [\"balanced\", None],\n",
    "                            \"random_state\": [0],\n",
    "                            \"max_iter\": [1000]\n",
    "                        },\n",
    "                        X_train_4,\n",
    "                        y_train,\n",
    "                        clear=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "a8b0edf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier best validation score is 0.78894 +- 0.00711,\n",
      "obtained with {'alpha': 0.0001, 'early_stopping': True, 'hidden_layer_sizes': (100,), 'random_state': 0, 'solver': 'adam'}\n"
     ]
    }
   ],
   "source": [
    "gs_mlp = grid_search(MLPClassifier(),\n",
    "                     {\n",
    "                         \"hidden_layer_sizes\": [(100,), (40, 40, 30, 10), (40, 40), (100, 30, 10), (100, 100, 100), (15, 15, 15, 15, 15, 7)],\n",
    "                         \"alpha\": [0.0001],\n",
    "                         \"solver\": [\"adam\"],\n",
    "                         \"early_stopping\": [True],\n",
    "                         \"random_state\": [0],\n",
    "                     },\n",
    "                     X_train_4,\n",
    "                     y_train,\n",
    "                     clear=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "86a2ec25",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV 1/5] END .......................voting=soft;, score=0.836 total time=  57.4s\n",
      "[CV 2/5] END .......................voting=soft;, score=0.818 total time=  54.7s\n",
      "[CV 3/5] END .......................voting=soft;, score=0.836 total time=  57.0s\n",
      "[CV 4/5] END .......................voting=soft;, score=0.820 total time= 1.2min\n",
      "[CV 5/5] END .......................voting=soft;, score=0.826 total time=  55.5s\n",
      "VotingClassifier best validation score is 0.82728 +- 0.00730,\n",
      "obtained with {'voting': 'soft'}\n"
     ]
    }
   ],
   "source": [
    "# Only kept models with a predict proba function for soft voting + good val score\n",
    "gs_ensemble = grid_search(VotingClassifier([('svc', SVC(probability=True, **gs_svc.best_params_)),\n",
    "                                            ('rf', RandomForestClassifier(**gs_random_forest.best_params_)),\n",
    "                                            ('gbc', GradientBoostingClassifier(**gs_gbc.best_params_)),\n",
    "                                            ('etc', ExtraTreesClassifier(**gs_etc.best_params_))]),\n",
    "                          {\n",
    "                              \"voting\": [\"soft\"], #[\"hard\", \"soft\"],\n",
    "                          },\n",
    "                          X_train_4, \n",
    "                          y_train,\n",
    "                          clear=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "3fa2da40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV 1/5] END .......................voting=soft;, score=0.840 total time=  48.3s\n",
      "[CV 2/5] END .......................voting=soft;, score=0.820 total time=  49.1s\n",
      "[CV 3/5] END .......................voting=soft;, score=0.843 total time=  46.3s\n",
      "[CV 4/5] END .......................voting=soft;, score=0.817 total time=  51.0s\n",
      "[CV 5/5] END .......................voting=soft;, score=0.831 total time=  49.1s\n",
      "VotingClassifier best validation score is 0.83030 +- 0.01007,\n",
      "obtained with {'voting': 'soft'}\n"
     ]
    }
   ],
   "source": [
    "# Only kept models with a predict proba function for soft voting + good val score\n",
    "gs_ensemble_2 = grid_search(VotingClassifier([('rf', RandomForestClassifier(**gs_random_forest.best_params_)),\n",
    "                                              ('gbc', GradientBoostingClassifier(**gs_gbc.best_params_)),\n",
    "                                              ('etc', ExtraTreesClassifier(**gs_etc.best_params_))]),\n",
    "                            {\n",
    "                                \"voting\": [\"soft\"], #[\"hard\", \"soft\"],\n",
    "                            },\n",
    "                            X_train_4, \n",
    "                            y_train,\n",
    "                            clear=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "9476a765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV 1/5] END .......................voting=soft;, score=0.829 total time=  52.9s\n",
      "[CV 2/5] END .......................voting=soft;, score=0.820 total time=  53.1s\n",
      "[CV 3/5] END .......................voting=soft;, score=0.834 total time=  58.4s\n",
      "[CV 4/5] END .......................voting=soft;, score=0.816 total time=  56.3s\n",
      "[CV 5/5] END .......................voting=soft;, score=0.824 total time=  56.4s\n",
      "VotingClassifier best validation score is 0.82466 +- 0.00602,\n",
      "obtained with {'voting': 'soft'}\n"
     ]
    }
   ],
   "source": [
    "# Only kept models with a predict proba function for soft voting + good val score\n",
    "gs_ensemble_3 = grid_search(VotingClassifier([('rf', RandomForestClassifier(**gs_random_forest.best_params_)),\n",
    "                                              ('gbc', GradientBoostingClassifier(**gs_gbc.best_params_)),\n",
    "                                              ('etc', ExtraTreesClassifier(**gs_etc.best_params_)),\n",
    "                                              ('svc', SVC(probability=True, **gs_svc.best_params_)),\n",
    "                                              ('mlp', MLPClassifier(**gs_mlp.best_params_))]),\n",
    "                            {\n",
    "                                \"voting\": [\"soft\"], #[\"hard\", \"soft\"],\n",
    "                            },\n",
    "                            X_train_4, \n",
    "                            y_train,\n",
    "                            clear=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd4f19a",
   "metadata": {},
   "source": [
    "---\n",
    "## Generate new submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "957c172a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gs_ensemble_2\n",
    "sub_id = 18\n",
    "prediction = pd.DataFrame(model.predict(X_test_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "221af8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_submission(sub_id, prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95891ad7",
   "metadata": {},
   "source": [
    "**Solutions must be submitted on the [project website](https://aml.ise.inf.ethz.ch/task2/#submission).**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

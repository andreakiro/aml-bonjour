{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c8e3aa7",
   "metadata": {},
   "source": [
    "# `AML — Task 2:` Heart rhythm classification from raw ECG signals\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f126d6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import biosppy.signals.ecg as ecg\n",
    "import biosppy.signals.tools as tools\n",
    "from biosppy.plotting import plot_ecg\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6dc16cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b29de4",
   "metadata": {},
   "source": [
    "---\n",
    "## Dataset import and export to `csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acdf2170",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_from_csv(extension=\"\", drop_id = True):\n",
    "    X_train = pd.read_csv('data/X_train' + extension + '.csv')\n",
    "    y_train = pd.read_csv('data/y_train' + extension + '.csv')\n",
    "    X_test  = pd.read_csv('data/X_test' + extension + '.csv')\n",
    "    \n",
    "    if drop_id:\n",
    "        X_train = X_train.drop(columns=['id'])\n",
    "        y_train = y_train.drop(columns=['id'])\n",
    "        X_test  = X_test.drop(columns=['id'])\n",
    "     \n",
    "    return X_train, y_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "910612cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_to_csv(X_train, y_train, X_test, extension=\"_cleaned\"):\n",
    "    X_train.to_csv('data/X_train' + extension + '.csv', index=False)\n",
    "    y_train.to_csv('data/y_train' + extension + '.csv', index=False)\n",
    "    X_test.to_csv('data/X_test' + extension + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a707ea",
   "metadata": {},
   "source": [
    "## Submission export to `csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3acf5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_submission(sub_id, pred, basepath='submissions/task2-sub'):\n",
    "    result = pred.copy().rename(columns={0: 'y'})\n",
    "    result['id'] = range(0, len(result))\n",
    "    result = result[['id', 'y']]\n",
    "    result.to_csv(basepath + str(sub_id) + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ffbae4",
   "metadata": {},
   "source": [
    "---\n",
    "## Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3faf2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(time_series: pd.Series, sampling_rate=300.0) -> np.array:\n",
    "    # Drop nan values in the time series\n",
    "    no_nans = time_series.dropna()\n",
    "    \n",
    "    # Extract main features\n",
    "    ts, filtered, rpeaks, _, templates, _, heart_rate = ecg.ecg(no_nans, sampling_rate, show=False)\n",
    "    # If one of these assert fails, let's code the use of another segmenter using the filtered ecg\n",
    "    assert len(rpeaks) > 1, 'ECG cannot have a single R peak'\n",
    "    assert len(templates) > 1, 'ECG cannot have a single heartbeat'\n",
    "    \n",
    "    # Extract R peak amplitudes\n",
    "    rpeaks_amplitudes = [filtered[rpeak] for rpeak in rpeaks]\n",
    "    # Extract mean R peak amplitude\n",
    "    rpeaks_mean = np.mean(rpeaks_amplitudes)\n",
    "    # Extract std of R peak amplitudes\n",
    "    rpeaks_std = np.std(rpeaks_amplitudes)\n",
    "    \n",
    "    #TODO: maybe change durations to seconds?\n",
    "    # Extract RR durations\n",
    "    rr_durations = [r2 - r1 for r1, r2 in zip(rpeaks, rpeaks[1:])]\n",
    "    # Extract mean RR duration\n",
    "    rr_durations_mean = np.mean(rr_durations)\n",
    "    # Extract std of RR durations\n",
    "    rr_durations_std = np.std(rr_durations)\n",
    "    \n",
    "    #TODO: check correlation between rrdurations and heart rate\n",
    "    #RESULT: corr of mean ~0.7, corr of std ~0.6\n",
    "    if len(heart_rate) == 0:\n",
    "        #Temp fix\n",
    "        heart_rate = rr_durations\n",
    "    # Extract mean heart rate\n",
    "    heart_rate_mean = np.mean(heart_rate)\n",
    "    # Extract std of mean heart\n",
    "    heart_rate_std = np.std(heart_rate)\n",
    "    \n",
    "    #TODO: Extract SNR ratio (http://www.cinc.org/archives/2011/pdf/0609.pdf)\n",
    "    # For now, we use this\n",
    "    snr = np.quantile(np.std(templates, axis=0), 0.35)\n",
    "    \n",
    "    window_size = 50\n",
    "    # Extract S peaks\n",
    "    speaks = [rpeak + np.argmin(filtered[rpeak:rpeak+window_size]) for rpeak in rpeaks]\n",
    "    # Extract S peak amplitudes\n",
    "    speaks_amplitudes = [filtered[speak] for speak in speaks]\n",
    "    # Extract mean S peak amplitude\n",
    "    speaks_mean = np.mean(speaks_amplitudes)\n",
    "    # Extract std of S peak amplitudes\n",
    "    speaks_std = np.std(speaks_amplitudes)\n",
    "    \n",
    "    # Extract Q peaks\n",
    "    qpeaks = [rpeak - window_size + np.argmin(filtered[rpeak-window_size:rpeak]) for rpeak in rpeaks]\n",
    "    # Extract Q peak amplitudes\n",
    "    qpeaks_amplitudes = [filtered[qpeak] for qpeak in qpeaks]\n",
    "    # Extract mean Q peak amplitude\n",
    "    qpeaks_mean = np.mean(qpeaks_amplitudes)\n",
    "    # Extract std of Q peak amplitudes\n",
    "    qpeaks_std = np.std(qpeaks_amplitudes)\n",
    "    \n",
    "    # Extract QRS durations\n",
    "    qrs_durations = [speak - qpeak for qpeak, speak in zip(qpeaks, speaks)]\n",
    "    # Extract mean QRS duration\n",
    "    qrs_durations_mean = np.mean(qrs_durations)\n",
    "    # Extract std of QRS durations\n",
    "    qrs_durations_std = np.std(qrs_durations)\n",
    "    \n",
    "    # Use this to go from index differences to seconds\n",
    "    index_to_time = ts[-1] / len(filtered)\n",
    "    # Extract pNN28 (https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1767394/)\n",
    "    pNN28 = (np.array(rr_durations) * index_to_time > 0.028).sum() / len(rr_durations)\n",
    "    \n",
    "    # Return extracted features\n",
    "    return pd.Series([rpeaks_mean, \n",
    "                      rpeaks_std,\n",
    "                      rr_durations_mean, \n",
    "                      rr_durations_std, \n",
    "                      heart_rate_mean, \n",
    "                      heart_rate_std, \n",
    "                      snr,\n",
    "                      speaks_mean,\n",
    "                      speaks_std,\n",
    "                      qpeaks_mean,\n",
    "                      qpeaks_std,\n",
    "                      qrs_durations_mean,\n",
    "                      qrs_durations_std,\n",
    "                      pNN28,])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b98812",
   "metadata": {},
   "source": [
    "## Data standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e31de0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_data(X_train, X_test):\n",
    "    # Do the scaling, saving the scaler to use it for X_test too. No need imputation, just ignore Nan values\n",
    "    scaler = StandardScaler().fit(X_train)\n",
    "    X_train_scaled = scaler.transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    return (X_train_scaled, X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfed9a4c",
   "metadata": {},
   "source": [
    "---\n",
    "## Model defintion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30f886d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_svc(X_train, y_train):\n",
    "    svc = SVC()\n",
    "    gs_svc_params = {\n",
    "        \"kernel\": [\"rbf\", \"poly\", \"sigmoid\"],\n",
    "        \"C\": np.logspace(0, 1, 2),\n",
    "        \"class_weight\": [\"balanced\", None]\n",
    "    }\n",
    "    gs_svc = GridSearchCV(svc, gs_svc_params, cv=5, verbose=3, scoring='f1_micro', error_score='raise')\n",
    "    gs_svc.fit(X_train, y_train)\n",
    "    \n",
    "    print(f\"The best validation score obtained is {gs_svc.best_score_:.5f} with\\n\\t{gs_svc.best_params_}\")\n",
    "    \n",
    "    return gs_svc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16344ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_random_forest(X_train, y_train):\n",
    "    random_forest = RandomForestClassifier()\n",
    "    gs_forest_params = {\n",
    "     \"n_estimators\": np.arange(100, 400, 100),\n",
    "     \"max_depth\": [None], #np.arange(2, 8, 1),\n",
    "     \"min_samples_split\": [2], #np.arange(2, 8, 1),\n",
    "     \"min_samples_leaf\": [1], #np.arange(1, 9, 2),\n",
    "     \"class_weight\": [\"balanced\", None],\n",
    "     \"random_state\": [0], \n",
    "    }\n",
    "    \n",
    "    gs_forest = GridSearchCV(random_forest, gs_forest_params, cv=5, verbose=3, scoring='f1_micro', error_score='raise')\n",
    "    gs_forest.fit(X_train, y_train)\n",
    "\n",
    "    print(f\"The best validation score obtained is {gs_forest.best_score_:.5f} with\\n\\t{gs_forest.best_params_}\")\n",
    "    \n",
    "    return gs_forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b10598c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_ensemble(models, X_train, y_train):\n",
    "    ensemble = VotingClassifier(estimators=[(str(i), model) for i, model in enumerate(models)])\n",
    "    gs_ensemble_params = {\n",
    "     \"voting\": [\"hard\", \"soft\"]\n",
    "    }\n",
    "    \n",
    "    gs_ensemble = GridSearchCV(ensemble, gs_ensemble_params, cv=5,\n",
    "                               verbose=3, scoring='f1_micro', error_score='raise')\n",
    "    gs_ensemble.fit(X_train, y_train)\n",
    "\n",
    "    print(f\"The best validation score obtained is {gs_ensemble.best_score_:.5f} with\\n\\t{gs_ensemble.best_params_}\")\n",
    "    \n",
    "    return gs_ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436814d8",
   "metadata": {},
   "source": [
    "---\n",
    "## Main Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2dbf8612",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_raw, y_train_raw, X_test_raw = load_from_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abf5044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maybe slower, but progress visible so good for debug\n",
    "# X_train = pd.DataFrame()\n",
    "# for i, s in X_train_raw.iterrows():\n",
    "#     print(f\"Extracting row #{i+1:4} of {X_train_raw.shape[0]}\", end='\\r')\n",
    "#     X_train = X_train.append(extract_features(s), ignore_index=True)\n",
    "# print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "080216a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train = X_train_raw.progress_apply(extract_features, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e22045f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test_raw.progress_apply(extract_features, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c5895c58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train has 0 null values.\n",
      "X_test has 0 null values.\n"
     ]
    }
   ],
   "source": [
    "print(f\"X_train has {X_train.isna().sum().sum()} null values.\")\n",
    "print(f\"X_test has {X_test.isna().sum().sum()} null values.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "49458a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = standardize_data(X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8aef3c96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "[CV 1/5] END C=1.0, class_weight=balanced, kernel=rbf;, score=0.706 total time=   1.2s\n",
      "[CV 2/5] END C=1.0, class_weight=balanced, kernel=rbf;, score=0.658 total time=   1.1s\n",
      "[CV 3/5] END C=1.0, class_weight=balanced, kernel=rbf;, score=0.693 total time=   1.1s\n",
      "[CV 4/5] END C=1.0, class_weight=balanced, kernel=rbf;, score=0.683 total time=   1.1s\n",
      "[CV 5/5] END C=1.0, class_weight=balanced, kernel=rbf;, score=0.691 total time=   1.1s\n",
      "[CV 1/5] END C=1.0, class_weight=balanced, kernel=poly;, score=0.682 total time=   1.0s\n",
      "[CV 2/5] END C=1.0, class_weight=balanced, kernel=poly;, score=0.684 total time=   1.1s\n",
      "[CV 3/5] END C=1.0, class_weight=balanced, kernel=poly;, score=0.721 total time=   1.1s\n",
      "[CV 4/5] END C=1.0, class_weight=balanced, kernel=poly;, score=0.710 total time=   1.0s\n",
      "[CV 5/5] END C=1.0, class_weight=balanced, kernel=poly;, score=0.697 total time=   1.1s\n",
      "[CV 1/5] END C=1.0, class_weight=balanced, kernel=sigmoid;, score=0.440 total time=   0.8s\n",
      "[CV 2/5] END C=1.0, class_weight=balanced, kernel=sigmoid;, score=0.477 total time=   0.8s\n",
      "[CV 3/5] END C=1.0, class_weight=balanced, kernel=sigmoid;, score=0.446 total time=   1.0s\n",
      "[CV 4/5] END C=1.0, class_weight=balanced, kernel=sigmoid;, score=0.464 total time=   0.9s\n",
      "[CV 5/5] END C=1.0, class_weight=balanced, kernel=sigmoid;, score=0.467 total time=   0.9s\n",
      "[CV 1/5] END C=1.0, class_weight=None, kernel=rbf;, score=0.742 total time=   0.8s\n",
      "[CV 2/5] END C=1.0, class_weight=None, kernel=rbf;, score=0.729 total time=   0.8s\n",
      "[CV 3/5] END C=1.0, class_weight=None, kernel=rbf;, score=0.757 total time=   0.8s\n",
      "[CV 4/5] END C=1.0, class_weight=None, kernel=rbf;, score=0.740 total time=   0.8s\n",
      "[CV 5/5] END C=1.0, class_weight=None, kernel=rbf;, score=0.744 total time=   0.8s\n",
      "[CV 1/5] END C=1.0, class_weight=None, kernel=poly;, score=0.665 total time=   0.7s\n",
      "[CV 2/5] END C=1.0, class_weight=None, kernel=poly;, score=0.665 total time=   0.7s\n",
      "[CV 3/5] END C=1.0, class_weight=None, kernel=poly;, score=0.690 total time=   0.8s\n",
      "[CV 4/5] END C=1.0, class_weight=None, kernel=poly;, score=0.670 total time=   0.7s\n",
      "[CV 5/5] END C=1.0, class_weight=None, kernel=poly;, score=0.662 total time=   0.7s\n",
      "[CV 1/5] END C=1.0, class_weight=None, kernel=sigmoid;, score=0.541 total time=   0.6s\n",
      "[CV 2/5] END C=1.0, class_weight=None, kernel=sigmoid;, score=0.611 total time=   0.6s\n",
      "[CV 3/5] END C=1.0, class_weight=None, kernel=sigmoid;, score=0.581 total time=   0.5s\n",
      "[CV 4/5] END C=1.0, class_weight=None, kernel=sigmoid;, score=0.577 total time=   0.6s\n",
      "[CV 5/5] END C=1.0, class_weight=None, kernel=sigmoid;, score=0.574 total time=   0.5s\n",
      "[CV 1/5] END C=10.0, class_weight=balanced, kernel=rbf;, score=0.725 total time=   1.0s\n",
      "[CV 2/5] END C=10.0, class_weight=balanced, kernel=rbf;, score=0.700 total time=   1.1s\n",
      "[CV 3/5] END C=10.0, class_weight=balanced, kernel=rbf;, score=0.727 total time=   1.0s\n",
      "[CV 4/5] END C=10.0, class_weight=balanced, kernel=rbf;, score=0.710 total time=   1.0s\n",
      "[CV 5/5] END C=10.0, class_weight=balanced, kernel=rbf;, score=0.717 total time=   1.0s\n",
      "[CV 1/5] END C=10.0, class_weight=balanced, kernel=poly;, score=0.710 total time=   1.1s\n",
      "[CV 2/5] END C=10.0, class_weight=balanced, kernel=poly;, score=0.703 total time=   1.3s\n",
      "[CV 3/5] END C=10.0, class_weight=balanced, kernel=poly;, score=0.700 total time=   1.2s\n",
      "[CV 4/5] END C=10.0, class_weight=balanced, kernel=poly;, score=0.697 total time=   1.2s\n",
      "[CV 5/5] END C=10.0, class_weight=balanced, kernel=poly;, score=0.703 total time=   1.3s\n",
      "[CV 1/5] END C=10.0, class_weight=balanced, kernel=sigmoid;, score=0.424 total time=   0.8s\n",
      "[CV 2/5] END C=10.0, class_weight=balanced, kernel=sigmoid;, score=0.451 total time=   0.9s\n",
      "[CV 3/5] END C=10.0, class_weight=balanced, kernel=sigmoid;, score=0.430 total time=   0.7s\n",
      "[CV 4/5] END C=10.0, class_weight=balanced, kernel=sigmoid;, score=0.448 total time=   0.7s\n",
      "[CV 5/5] END C=10.0, class_weight=balanced, kernel=sigmoid;, score=0.443 total time=   0.8s\n",
      "[CV 1/5] END C=10.0, class_weight=None, kernel=rbf;, score=0.753 total time=   0.9s\n",
      "[CV 2/5] END C=10.0, class_weight=None, kernel=rbf;, score=0.732 total time=   0.9s\n",
      "[CV 3/5] END C=10.0, class_weight=None, kernel=rbf;, score=0.762 total time=   0.9s\n",
      "[CV 4/5] END C=10.0, class_weight=None, kernel=rbf;, score=0.746 total time=   0.9s\n",
      "[CV 5/5] END C=10.0, class_weight=None, kernel=rbf;, score=0.749 total time=   0.9s\n",
      "[CV 1/5] END C=10.0, class_weight=None, kernel=poly;, score=0.708 total time=   1.1s\n",
      "[CV 2/5] END C=10.0, class_weight=None, kernel=poly;, score=0.706 total time=   1.1s\n",
      "[CV 3/5] END C=10.0, class_weight=None, kernel=poly;, score=0.733 total time=   1.1s\n",
      "[CV 4/5] END C=10.0, class_weight=None, kernel=poly;, score=0.704 total time=   1.0s\n",
      "[CV 5/5] END C=10.0, class_weight=None, kernel=poly;, score=0.724 total time=   1.2s\n",
      "[CV 1/5] END C=10.0, class_weight=None, kernel=sigmoid;, score=0.541 total time=   0.5s\n",
      "[CV 2/5] END C=10.0, class_weight=None, kernel=sigmoid;, score=0.584 total time=   0.5s\n",
      "[CV 3/5] END C=10.0, class_weight=None, kernel=sigmoid;, score=0.553 total time=   0.5s\n",
      "[CV 4/5] END C=10.0, class_weight=None, kernel=sigmoid;, score=0.547 total time=   0.5s\n",
      "[CV 5/5] END C=10.0, class_weight=None, kernel=sigmoid;, score=0.563 total time=   0.5s\n",
      "The best validation score obtained is 0.74849 with\n",
      "\t{'C': 10.0, 'class_weight': None, 'kernel': 'rbf'}\n"
     ]
    }
   ],
   "source": [
    "gs_svc = best_svc(X_train, np.array(y_train_raw).ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ff0ae041",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "[CV 1/5] END class_weight=balanced, max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=100, random_state=0;, score=0.747 total time=   1.1s\n",
      "[CV 2/5] END class_weight=balanced, max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=100, random_state=0;, score=0.741 total time=   1.2s\n",
      "[CV 3/5] END class_weight=balanced, max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=100, random_state=0;, score=0.770 total time=   1.1s\n",
      "[CV 4/5] END class_weight=balanced, max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=100, random_state=0;, score=0.754 total time=   1.1s\n",
      "[CV 5/5] END class_weight=balanced, max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=100, random_state=0;, score=0.752 total time=   1.0s\n",
      "[CV 1/5] END class_weight=balanced, max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=200, random_state=0;, score=0.749 total time=   2.0s\n",
      "[CV 2/5] END class_weight=balanced, max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=200, random_state=0;, score=0.736 total time=   2.0s\n",
      "[CV 3/5] END class_weight=balanced, max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=200, random_state=0;, score=0.768 total time=   2.2s\n",
      "[CV 4/5] END class_weight=balanced, max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=200, random_state=0;, score=0.750 total time=   2.0s\n",
      "[CV 5/5] END class_weight=balanced, max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=200, random_state=0;, score=0.750 total time=   2.0s\n",
      "[CV 1/5] END class_weight=balanced, max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=300, random_state=0;, score=0.750 total time=   3.0s\n",
      "[CV 2/5] END class_weight=balanced, max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=300, random_state=0;, score=0.739 total time=   3.0s\n",
      "[CV 3/5] END class_weight=balanced, max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=300, random_state=0;, score=0.768 total time=   3.0s\n",
      "[CV 4/5] END class_weight=balanced, max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=300, random_state=0;, score=0.747 total time=   3.1s\n",
      "[CV 5/5] END class_weight=balanced, max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=300, random_state=0;, score=0.750 total time=   3.0s\n",
      "[CV 1/5] END class_weight=None, max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=100, random_state=0;, score=0.756 total time=   1.0s\n",
      "[CV 2/5] END class_weight=None, max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=100, random_state=0;, score=0.749 total time=   1.0s\n",
      "[CV 3/5] END class_weight=None, max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=100, random_state=0;, score=0.760 total time=   1.1s\n",
      "[CV 4/5] END class_weight=None, max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=100, random_state=0;, score=0.749 total time=   1.0s\n",
      "[CV 5/5] END class_weight=None, max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=100, random_state=0;, score=0.739 total time=   1.0s\n",
      "[CV 1/5] END class_weight=None, max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=200, random_state=0;, score=0.763 total time=   2.0s\n",
      "[CV 2/5] END class_weight=None, max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=200, random_state=0;, score=0.743 total time=   2.0s\n",
      "[CV 3/5] END class_weight=None, max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=200, random_state=0;, score=0.759 total time=   2.0s\n",
      "[CV 4/5] END class_weight=None, max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=200, random_state=0;, score=0.751 total time=   2.1s\n",
      "[CV 5/5] END class_weight=None, max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=200, random_state=0;, score=0.750 total time=   2.2s\n",
      "[CV 1/5] END class_weight=None, max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=300, random_state=0;, score=0.761 total time=   3.0s\n",
      "[CV 2/5] END class_weight=None, max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=300, random_state=0;, score=0.745 total time=   3.0s\n",
      "[CV 3/5] END class_weight=None, max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=300, random_state=0;, score=0.760 total time=   3.1s\n",
      "[CV 4/5] END class_weight=None, max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=300, random_state=0;, score=0.749 total time=   3.0s\n",
      "[CV 5/5] END class_weight=None, max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=300, random_state=0;, score=0.750 total time=   3.0s\n",
      "The best validation score obtained is 0.75298 with\n",
      "\t{'class_weight': None, 'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200, 'random_state': 0}\n"
     ]
    }
   ],
   "source": [
    "gs_random_forest = best_random_forest(X_train, np.array(y_train_raw).ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d0fd0816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "[CV 1/5] END .......................voting=hard;, score=0.761 total time=   6.3s\n",
      "[CV 2/5] END .......................voting=hard;, score=0.731 total time=   6.6s\n",
      "[CV 3/5] END .......................voting=hard;, score=0.753 total time=   6.6s\n",
      "[CV 4/5] END .......................voting=hard;, score=0.745 total time=   6.5s\n",
      "[CV 5/5] END .......................voting=hard;, score=0.748 total time=   5.9s\n",
      "[CV 1/5] END .......................voting=soft;, score=0.773 total time=   6.2s\n",
      "[CV 2/5] END .......................voting=soft;, score=0.757 total time=   6.5s\n",
      "[CV 3/5] END .......................voting=soft;, score=0.774 total time=   6.3s\n",
      "[CV 4/5] END .......................voting=soft;, score=0.766 total time=   5.9s\n",
      "[CV 5/5] END .......................voting=soft;, score=0.762 total time=   5.8s\n",
      "The best validation score obtained is 0.76666 with\n",
      "\t{'voting': 'soft'}\n"
     ]
    }
   ],
   "source": [
    "ensemble = best_ensemble([SVC(probability=True, **gs_svc.best_params_), RandomForestClassifier(**gs_random_forest.best_params_)], \n",
    "                         X_train, np.array(y_train_raw).ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "957c172a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ensemble\n",
    "sub_id = 8\n",
    "prediction = pd.DataFrame(model.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "221af8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_submission(sub_id, prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95891ad7",
   "metadata": {},
   "source": [
    "**Solutions must be submitted on the [project website](https://aml.ise.inf.ethz.ch/task2/).**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c8e3aa7",
   "metadata": {},
   "source": [
    "# `AML — Task 2:` Heart rhythm classification from raw ECG signals\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a018a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For TQDM :\n",
    "#! python3.6 -m pip install ipywidgets\n",
    "#! python3.6 -m pip install --upgrade jupyter\n",
    "#! jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f126d6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import biosppy.signals.ecg as ecg\n",
    "import biosppy.signals.tools as tools\n",
    "from biosppy.plotting import plot_ecg\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6dc16cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_selection import SelectFpr, SelectFdr, SelectFwe, f_classif, chi2, mutual_info_classif\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.linear_model import RidgeClassifier, LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier,\\\n",
    "                             AdaBoostClassifier, VotingClassifier, ExtraTreesClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b29de4",
   "metadata": {},
   "source": [
    "---\n",
    "## Dataset import and export to `csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acdf2170",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_from_csv(extension=\"\", drop_id = True):\n",
    "    X_train = pd.read_csv('data/X_train' + extension + '.csv')\n",
    "    y_train = pd.read_csv('data/y_train' + extension + '.csv')\n",
    "    X_test  = pd.read_csv('data/X_test' + extension + '.csv')\n",
    "    \n",
    "    if drop_id:\n",
    "        X_train = X_train.drop(columns=['id'])\n",
    "        y_train = y_train.drop(columns=['id'])\n",
    "        X_test  = X_test.drop(columns=['id'])\n",
    "     \n",
    "    return X_train, y_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "910612cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_to_csv(X_train, y_train, X_test, extension=\"_cleaned\"):\n",
    "    X_train.to_csv('data/X_train' + extension + '.csv', index=False)\n",
    "    y_train.to_csv('data/y_train' + extension + '.csv', index=False)\n",
    "    X_test.to_csv('data/X_test' + extension + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a707ea",
   "metadata": {},
   "source": [
    "## Submission export to `csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3acf5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_submission(sub_id, pred, basepath='submissions/task2-sub'):\n",
    "    result = pred.copy().rename(columns={0: 'y'})\n",
    "    result['id'] = range(0, len(result))\n",
    "    result = result[['id', 'y']]\n",
    "    result.to_csv(basepath + str(sub_id) + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ffbae4",
   "metadata": {},
   "source": [
    "---\n",
    "## Data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809894cd",
   "metadata": {},
   "source": [
    "### Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fdd8ab55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_stats_from(features_series: list) -> pd.Series:\n",
    "    # pd.describe().drop(count) returns 7 interesting features (min, max, std...)\n",
    "    fs = [pd.Series(s).describe().drop([\"count\"]) for s in features_series]\n",
    "    return pd.concat(fs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b98a4871",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_qpeaks_features(filtered: np.array, rpeaks: np.array, window_size=50):\n",
    "    qpeaks = [rpeak - window_size + np.argmin(filtered[rpeak-window_size:rpeak]) for rpeak in rpeaks]\n",
    "    qpeaks_amplitudes = [filtered[qpeak] for qpeak in qpeaks]\n",
    "    return qpeaks, qpeaks_amplitudes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11deacd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_speaks_features(filtered: np.array, rpeaks: np.array, window_size=50):\n",
    "    speaks = [rpeak + np.argmin(filtered[rpeak:rpeak+window_size]) for rpeak in rpeaks]\n",
    "    speaks_amplitudes = [filtered[speak] for speak in speaks]\n",
    "    return speaks, speaks_amplitudes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08f32d8",
   "metadata": {},
   "source": [
    "### Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d47da7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(time_series: pd.Series, sampling_rate=300) -> pd.Series:\n",
    "    # Drop nan values in the time series\n",
    "    no_nans = time_series.dropna()\n",
    "    \n",
    "    # Extract main features from ECG\n",
    "    ts, filtered, rpeaks, _, templates, _, heart_rate = ecg.ecg(no_nans, sampling_rate, show=False)\n",
    "    assert len(rpeaks) > 1, 'ECG cannot have a single R peak'\n",
    "    assert len(templates) > 1, 'ECG cannot have a single heartbeat'\n",
    "    \n",
    "    # Extract Q,R,S peak features\n",
    "    rpeaks_amplitude = [filtered[rpeak] for rpeak in rpeaks]\n",
    "    qpeaks, qpeaks_amplitude = extract_qpeaks_features(filtered, rpeaks)\n",
    "    speaks, speaks_amplitude = extract_speaks_features(filtered, rpeaks)\n",
    "    \n",
    "    # Extract RR, QRS durations features\n",
    "    rr_durations = [r2 - r1 for r1, r2 in zip(rpeaks, rpeaks[1:])]\n",
    "    qrs_durations = [speak - qpeak for qpeak, speak in zip(qpeaks, speaks)]\n",
    "    \n",
    "    # Extract QRS direction features\n",
    "    qrs_direction = [q + r + s for q, r, s in zip(qpeaks, rpeaks, speaks)]\n",
    "    \n",
    "    # Extract Q/R ratio features\n",
    "    qr_ratio = [q / r for q, r in zip(qpeaks, rpeaks)]\n",
    "    \n",
    "    #TODO: Extract SNR ratio (http://www.cinc.org/archives/2011/pdf/0609.pdf)\n",
    "    snr = np.quantile(np.std(templates, axis=0), 0.35)\n",
    "    \n",
    "    # Use this to go from index differences to seconds\n",
    "    index_to_time = ts[-1] / len(filtered)\n",
    "    # Extract pNN28 (https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1767394/)\n",
    "    pNN28 = (np.array(rr_durations) * index_to_time > 0.028).sum() / len(rr_durations)\n",
    "    \n",
    "    ecg_features = extract_stats_from([\n",
    "        rpeaks_amplitude, qpeaks_amplitude, speaks_amplitude,\n",
    "        rr_durations, qrs_durations, qrs_direction, qr_ratio\n",
    "    ])\n",
    "    \n",
    "    return ecg_features.append(pd.Series([snr, pNN28]), ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b98812",
   "metadata": {},
   "source": [
    "---\n",
    "## Data standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e31de0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_data(X_train, X_test):\n",
    "    scaler = StandardScaler().fit(X_train)\n",
    "    X_train_scaled = scaler.transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    return pd.DataFrame(X_train_scaled), pd.DataFrame(X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a616b1",
   "metadata": {},
   "source": [
    "---\n",
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6178196e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_constant_features(X_train, X_test, verbose=True):\n",
    "    non_constant_features_mask = X_train.apply(pd.Series.nunique) != 1\n",
    "    X_train_selected_features = X_train.loc[:, non_constant_features_mask]\n",
    "    X_test_selected_features = X_test.loc[:, non_constant_features_mask]\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"{X_train.shape[1]-X_train_selected_features.shape[1]} features removed because of constant values ({100*(X_train.shape[1]-X_train_selected_features.shape[1])/X_train.shape[1]:.2f}%).\")\n",
    "    \n",
    "    return X_train_selected_features, X_test_selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd321209",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_too_correlated_features(X_train, X_test, threshold=0.98, verbose=True):\n",
    "    X_train_corr_ = X_train.corr()\n",
    "\n",
    "    X_train_too_correlated = (X_train_corr_.mask(\n",
    "        np.tril(np.ones([len(X_train_corr_)]*2, dtype=bool))).abs() > threshold).any()\n",
    "    \n",
    "    X_train_selected_features = X_train.loc[:, (~X_train_too_correlated)]\n",
    "    X_test_selected_features = X_test.loc[:, (~X_train_too_correlated)]\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"{X_train.shape[1]-X_train_selected_features.shape[1]} features removed because of correlation with another feature > {threshold} ({100*(X_train.shape[1]-X_train_selected_features.shape[1])/X_train.shape[1]:.2f}%).\")\n",
    "\n",
    "    return X_train_selected_features, X_test_selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fcae7ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_useless_features(X_train, y_train, X_test, selector_type, stat, verbose=1):\n",
    "    # See https://stats.stackexchange.com/questions/328358/fpr-fdr-and-fwe-for-feature-selection\n",
    "    # and https://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection\n",
    "    \n",
    "    assert selector_type in [\"fpr\", \"fdr\", \"fwe\"], \"Unrecognised selector type\"\n",
    "    assert stat in [f_classif, chi2, mutual_info_classif], \"Unrecognised stat\"\n",
    "    selector = None\n",
    "    if selector_type == \"fpr\":\n",
    "        selector = SelectFpr(stat)\n",
    "    elif selector_type == \"fdr\":\n",
    "        selector = SelectFdr(stat)\n",
    "    elif selector_type == \"fwe\":\n",
    "        selector = SelectFwe(stat)\n",
    "    \n",
    "    selector.fit(X_train, y_train)\n",
    "    X_train_selected_features = pd.DataFrame(selector.transform(X_train))\n",
    "    X_test_selected_features = pd.DataFrame(selector.transform(X_test))\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"{X_train.shape[1]-X_train_selected_features.shape[1]} features removed because of complicated p-value stuff we don't understand ({100*(X_train.shape[1]-X_train_selected_features.shape[1])/X_train.shape[1]:.2f}%).\")\n",
    "        \n",
    "    return X_train_selected_features, X_test_selected_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfed9a4c",
   "metadata": {},
   "source": [
    "---\n",
    "## Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4891314a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(model, params, X_train, y_train, clear=False):\n",
    "    gs = GridSearchCV(model, params, cv=5, verbose=3, scoring='f1_micro', error_score='raise')\n",
    "    gs.fit(X_train, y_train)\n",
    "    \n",
    "    if clear:\n",
    "        clear_output(wait=True)\n",
    "    print(f\"{type(gs.best_estimator_).__name__} best validation score is {gs.best_score_:.5f} +- {gs.cv_results_['std_test_score'][gs.best_index_]:.5f},\\nobtained with {gs.best_params_}\")\n",
    "    \n",
    "    return gs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436814d8",
   "metadata": {},
   "source": [
    "---\n",
    "## Main Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358f9688",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2dbf8612",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_raw, y_train_raw, X_test_raw = load_from_csv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b44965",
   "metadata": {},
   "source": [
    "### Extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "080216a8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4eed9e59d1f84881bcac9ae46f221c79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5117 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_train = X_train_raw.progress_apply(extract_features, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e22045f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4d93f8a64114abd92888a2960ba4707",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3411 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_test = X_test_raw.progress_apply(extract_features, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c5895c58",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train has 0 null values.\n",
      "X_test has 0 null values.\n"
     ]
    }
   ],
   "source": [
    "print(f\"X_train has {X_train.isna().sum().sum()} null values.\")\n",
    "print(f\"X_test has {X_test.isna().sum().sum()} null values.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4c9e8af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(y_train_raw).ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ffc823",
   "metadata": {},
   "source": [
    "### Standardize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "49458a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = standardize_data(X_train, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb48401e",
   "metadata": {},
   "source": [
    "### Select features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "60abe361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 features removed because of constant values (0.00%).\n"
     ]
    }
   ],
   "source": [
    "X_train_1, X_test_1 = remove_constant_features(X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "17548772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 features removed because of correlation with another feature > 0.98 (15.69%).\n"
     ]
    }
   ],
   "source": [
    "X_train_2, X_test_2 = remove_too_correlated_features(X_train_1, X_test_1, threshold=0.98)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "74ac5095",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 features removed because of complicated p-value stuff we don't understand (4.65%).\n"
     ]
    }
   ],
   "source": [
    "X_train_3, X_test_3 = remove_useless_features(X_train_2, y_train, X_test_2, selector_type=\"fpr\", stat=f_classif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34218305",
   "metadata": {},
   "source": [
    "### Train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8aef3c96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC best validation score is 0.76256 +- 0.00560,\n",
      "obtained with {'C': 10, 'class_weight': None, 'kernel': 'rbf', 'random_state': 0}\n"
     ]
    }
   ],
   "source": [
    "gs_svc = grid_search(SVC(),\n",
    "                     {\n",
    "                         \"kernel\": [\"rbf\", \"poly\"], #[\"rbf\", \"poly\", \"sigmoid\"],\n",
    "                         \"C\": [1, 10, 50, 100],\n",
    "                         \"class_weight\": [\"balanced\", None],\n",
    "                         \"random_state\": [0],\n",
    "                     },\n",
    "                     X_train_3,\n",
    "                     y_train,\n",
    "                     clear=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ff0ae041",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier best validation score is 0.78132 +- 0.01226,\n",
      "obtained with {'class_weight': 'balanced', 'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 4, 'n_estimators': 100, 'random_state': 0}\n"
     ]
    }
   ],
   "source": [
    "gs_random_forest = grid_search(RandomForestClassifier(),\n",
    "                               {\n",
    "                                   \"n_estimators\": np.arange(100, 300, 200),\n",
    "                                   \"max_depth\": [None], #np.arange(2, 8, 1),\n",
    "                                   \"min_samples_split\": [2, 4], #np.arange(2, 8, 1),\n",
    "                                   \"min_samples_leaf\": [1, 4], #np.arange(1, 9, 2),\n",
    "                                   \"class_weight\": [\"balanced\", None],\n",
    "                                   \"random_state\": [0], \n",
    "                               },\n",
    "                               X_train_3,\n",
    "                               y_train,\n",
    "                               clear=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4d4cce3d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoostingClassifier best validation score is 0.78953 +- 0.00390,\n",
      "obtained with {'criterion': 'squared_error', 'learning_rate': 0.1, 'loss': 'deviance', 'min_samples_leaf': 3, 'min_samples_split': 4, 'n_estimators': 200, 'n_iter_no_change': None, 'subsample': 0.7, 'tol': 0.0001}\n"
     ]
    }
   ],
   "source": [
    "gs_gbc = grid_search(GradientBoostingClassifier(),\n",
    "                     {\n",
    "                         \"loss\": [\"deviance\"],\n",
    "                         \"learning_rate\": [0.1, 1],\n",
    "                         \"n_estimators\": [50, 200],\n",
    "                         \"subsample\": [0.7, 1],\n",
    "                         \"criterion\": [\"squared_error\"], #[\"friedman_mse\", \"mse\"],\n",
    "                         \"min_samples_split\": [4], #[2, 4],\n",
    "                         \"min_samples_leaf\": [3], #[1, 3],\n",
    "                         \"n_iter_no_change\": [None],\n",
    "                         \"tol\": [1e-4],\n",
    "                     },\n",
    "                     X_train_3, \n",
    "                     y_train,\n",
    "                     clear=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99dbe220",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# No predict_proba\n",
    "gs_knn = grid_search(KNeighborsClassifier(),\n",
    "                     {\n",
    "                         \"n_neighbors\": np.arange(2, 10, 1),\n",
    "                         \"weights\": [\"uniform\", \"distance\"],\n",
    "                     },\n",
    "                     X_train, \n",
    "                     y_train,\n",
    "                     clear=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f68e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes 1min to train a single instance, but scores of ~0.7...\n",
    "gs_gp = grid_search(GaussianProcessClassifier(),\n",
    "                    {\n",
    "                        \"kernel\": [None],\n",
    "                        \"multi_class\": [\"one_vs_rest\", \"one_vs_one\"],\n",
    "                        \"random_state\": [0],\n",
    "                    },\n",
    "                    X_train, \n",
    "                    y_train,\n",
    "                    clear=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b160fba4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier best validation score is 0.72210 +- 0.00570,\n",
      "obtained with {'base_estimator': None, 'learning_rate': 0.1, 'n_estimators': 300, 'random_state': 0}\n"
     ]
    }
   ],
   "source": [
    "gs_ab = grid_search(AdaBoostClassifier(),\n",
    "                    {\n",
    "                        \"base_estimator\": [None],\n",
    "                        \"n_estimators\": [50, 300],\n",
    "                        \"learning_rate\": [0.1, 1.0],\n",
    "                        \"random_state\": [0],\n",
    "                    },\n",
    "                    X_train_3, \n",
    "                    y_train,\n",
    "                    clear=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ac8fc3ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExtraTreesClassifier best validation score is 0.77565 +- 0.00596,\n",
      "obtained with {'bootstrap': False, 'class_weight': 'balanced', 'criterion': 'gini', 'n_estimators': 500, 'random_state': 0}\n"
     ]
    }
   ],
   "source": [
    "gs_etc = grid_search(ExtraTreesClassifier(),\n",
    "                    {\n",
    "                        \"n_estimators\": [50, 100, 500],\n",
    "                        \"criterion\": [\"gini\", \"entropy\"],\n",
    "                        \"bootstrap\": [True, False],\n",
    "                        \"class_weight\": [\"balanced\", \"balanced_subsample\"],\n",
    "                        \"random_state\": [0],\n",
    "                    },\n",
    "                    X_train_3, \n",
    "                    y_train,\n",
    "                    clear=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b183697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No predict_proba\n",
    "gs_ridge = grid_search(RidgeClassifier(),\n",
    "                       {\n",
    "                           \"alpha\": [0.1, 1, 5],\n",
    "                           \"class_weight\": [\"balanced\", None],\n",
    "                           \"random_state\": [0],\n",
    "                       },\n",
    "                       X_train,\n",
    "                       y_train,\n",
    "                       clear=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d0001a07",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression best validation score is 0.69064 +- 0.00282,\n",
      "obtained with {'C': 10, 'class_weight': None, 'max_iter': 1000, 'penalty': 'l2', 'random_state': 0}\n"
     ]
    }
   ],
   "source": [
    "gs_logreg = grid_search(LogisticRegression(),\n",
    "                        {\n",
    "                            \"penalty\": [\"l2\"],\n",
    "                            \"C\": [0.1, 1, 10],\n",
    "                            \"class_weight\": [\"balanced\", None],\n",
    "                            \"random_state\": [0],\n",
    "                            \"max_iter\": [1000]\n",
    "                        },\n",
    "                        X_train_3,\n",
    "                        y_train,\n",
    "                        clear=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a8b0edf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier best validation score is 0.74458 +- 0.00432,\n",
      "obtained with {'alpha': 0.0001, 'early_stopping': True, 'hidden_layer_sizes': (100, 30, 10), 'random_state': 0, 'solver': 'adam'}\n"
     ]
    }
   ],
   "source": [
    "gs_mlp = grid_search(MLPClassifier(),\n",
    "                     {\n",
    "                         \"hidden_layer_sizes\": [(100,), (40, 40, 30, 10), (40, 40), (100, 30, 10)],\n",
    "                         \"alpha\": [0.0001],\n",
    "                         \"solver\": [\"adam\"],\n",
    "                         \"early_stopping\": [True],\n",
    "                         \"random_state\": [0],\n",
    "                     },\n",
    "                     X_train_3,\n",
    "                     y_train,\n",
    "                     clear=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "86a2ec25",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VotingClassifier best validation score is 0.79324 +- 0.00911,\n",
      "obtained with {'voting': 'soft'}\n"
     ]
    }
   ],
   "source": [
    "# Only kept models with a predict proba function for soft voting + > 0.75 val score\n",
    "gs_ensemble = grid_search(VotingClassifier([('svc', SVC(probability=True, **gs_svc.best_params_)),\n",
    "                                            ('rf', RandomForestClassifier(**gs_random_forest.best_params_)),\n",
    "                                            ('gbc', GradientBoostingClassifier(**gs_gbc.best_params_)),\n",
    "                                            ('etc', ExtraTreesClassifier(**gs_etc.best_params_))]),\n",
    "                          {\n",
    "                              \"voting\": [\"soft\"], #[\"hard\", \"soft\"],\n",
    "                          },\n",
    "                          X_train_3, \n",
    "                          y_train,\n",
    "                          clear=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd4f19a",
   "metadata": {},
   "source": [
    "---\n",
    "## Generate new submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "957c172a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gs_ensemble\n",
    "sub_id = 14\n",
    "prediction = pd.DataFrame(model.predict(X_test_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "221af8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_submission(sub_id, prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95891ad7",
   "metadata": {},
   "source": [
    "**Solutions must be submitted on the [project website](https://aml.ise.inf.ethz.ch/task2/).**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

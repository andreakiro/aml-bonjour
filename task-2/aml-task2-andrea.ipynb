{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c8e3aa7",
   "metadata": {},
   "source": [
    "# `AML â€” Task 2:` Heart rhythm classification from raw ECG signals\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988c0a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For TQDM :\n",
    "#! python3.6 -m pip install ipywidgets\n",
    "#! python3.6 -m pip install --upgrade jupyter\n",
    "#! jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f126d6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import biosppy.signals.ecg as ecg\n",
    "import biosppy.signals.tools as tools\n",
    "from biosppy.plotting import plot_ecg\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc16cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b29de4",
   "metadata": {},
   "source": [
    "---\n",
    "## Dataset import and export to `csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acdf2170",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_from_csv(extension=\"\", drop_id = True):\n",
    "    X_train = pd.read_csv('data/X_train' + extension + '.csv')\n",
    "    y_train = pd.read_csv('data/y_train' + extension + '.csv')\n",
    "    X_test  = pd.read_csv('data/X_test' + extension + '.csv')\n",
    "    \n",
    "    if drop_id:\n",
    "        X_train = X_train.drop(columns=['id'])\n",
    "        y_train = y_train.drop(columns=['id'])\n",
    "        X_test  = X_test.drop(columns=['id'])\n",
    "     \n",
    "    return X_train, y_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910612cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_to_csv(X_train, y_train, X_test, extension=\"_cleaned\"):\n",
    "    X_train.to_csv('data/X_train' + extension + '.csv', index=False)\n",
    "    y_train.to_csv('data/y_train' + extension + '.csv', index=False)\n",
    "    X_test.to_csv('data/X_test' + extension + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a707ea",
   "metadata": {},
   "source": [
    "## Submission export to `csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3acf5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_submission(sub_id, pred, basepath='submissions/task2-sub'):\n",
    "    result = pred.copy().rename(columns={0: 'y'})\n",
    "    result['id'] = range(0, len(result))\n",
    "    result = result[['id', 'y']]\n",
    "    result.to_csv(basepath + str(sub_id) + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ffbae4",
   "metadata": {},
   "source": [
    "---\n",
    "## Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef828a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def series_to_heartbeats(time_series: pd.Series, sampling_rate=300.0) -> np.array:\n",
    "    no_nans = time_series.dropna()\n",
    "    rpeaks = ecg.engzee_segmenter(no_nans, sampling_rate)['rpeaks']\n",
    "    beats, rpeaks = ecg.extract_heartbeats(no_nans, rpeaks, sampling_rate)\n",
    "    beats = beats if len(beats.shape) == 2 else beats.reshape((1, -1))\n",
    "    return beats, rpeaks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07e4ffe",
   "metadata": {},
   "source": [
    "### Feature extraction methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6bd60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_rpeak_features(filtered: np.array, rpeaks: np.array):\n",
    "    rpeaks_amplitudes = [filtered[rpeak] for rpeak in rpeaks]\n",
    "    rpeaks_mean = np.mean(rpeaks_amplitudes)\n",
    "    rpeaks_std = np.std(rpeaks_amplitudes)\n",
    "    return rpeaks, rpeaks_amplitudes, rpeaks_mean, rpeaks_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98a4871",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_qpeak_features(filtered: np.array, rpeaks: np.array, window_size=50):\n",
    "    qpeaks = [rpeak - window_size + np.argmin(filtered[rpeak-window_size:rpeak]) for rpeak in rpeaks]\n",
    "    qpeaks_amplitudes = [filtered[qpeak] for qpeak in qpeaks]\n",
    "    qpeaks_mean = np.mean(qpeaks_amplitudes)\n",
    "    qpeaks_std = np.std(qpeaks_amplitudes)\n",
    "    return qpeaks, qpeaks_amplitudes, qpeaks_mean, qpeaks_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11deacd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_speak_features(filtered: np.array, rpeaks: np.array, window_size=50):\n",
    "    speaks = [rpeak + np.argmin(filtered[rpeak:rpeak+window_size]) for rpeak in rpeaks]\n",
    "    speaks_amplitudes = [filtered[speak] for speak in speaks]\n",
    "    speaks_mean = np.mean(speaks_amplitudes)\n",
    "    speaks_std = np.std(speaks_amplitudes)\n",
    "    return speaks, speaks_amplitudes, speaks_mean, speaks_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48ba4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_qrs_durations_features(qpeaks: np.array, speaks: np.array):\n",
    "    qrs_durations = [speak - qpeak for qpeak, speak in zip(qpeaks, speaks)]\n",
    "    qrs_durations_mean = np.mean(qrs_durations)\n",
    "    qrs_durations_std = np.std(qrs_durations)\n",
    "    return qrs_durations, qrs_durations_mean, qrs_durations_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd135ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: maybe change durations to seconds?\n",
    "def extract_rr_durations_features(rpeaks: np.array):\n",
    "    rr_durations = [r2 - r1 for r1, r2 in zip(rpeaks, rpeaks[1:])]\n",
    "    rr_durations_mean = np.mean(rr_durations)\n",
    "    rr_durations_std = np.std(rr_durations)\n",
    "    return rr_durations, rr_durations_mean, rr_durations_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32f01aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_heart_rate_features(heart_rate: np.array):\n",
    "    heart_rate_mean = np.mean(heart_rate)\n",
    "    heart_rate_std = np.std(heart_rate)\n",
    "    return heart_rate_mean, heart_rate_std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bdc0f42",
   "metadata": {},
   "source": [
    "### Main feature extraction method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3faf2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(time_series: pd.Series, sampling_rate=300) -> np.array:\n",
    "    # Drop nan values in the time series\n",
    "    no_nans = time_series.dropna()\n",
    "    \n",
    "    # Extract main features\n",
    "    ts, filtered, rpeaks, _, templates, _, heart_rate = ecg.ecg(no_nans, sampling_rate, show=False)\n",
    "    assert len(rpeaks) > 1, 'ECG cannot have a single R peak'\n",
    "    assert len(templates) > 1, 'ECG cannot have a single heartbeat'\n",
    "    \n",
    "    # Extract Q,R,S peak features\n",
    "    speaks, qpeaks_amplitudes, qpeaks_mean, qpeaks_std = extract_qpeak_features(filtered, rpeaks)\n",
    "    rpeaks, rpeaks_amplitudes, rpeaks_mean, rpeaks_std = extract_rpeak_features(filtered, rpeaks)\n",
    "    qpeaks, speaks_amplitudes, speaks_mean, speaks_std = extract_speak_features(filtered, rpeaks)\n",
    "    \n",
    "    # Extract RR, QRS durations features\n",
    "    rr_durations, rr_durations_mean, rr_durations_std = extract_rr_durations_features(rpeaks)\n",
    "    qrs_durations, qrs_durations_mean, qrs_durations_std = extract_qrs_durations_features(qpeaks, speaks)\n",
    "    \n",
    "    # Extract heart rate features\n",
    "    if len(heart_rate) == 0:\n",
    "        heart_rate = rr_durations #Â temp fix\n",
    "    heart_rate_mean, heart_rate_std = extract_heart_rate_features(heart_rate)\n",
    "    \n",
    "    #TODO: Extract SNR ratio (http://www.cinc.org/archives/2011/pdf/0609.pdf)\n",
    "    snr = np.quantile(np.std(templates, axis=0), 0.35)\n",
    "    \n",
    "    # Use this to go from index differences to seconds\n",
    "    index_to_time = ts[-1] / len(filtered)\n",
    "    # Extract pNN28 (https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1767394/)\n",
    "    pNN28 = (np.array(rr_durations) * index_to_time > 0.028).sum() / len(rr_durations)\n",
    "    \n",
    "    # Return extracted features\n",
    "    return pd.Series([rpeaks_mean, \n",
    "                      rpeaks_std,\n",
    "                      rr_durations_mean, \n",
    "                      rr_durations_std, \n",
    "                      heart_rate_mean, \n",
    "                      heart_rate_std, \n",
    "                      snr,\n",
    "                      speaks_mean,\n",
    "                      speaks_std,\n",
    "                      qpeaks_mean,\n",
    "                      qpeaks_std,\n",
    "                      qrs_durations_mean,\n",
    "                      qrs_durations_std,\n",
    "                      pNN28,])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b98812",
   "metadata": {},
   "source": [
    "---\n",
    "##Â Data standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e31de0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_data(X_train, X_test):\n",
    "    scaler = StandardScaler().fit(X_train)\n",
    "    X_train_scaled = scaler.transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    return (X_train_scaled, X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfed9a4c",
   "metadata": {},
   "source": [
    "---\n",
    "## Models defintions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f886d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def svc(X_train, y_train):\n",
    "    svc = SVC()\n",
    "    gs_svc_params = {\n",
    "        \"kernel\": [\"rbf\", \"poly\", \"sigmoid\"],\n",
    "        \"C\": np.logspace(0, 1, 2),\n",
    "        \"class_weight\": [\"balanced\", None]\n",
    "    }\n",
    "    gs_svc = GridSearchCV(svc, gs_svc_params, cv=5, verbose=3, scoring='f1_micro', error_score='raise')\n",
    "    gs_svc.fit(X_train, y_train)\n",
    "    \n",
    "    print(f\"The best validation score obtained is {gs_svc.best_score_:.5f} +- \\\n",
    "        {gs_svc.cs_results_['std_test_score'][gs_svc.best_index_]} with\\n\\t{gs_svc.best_params_}\")\n",
    "    \n",
    "    return gs_svc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16344ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest(X_train, y_train):\n",
    "    random_forest = RandomForestClassifier()\n",
    "    gs_forest_params = {\n",
    "     \"n_estimators\": np.arange(100, 400, 100),\n",
    "     \"max_depth\": [None], #np.arange(2, 8, 1),\n",
    "     \"min_samples_split\": [2], #np.arange(2, 8, 1),\n",
    "     \"min_samples_leaf\": [1], #np.arange(1, 9, 2),\n",
    "     \"class_weight\": [\"balanced\", None],\n",
    "     \"random_state\": [0], \n",
    "    }\n",
    "    \n",
    "    gs_forest = GridSearchCV(random_forest, gs_forest_params, cv=5, verbose=3, \n",
    "                             scoring='f1_micro', error_score='raise')\n",
    "    \n",
    "    gs_forest.fit(X_train, y_train)\n",
    "\n",
    "    print(f\"The best validation score obtained is {gs_forest.best_score_:.5f} +- \\\n",
    "        {gs_forest.cs_results_['std_test_score'][gs_forest.best_index_]} with\\n\\t{gs_forest.best_params_}\")\n",
    "    \n",
    "    return gs_forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3c1fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gbc(X_train, y_train):\n",
    "    gbc = GradientBoostingClassifier()\n",
    "    gs_gbc_params = {\n",
    "        \"loss\": [\"deviance\"],\n",
    "        \"learning_rate\": [0.1], #Â TODO\n",
    "        \"n_estimators\": [100], #np.arange(100, 400, 100),\n",
    "        \"subsample\": [1], # TODO\n",
    "        \"criterion\": [\"friedman_mse\"],\n",
    "        \"min_samples_split\": [2], #Â TODO\n",
    "        \"min_samples_leaf\": [1], #Â TODO\n",
    "        \"n_iter_no_change\": [None], # TODO\n",
    "        \"tol\": [1e-4], #Â TODO\n",
    "    }\n",
    "    \n",
    "    gs_gbc = GridSearchCV(gbc, gs_gbc_params, cv=5, verbose=3, scoring='f1_micro', error_score='raise')\n",
    "    gs_gbc.fit(X_train, y_train)\n",
    "    \n",
    "    print(f\"The best validation score obtained is {gs_gbc.best_score_:.5f} +- \\\n",
    "        {gs_gbc.cs_results_['std_test_score'][gs_gbc.best_index_]} with\\n\\t{gs_gbc.best_params_}\")\n",
    "    \n",
    "    return gs_gbc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10598c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble(models, X_train, y_train):\n",
    "    ensemble = VotingClassifier(estimators=[(str(i), model) for i, model in enumerate(models)])\n",
    "    gs_ensemble_params = {\n",
    "     \"voting\": [\"hard\", \"soft\"]\n",
    "    }\n",
    "    \n",
    "    gs_ensemble = GridSearchCV(ensemble, gs_ensemble_params, cv=5, verbose=3, \n",
    "                               scoring='f1_micro', error_score='raise')\n",
    "    \n",
    "    gs_ensemble.fit(X_train, y_train)\n",
    "\n",
    "    print(f\"The best validation score obtained is {gs_ensemble.best_score_:.5f} +- \\\n",
    "        {gs_ensemble.cs_results_['std_test_score'][gs_ensemble.best_index_]} with\\n\\t{gs_ensemble.best_params_}\")\n",
    "    \n",
    "    return gs_ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436814d8",
   "metadata": {},
   "source": [
    "---\n",
    "## Main Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358f9688",
   "metadata": {},
   "source": [
    "#### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbf8612",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_raw, y_train_raw, X_test_raw = load_from_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f98064b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_raw.isna().sum(axis='columns').hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe35dd68",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(np.mean(series_to_heartbeats(X_train_raw.iloc[0])[0], axis=0)).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b44965",
   "metadata": {},
   "source": [
    "####Â Extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080216a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train = X_train_raw.progress_apply(extract_features, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22045f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test_raw.progress_apply(extract_features, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5895c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"X_train has {X_train.isna().sum().sum()} null values.\")\n",
    "print(f\"X_test has {X_test.isna().sum().sum()} null values.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ffc823",
   "metadata": {},
   "source": [
    "#### Standardize dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49458a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = standardize_data(X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aef3c96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gs_svc = svc(X_train, np.array(y_train_raw).ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0ae041",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gs_random_forest = random_forest(X_train, np.array(y_train_raw).ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4cce3d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "gs_gbc = gbc(X_train, np.array(y_train_raw).ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a2ec25",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gs_ensemble = ensemble([\n",
    "    SVC(probability=True, **gs_svc.best_params_), \n",
    "    RandomForestClassifier(**gs_random_forest.best_params_),\n",
    "    GradientBoostingClassifier(**gs_gbc.best_params_)\n",
    "], X_train, np.array(y_train_raw).ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd4f19a",
   "metadata": {},
   "source": [
    "---\n",
    "## Generate new submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957c172a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gs_ensemble\n",
    "sub_id = 10\n",
    "prediction = pd.DataFrame(model.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221af8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create_submission(sub_id, prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95891ad7",
   "metadata": {},
   "source": [
    "**Solutions must be submitted on the [project website](https://aml.ise.inf.ethz.ch/task2/).**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

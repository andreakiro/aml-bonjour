{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c167fc74",
   "metadata": {},
   "source": [
    "# AML — Task 1\n",
    "## Predict the age of a brain from MRI features\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ee9168",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import zscore\n",
    "from pandas_profiling import ProfileReport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5e53aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import SimpleImputer, KNNImputer, IterativeImputer\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, SelectPercentile, f_regression\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, cross_val_score\n",
    "from sklearn.ensemble import IsolationForest, GradientBoostingRegressor, AdaBoostRegressor, RandomForestRegressor\n",
    "from sklearn.neighbors import LocalOutlierFactor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fcf6c09",
   "metadata": {},
   "source": [
    "## Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d3d302",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_raw_data():\n",
    "    X_train = pd.read_csv('data/X_train.csv').drop(columns=['id'])\n",
    "    y_train = pd.read_csv('data/y_train.csv').drop(columns=['id'])\n",
    "    X_test = pd.read_csv('data/X_test.csv').drop(columns=['id'])\n",
    "    return X_train, y_train, X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6ce6cd",
   "metadata": {},
   "source": [
    "## Export dataset to `csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea13a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_to_csv(X_train_cleaned, y_train_cleaned, X_test_cleaned):\n",
    "    X_train_cleaned.to_csv('data/X_train_cleaned.csv', index=False)\n",
    "    y_train_cleaned.to_csv('data/y_train_cleaned.csv', index=False)\n",
    "    X_test_cleaned.to_csv('data/X_test_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6565ca",
   "metadata": {},
   "source": [
    "## Export `csv` submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f30a346",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_submission(prediction, sub_id, basepath = 'submissions/task1-sub'):\n",
    "    result = prediction.copy()\n",
    "    result = result.rename(columns={0: 'y'})\n",
    "    result['id'] = range(0, len(result))\n",
    "    result = result[['id', 'y']]\n",
    "    result.to_csv(basepath+str(sub_id) + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d950ba6",
   "metadata": {},
   "source": [
    "## Outlier detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8046b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers(X_train, y_train, contamination='auto', verbose=1):\n",
    "    \"\"\"\n",
    "    Remove the ouliers from our dataset. Temporarily replace the nan values by \n",
    "    the median to perform the outlier detection.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train : pd.df\n",
    "        The features (what we will use to see the outliers)\n",
    "    y_train : pd.df\n",
    "        The labels\n",
    "    contamination : int, optional\n",
    "        The percent of outliers found by the isolation forest if it is used.\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    (pd.df, pd.df)\n",
    "        The data with the outliers rows removed\n",
    "    \"\"\"\n",
    "    # Save a mask of the imputed values to be able to redo the imputation once the outlier detection is done\n",
    "    X_train_null_mask = X_train.isna()\n",
    "    \n",
    "    # Need to impute nan values for the outlier detection to work (cannot deal with nan)\n",
    "    X_train_imputed = pd.DataFrame(SimpleImputer(strategy=\"median\", verbose=verbose).fit_transform(X_train))\n",
    "    \n",
    "    #clf = IsolationForest(contamination=contamination, random_state=0) # modify here\n",
    "    clf = LocalOutlierFactor(contamination=contamination) # modify here\n",
    "    outliers_mask = pd.Series(clf.fit_predict(X_train_imputed))\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Detected {(outliers_mask == -1).sum()} outliers, out of {outliers_mask.shape[0]} samples ({100 * (outliers_mask == -1).sum() / outliers_mask.shape[0]:.2f}%).\")\n",
    "    \n",
    "    # Put back the nan values\n",
    "    # convert the null mask to np.array so it is correctly applied since X_train indexes have changed\n",
    "    X_train_no_outliers = X_train_imputed.mask(np.array(X_train_null_mask))\n",
    "    \n",
    "    # Remove outliers from the training set\n",
    "    X_train_no_outliers = X_train_no_outliers.loc[outliers_mask == 1, :]\n",
    "    y_train_no_outliers = y_train.loc[outliers_mask == 1, :]\n",
    "    \n",
    "    return (X_train_no_outliers, y_train_no_outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e9ed3f",
   "metadata": {},
   "source": [
    "## Data scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae923af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale(X_train, X_test):\n",
    "    # Do the scaling, saving the scaler to use it for X_test too. No need for imputation, just ignore nan values.\n",
    "    scaler = StandardScaler().fit(X_train)\n",
    "    X_train_scaled = pd.DataFrame(scaler.transform(X_train))\n",
    "    # Cast X_test to np.array to avoid warning of model trained without feature names but X having some.\n",
    "    X_test_scaled = pd.DataFrame(scaler.transform(np.array(X_test)))\n",
    "    return (X_train_scaled, X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1eb8d49",
   "metadata": {},
   "source": [
    "## Data imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3f2ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_values(X_train, X_test, method='knn', max_iter=15):\n",
    "    print(f\"For the train dataset, there are {np.array(X_train.isna()).sum().sum()} nan values, out of {X_train.shape[0]*X_train.shape[1]} ({100*np.array(X_train.isna()).sum().sum()/(X_train.shape[0]*X_train.shape[1]):.2f}%).\")\n",
    "    \n",
    "    imputer = None\n",
    "    if method == 'knn':\n",
    "        imputer = KNNImputer(n_neighbors=6, weights='uniform').fit(X_train)\n",
    "    elif method == 'iterative':\n",
    "        # Runs VERY slowly\n",
    "        imputer = IterativeImputer(random_state=0, max_iter=max_iter, verbose=2).fit(X_train)\n",
    "    \n",
    "    X_train_imputed = pd.DataFrame(imputer.transform(X_train))\n",
    "    X_test_imputed = pd.DataFrame(imputer.transform(X_test))\n",
    "    return (X_train_imputed, X_test_imputed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6611b8",
   "metadata": {},
   "source": [
    "## Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bccfe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_constant_features(X_train, X_test, verbose=1):\n",
    "    non_constant_features_mask = X_train.apply(pd.Series.nunique) != 1\n",
    "    X_train_selected_features = X_train.loc[:, non_constant_features_mask]\n",
    "    X_test_selected_features = X_test.loc[:, non_constant_features_mask]\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"{X_train.shape[1]-X_train_selected_features.shape[1]} features removed because of constant values ({100*(X_train.shape[1]-X_train_selected_features.shape[1])/X_train.shape[1]:.2f}%).\")\n",
    "    \n",
    "    return X_train_selected_features, X_test_selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50af162",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_too_correlated_features(X_train, X_test, threshold=0.98, verbose=1):\n",
    "    X_train_corr_ = X_train.corr()\n",
    "\n",
    "    X_train_too_correlated = (X_train_corr_.mask(\n",
    "        np.tril(np.ones([len(X_train_corr_)]*2, dtype=bool))).abs() > threshold).any()\n",
    "    \n",
    "    X_train_selected_features = X_train.loc[:, (~X_train_too_correlated)]\n",
    "    X_test_selected_features = X_test.loc[:, (~X_train_too_correlated)]\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"{X_train.shape[1]-X_train_selected_features.shape[1]} features removed because of correlation with another feature > {threshold} ({100*(X_train.shape[1]-X_train_selected_features.shape[1])/X_train.shape[1]:.2f}%).\")\n",
    "\n",
    "    return X_train_selected_features, X_test_selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88d6ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_random_features(X_train, y_train, X_test, Xtrm=None, Xtem=None, k=190, verbose=1):\n",
    "    selector = SelectKBest(f_regression, k=k) # modify here\n",
    "    selector.fit(X_train, np.array(y_train).ravel())\n",
    "    X_train_selected_features = pd.DataFrame(selector.transform(X_train))\n",
    "    X_test_selected_features = pd.DataFrame(selector.transform(X_test))\n",
    "    if Xtrm is not None:\n",
    "        Xtrm = pd.DataFrame(selector.transform(Xtrm))\n",
    "    if Xtem is not None:\n",
    "        Xtem = pd.DataFrame(selector.transform(Xtem))\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"{X_train.shape[1]-X_train_selected_features.shape[1]} features removed because of low correlation with target ({100*(X_train.shape[1]-X_train_selected_features.shape[1])/X_train.shape[1]:.2f}%).\")\n",
    "        \n",
    "    return X_train_selected_features, X_test_selected_features, Xtrm, Xtem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957f39c2",
   "metadata": {},
   "source": [
    "## Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7441e1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data():\n",
    "    print(\"Loading raw data...\")\n",
    "    X_train, y_train, X_test = load_raw_data()\n",
    "\n",
    "    print(\"Removing outliers...\")\n",
    "    X_train, y_train = remove_outliers(X_train, y_train)\n",
    "\n",
    "    print(\"Scaling data...\")\n",
    "    X_train, X_test = scale(X_train, X_test)\n",
    "\n",
    "    print(\"Selecting features...\")\n",
    "    X_train, X_test = remove_constant_features(X_train, X_test)\n",
    "    X_train, X_test = remove_too_correlated_features(X_train, X_test, threshold=0.98)\n",
    "\n",
    "    X_train_mask = X_train.isna()\n",
    "    X_test_mask = X_test.isna()\n",
    "    X_train, X_test = impute_values(X_train, X_test, method='knn')\n",
    "    X_train, X_test, X_train_mask, X_test_mask = remove_random_features(\n",
    "        X_train, y_train, X_test, Xtrm=X_train_mask, Xtem=X_test_mask, k=190)\n",
    "    X_train = X_train.mask(np.array(X_train_mask))\n",
    "    X_test = X_test.mask(np.array(X_test_mask))\n",
    "\n",
    "    print(\"Imputing nan values...\")\n",
    "    X_train, X_test = impute_values(X_train, X_test, method='iterative', max_iter=100)\n",
    "\n",
    "    print(\"Exporting clean data to csv...\")\n",
    "    export_to_csv(X_train, y_train, X_test)\n",
    "\n",
    "    print(\"All done!\")\n",
    "    \n",
    "    return X_train, y_train, X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efea75cf",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7636bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_svr(X_train, y_train):\n",
    "    svr = SVR()\n",
    "    gs_svr_params = {\n",
    "        'kernel': ['rbf'],#, 'poly', 'sigmoid'],\n",
    "        'C': np.logspace(0, 3, 6),\n",
    "        'epsilon': np.logspace(-4, -1, 7),\n",
    "    }\n",
    "    gs_svr = GridSearchCV(svr, gs_svr_params, cv=5, verbose=3)\n",
    "    gs_svr.fit(X_train, y_train)\n",
    "    \n",
    "    print(f\"\"\"The best validation score obtained is {gs_svr.best_score_:.5f} with\n",
    "    \\tkernel: {gs_svr.best_params_['kernel']}\n",
    "    \\tC: {gs_svr.best_params_['C']}\n",
    "    \\tepsilon: {gs_svr.best_params_['epsilon']}\"\"\")\n",
    "    \n",
    "    return gs_svr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14eede0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_gbr(X_train, y_train):\n",
    "    gbr = GradientBoostingRegressor()\n",
    "    gs_gbr_params = {\n",
    "     \"loss\":[\"ls\"],\n",
    "     \"learning_rate\": [0.03, 0.035, 0.04],\n",
    "     \"n_estimators\": np.arange(330, 421, 30),\n",
    "     \"subsample\": np.arange(0.3, 0.41, 0.05),\n",
    "     \"max_depth\": [4, 8],\n",
    "     \"min_samples_split\": [4, 5, 6],\n",
    "     \"min_samples_leaf\": [1, 5],\n",
    "     \"random_state\": [0], \n",
    "     \"verbose\": [1]\n",
    "    }\n",
    "    gs_gbr = GridSearchCV(gbr, gs_gbr_params, cv=4, verbose=3, error_score='raise')\n",
    "    gs_gbr.fit(X_train, y_train)\n",
    "\n",
    "    print(f\"\"\"The best validation score obtained is {gs_gbr.best_score_:.5f} with\n",
    "    \\tloss: {gs_gbr.best_params_['loss']}\n",
    "    \\tlearning_rate: {gs_gbr.best_params_['learning_rate']}\n",
    "    \\tn_estimators: {gs_gbr.best_params_['n_estimators']}\n",
    "    \\tsubsample: {gs_gbr.best_params_['subsample']}\n",
    "    \\tmax_depth: {gs_gbr.best_params_['max_depth']}\n",
    "    \\tmin_samples_split: {gs_gbr.best_params_['min_samples_split']}\n",
    "    \\tmin_samples_leaf: {gs_gbr.best_params_['min_samples_leaf']}\n",
    "    \"\"\")\n",
    "        \n",
    "    return gs_gbr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc23234",
   "metadata": {},
   "source": [
    "## Model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9e2b77",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train, y_train, X_test = clean_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613986a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "svr = best_svr(X_train, np.array(y_train).ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d4e23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gbr = best_gbr(X_train, np.array(y_train).ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3471c16b",
   "metadata": {},
   "source": [
    "## Create submission"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31586dc3",
   "metadata": {},
   "source": [
    "prediction = pd.DataFrame(svr.predict(X_test)) # modify here\n",
    "sub_id = 19 # modify here\n",
    "#create_submission(prediction, sub_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

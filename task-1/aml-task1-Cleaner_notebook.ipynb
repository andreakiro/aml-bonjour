{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "several-indian",
   "metadata": {},
   "source": [
    "# AML â€” Task 1\n",
    "## Predict the age of a brain from MRI features\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "wired-extra",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import zscore\n",
    "from pandas_profiling import ProfileReport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "exposed-guarantee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import SimpleImputer, KNNImputer, IterativeImputer\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectPercentile, f_regression\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, cross_val_score\n",
    "from sklearn.ensemble import IsolationForest, GradientBoostingRegressor\n",
    "from sklearn.neighbors import LocalOutlierFactor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "personalized-parameter",
   "metadata": {},
   "outputs": [],
   "source": [
    "from warnings import simplefilter\n",
    "from sklearn.exceptions import ConvergenceWarning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extraordinary-immunology",
   "metadata": {},
   "source": [
    "## Import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "white-institution",
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_data(folder=\"data/\", extension=\"\"):\n",
    "    X_train = pd.read_csv(folder + 'X_train' + extension + '.csv').drop(columns=['id'])\n",
    "    y_train = pd.read_csv(folder + 'y_train' + extension + '.csv').drop(columns=['id'])\n",
    "    X_test = pd.read_csv(folder + 'X_test' + extension + '.csv').drop(columns=['id'])\n",
    "    return X_train, y_train, X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indonesian-palmer",
   "metadata": {},
   "source": [
    "---\n",
    "## Exporting Data as csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "accessible-league",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_to_csv(X_train_cleaned, y_train_cleaned, X_test_cleaned, folder=\"data/\"):\n",
    "    X_train_cleaned.to_csv(folder + 'X_train_cleaned.csv', index=False)\n",
    "    y_train_cleaned.to_csv(folder + 'y_train_cleaned.csv', index=False)\n",
    "    X_test_cleaned.to_csv(folder + 'X_test_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cognitive-excellence",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 1: Outlier detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "wicked-villa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Remove the ouliers from our dataset. Replace temporarily the Nan values by the mean to perform outlier selection\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "X_train : pd.df\n",
    "    The features (what we will use to see the outliers)\n",
    "y_train : pd.df\n",
    "    The labels\n",
    "contamination : int, optional\n",
    "    The percent of outliers found by the isolation forest if it's used.\n",
    "verbose : int, optional\n",
    "    If not set to 0, print messages\n",
    "    \n",
    "Return\n",
    "------\n",
    "(pd.df, pd.df)\n",
    "    The data with the outliers rows removed\n",
    "\"\"\"\n",
    "def remove_outliers(X_train, y_train, contamination='auto', verbose=1, method=\"LocalOutlierFactor\"):\n",
    "    # Save a mask of the imputed values to be able to redo the imputation once the outlier detection is done\n",
    "    X_train_null_mask = X_train.isna()\n",
    "    \n",
    "    # Need to impute nan values for the outlier detection to work (cannot deal with nan)\n",
    "    X_train_imputed = pd.DataFrame(SimpleImputer(strategy=\"median\", verbose=verbose).fit_transform(X_train))\n",
    "    \n",
    "    clf = None\n",
    "    if method==\"LocalOutlierFactor\":\n",
    "        clf = LocalOutlierFactor(contamination=contamination)\n",
    "    elif method==\"IsolationForest\":\n",
    "        clf = IsolationForest(contamination=contamination, random_state=0, verbose=verbose)\n",
    "    else:\n",
    "        raise AttributeError(f\"Unvalid argument for method, must be 'LocalOutlierFactor' or 'IsolationForest', not '{method}'\")\n",
    "        \n",
    "    outliers_mask = pd.Series(clf.fit_predict(X_train_imputed)).map({1:1, -1:0}) #Mask with 0 for outliers and 1 for non outliers\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Detected {(outliers_mask == 0).sum()} outliers with method {method}, out of {outliers_mask.shape[0]} samples ({100 * (outliers_mask == 0).sum() / outliers_mask.shape[0]:.2f}%).\")\n",
    "    \n",
    "    #Replace the Nan values (The outlier detection shouldn't replace NaN values by itself)\n",
    "    X_train = pd.DataFrame(X_train).mask(X_train_null_mask, other=np.NaN, inplace=False)\n",
    "    \n",
    "    # Remove outliers from the training set\n",
    "    X_train = np.array(X_train)[outliers_mask == 1, :]\n",
    "    y_train = np.array(y_train)[outliers_mask == 1, :]\n",
    "    \n",
    "    X_train = pd.DataFrame(X_train)\n",
    "    y_train = pd.DataFrame(y_train)\n",
    "    \n",
    "    return (X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modular-leonard",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 2: Data scaling\n",
    "Done as soon as possible because can have an effect (e.g. on distances for `KNNImputer`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "burning-composition",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale(X_train_no_outliers, X_test):\n",
    "    # Do the scaling, saving the scaler to use it for X_test too. No need imputation, just ignore Nan values\n",
    "    scaler = StandardScaler().fit(X_train)\n",
    "    X_train_scaled = pd.DataFrame(scaler.transform(X_train_no_outliers))\n",
    "    # Cast X_test to np.array to avoid warning of model trained without feature names but X having some.\n",
    "    X_test_scaled = pd.DataFrame(scaler.transform(np.array(X_test)))\n",
    "    return (X_train_scaled, X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "featured-proxy",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 3: impute values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "stretch-surface",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_values(X_train, X_test, method='knn'):\n",
    "    print(f\"For the train dataset, there are {np.array(X_train.isna()).sum().sum()} nan values, out of {X_train.shape[0]*X_train.shape[1]} ({100*np.array(X_train.isna()).sum().sum()/(X_train.shape[0]*X_train.shape[1]):.2f}%).\")\n",
    "    \n",
    "    imputer = None\n",
    "    if method == 'knn':\n",
    "        imputer = KNNImputer(n_neighbors=6, weights='uniform').fit(X_train)\n",
    "    elif method == 'iterative':\n",
    "        # Runs VERY slowly and might cause the kernel to crash due to intenisve use of RAM\n",
    "        imputer = IterativeImputer(random_state=0, max_iter=15, verbose=2).fit(X_train)\n",
    "    else:\n",
    "        raise AttributeError(f\"Unvalid argument for method, must be 'knn' or 'iterative', not '{method}'\")\n",
    "    \n",
    "    X_train_imputed = pd.DataFrame(imputer.transform(X_train))\n",
    "    X_test_imputed = pd.DataFrame(imputer.transform(X_test))\n",
    "    return (X_train_imputed, X_test_imputed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "golden-external",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 4: Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "funny-washer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_features(X_train_imputed, X_test_imputed):\n",
    "    X_train_selected_features, X_test_selected_features = remove_constant_features(X_train_imputed, X_test_imputed)\n",
    "    X_train_selected_features, X_test_selected_features = remove_too_coorelated_features(X_train_selected_features, X_test_selected_features)\n",
    "    X_train_selected_features, X_test_selected_features = remove_random_features(X_train_selected_features, y_train, X_test_selected_features, percentile=80)\n",
    "\n",
    "    return X_train_selected_features, X_test_selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "reliable-holly",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_constant_features(X_train_imputed, X_test_imputed, verbose=1):\n",
    "    X_train_selected_features = X_train_imputed.loc[:, np.logical_and(X_train_imputed != X_train_imputed.iloc[0], X_train_imputed.notna()).any()]\n",
    "    X_test_selected_features = X_test_imputed.loc[:, np.logical_and(X_train_imputed != X_train_imputed.iloc[0], X_train_imputed.notna()).any()]\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"{X_train_imputed.shape[1]-X_train_selected_features.shape[1]} features removed because of constant values ({100*(X_train_imputed.shape[1]-X_train_selected_features.shape[1])/X_train_imputed.shape[1]:.2f}%)\")\n",
    "    \n",
    "    return X_train_selected_features, X_test_selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "solid-consistency",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_too_coorelated_features(X_train, X_test, threshold=0.7, verbose=1):\n",
    "    X_train_corr_ = X_train.corr()\n",
    "\n",
    "    X_train_too_correlated = (X_train_corr_.mask(\n",
    "        np.tril(np.ones([len(X_train_corr_)]*2, dtype=bool))).abs() > threshold).any()\n",
    "    \n",
    "    \n",
    "    X_train_selected_features = X_train.loc[:, (~X_train_too_correlated)]\n",
    "    X_test_selected_features = X_test.loc[:, (~X_train_too_correlated)]\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"{X_train.shape[1]-X_train_selected_features.shape[1]} features removed because of correlation > {threshold} ({100*(X_train.shape[1]-X_train_selected_features.shape[1])/X_train.shape[1]:.2f}%)\")\n",
    "\n",
    "    return X_train_selected_features, X_test_selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aging-account",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_random_features(X_train, y_train, X_test, Xtrm=None, Xtem=None, percentile=80, verbose=1):\n",
    "    selector = SelectPercentile(f_regression, percentile=percentile) # modify here\n",
    "    selector.fit(X_train, np.array(y_train).ravel())\n",
    "    X_train_selected_features = pd.DataFrame(selector.transform(X_train))\n",
    "    X_test_selected_features = pd.DataFrame(selector.transform(X_test))\n",
    "    if Xtrm is not None:\n",
    "        Xtrm = pd.DataFrame(selector.transform(Xtrm))\n",
    "    if Xtem is not None:\n",
    "        Xtem = pd.DataFrame(selector.transform(Xtem))\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"{X_train.shape[1]-X_train_selected_features.shape[1]} features removed because of low correlation with target ({100*(X_train.shape[1]-X_train_selected_features.shape[1])/X_train.shape[1]:.2f}%).\")\n",
    "        \n",
    "    return X_train_selected_features, X_test_selected_features, Xtrm, Xtem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "motivated-contribution",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 5 Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "accompanied-saudi",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train_cleaned, y_train_cleaned):\n",
    "    gs_lasso = best_lasso(X_train_cleaned, y_train_cleaned)\n",
    "    gs_svr = best_svr(X_train_cleaned, y_train_cleaned)\n",
    "    gs_gbr = best_gbr(X_train_cleaned, y_train_cleaned)\n",
    "    max_score = max(max(gs_gbr.best_score_, gs_lasso.best_score_), gs_svr.best_score_)\n",
    "\n",
    "    return gs_lasso if gs_lasso.best_score_ == max_score else gs_svr if gs_svr.best_score_ == max_score else gbr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "macro-immunology",
   "metadata": {},
   "source": [
    "### Model 1: Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "circular-addiction",
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_lasso(X_train_cleaned, y_train_cleaned):\n",
    "    lasso = Lasso(max_iter=100000)\n",
    "    gs_lasso_params = {\n",
    "    'alpha': np.logspace(-1, 0, 20),\n",
    "    }\n",
    "    gs_lasso = GridSearchCV(lasso, gs_lasso_params, cv=5, verbose=3)\n",
    "    gs_lasso.fit(X_train_cleaned, y_train_cleaned)\n",
    "    print(f\"The best validation score obtained is {gs_lasso.best_score_:.5f} with\\n\\talpha: {gs_lasso.best_params_['alpha']}\")\n",
    "    return gs_lasso;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "negative-reaction",
   "metadata": {},
   "source": [
    "### Model 2: SVR (SVM for regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "simplified-store",
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_svr(X_train_cleaned, y_train_cleaned):\n",
    "    svr = SVR()\n",
    "    gs_svr_params = {\n",
    "    'kernel': ['poly', 'rbf', 'sigmoid'],\n",
    "    'C': np.logspace(-1, 2.2, 4),\n",
    "    'epsilon': np.logspace(-2, 1, 3),\n",
    "    }\n",
    "    gs_svr = GridSearchCV(svr, gs_svr_params, cv=5, verbose=3)\n",
    "    gs_svr.fit(X_train_cleaned, y_train_cleaned)\n",
    "    print(f\"\"\"The best validation score obtained is {gs_svr.best_score_:.5f} with\n",
    "    \\tkernel: {gs_svr.best_params_['kernel']}\n",
    "    \\tC: {gs_svr.best_params_['C']}\n",
    "    \\tepsilon: {gs_svr.best_params_['epsilon']}\"\"\")\n",
    "    return gs_svr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ce6af1",
   "metadata": {},
   "source": [
    "### Model 3: Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f64b5186",
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_gbr(X_train, y_train):\n",
    "    gbr = GradientBoostingRegressor()\n",
    "    gs_gbr_params = {\n",
    "     \"loss\":[\"ls\"],\n",
    "     \"learning_rate\": np.logspace(-2, -1, 4),\n",
    "     \"n_estimators\": np.arange(50, 500, 50),\n",
    "     \"subsample\": np.arange(0.4, 1, 0.2),\n",
    "     \"max_depth\": np.arange(2, 8, 1),\n",
    "     \"min_samples_split\": np.arange(2, 8, 1),\n",
    "     \"min_samples_leaf\": np.arange(1, 9, 2),\n",
    "     \"random_state\": [0], \n",
    "     \"verbose\": [1]\n",
    "    }\n",
    "    gs_gbr = GridSearchCV(gbr, gs_gbr_params, cv=4, verbose=2, error_score='raise')\n",
    "    gs_gbr.fit(X_train, y_train)\n",
    "\n",
    "    print(f\"\"\"The best validation score obtained is {gs_gbr.best_score_:.5f} with\n",
    "    \\tloss: {gs_gbr.best_params_['loss']}\n",
    "    \\tlearning_rate: {gs_gbr.best_params_['learning_rate']}\n",
    "    \\tn_estimators: {gs_gbr.best_params_['n_estimators']}\n",
    "    \\tsubsample: {gs_gbr.best_params_['subsample']}\n",
    "    \\tmax_depth: {gs_gbr.best_params_['max_depth']}\n",
    "    \\tmin_samples_split: {gs_gbr.best_params_['min_samples_split']}\n",
    "    \\tmin_samples_leaf: {gs_gbr.best_params_['min_samples_leaf']}\n",
    "    \"\"\")\n",
    "        \n",
    "    return gs_gbr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "97f83eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def old_best_gbr(X_train, y_train):\n",
    "\n",
    "    params = {\n",
    "        \"loss\":\"ls\",\n",
    "        \"n_estimators\": 250,\n",
    "        \"learning_rate\": 0.025,\n",
    "        \"subsample\": 0.75,\n",
    "        \"max_depth\": 6,\n",
    "        \"min_samples_split\": 5,\n",
    "        \"n_iter_no_change\": 100,\n",
    "        \"validation_fraction\": 0.1,\n",
    "        \"random_state\": 0, \n",
    "        \"verbose\": 1,\n",
    "    }\n",
    "\n",
    "    gbr = GradientBoostingRegressor(**params)\n",
    "    \n",
    "    # Get the validation score\n",
    "    gbr_cv_scores = cross_val_score(gbr, X_train, y_train, n_jobs=-1, verbose=3)\n",
    "    print(f\"\"\"Validation score obtained is {np.mean(gbr_cv_scores):.4f} with\n",
    "    \\t{params}\"\"\")\n",
    "    \n",
    "    # Fit model (because previous function does not return fitted model)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
    "    gbr.fit(X_train, y_train)\n",
    "    \n",
    "    test_score = np.zeros(gbr.n_estimators_, dtype=np.float64)\n",
    "    for i, y_pred in enumerate(gbr.staged_predict(X_test)):\n",
    "        test_score[i] = gbr.loss_(y_test, y_pred)\n",
    "\n",
    "    fig = plt.figure(figsize=(6, 6))\n",
    "    plt.subplot(1, 1, 1)\n",
    "    plt.title(\"Deviance\")\n",
    "    plt.plot(\n",
    "        np.arange(gbr.n_estimators_),\n",
    "        gbr.train_score_,\n",
    "        \"b-\",\n",
    "        label=\"Training Set Deviance\",\n",
    "    )\n",
    "    plt.plot(\n",
    "        np.arange(gbr.n_estimators_), test_score, \"r-\", label=\"Test Set Deviance\"\n",
    "    )\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.xlabel(\"Boosting Iterations\")\n",
    "    plt.ylabel(\"Deviance\")\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return gbr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "julian-brunswick",
   "metadata": {},
   "source": [
    "---\n",
    "## Create CSV for submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "actual-ranch",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_submission(prediction, sub_id, basepath = 'submissions/task1-sub'):\n",
    "    result = prediction.copy()\n",
    "    result = result.rename(columns={0: 'y'})\n",
    "    result['id'] = range(0, len(result))\n",
    "    result = result[['id', 'y']]\n",
    "    result.to_csv(basepath+str(sub_id) + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "virgin-major",
   "metadata": {},
   "source": [
    "---\n",
    "## Final Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "91f911cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading raw data...\n",
      "Removing outliers...\n",
      "Detected 50 outliers with method LocalOutlierFactor, out of 1212 samples (4.13%).\n",
      "Scaling data...\n",
      "Selecting features...\n",
      "3 features removed because of constant values (0.36%)\n",
      "35 features removed because of correlation > 0.98 (4.22%)\n",
      "For the train dataset, there are 70349 nan values, out of 922628 (7.62%).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/loic/Enter/envs/InternetAnalytics/lib/python3.9/site-packages/sklearn/feature_selection/_univariate_selection.py:302: RuntimeWarning: invalid value encountered in true_divide\n",
      "  corr /= X_norms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "595 features removed because of low correlation with target (74.94%).\n",
      "Imputing nan values...\n",
      "For the train dataset, there are 17599 nan values, out of 231238 (7.61%).\n",
      "[IterativeImputer] Completing matrix with shape (1162, 199)\n",
      "[IterativeImputer] Ending imputation round 1/15, elapsed time 2.58\n",
      "[IterativeImputer] Change: 46.61194186774277, scaled tolerance: 0.014114220441223293 \n",
      "[IterativeImputer] Ending imputation round 2/15, elapsed time 5.78\n",
      "[IterativeImputer] Change: 4.496222306752934, scaled tolerance: 0.014114220441223293 \n",
      "[IterativeImputer] Ending imputation round 3/15, elapsed time 8.96\n",
      "[IterativeImputer] Change: 4.935402922708507, scaled tolerance: 0.014114220441223293 \n",
      "[IterativeImputer] Ending imputation round 4/15, elapsed time 12.00\n",
      "[IterativeImputer] Change: 1.5411124373844296, scaled tolerance: 0.014114220441223293 \n",
      "[IterativeImputer] Ending imputation round 5/15, elapsed time 15.05\n",
      "[IterativeImputer] Change: 0.5037903245494088, scaled tolerance: 0.014114220441223293 \n",
      "[IterativeImputer] Ending imputation round 6/15, elapsed time 18.13\n",
      "[IterativeImputer] Change: 0.2738000527790615, scaled tolerance: 0.014114220441223293 \n",
      "[IterativeImputer] Ending imputation round 7/15, elapsed time 21.30\n",
      "[IterativeImputer] Change: 0.21677179075126998, scaled tolerance: 0.014114220441223293 \n",
      "[IterativeImputer] Ending imputation round 8/15, elapsed time 24.41\n",
      "[IterativeImputer] Change: 0.18391338997024453, scaled tolerance: 0.014114220441223293 \n",
      "[IterativeImputer] Ending imputation round 9/15, elapsed time 27.49\n",
      "[IterativeImputer] Change: 0.16634835272965504, scaled tolerance: 0.014114220441223293 \n",
      "[IterativeImputer] Ending imputation round 10/15, elapsed time 30.82\n",
      "[IterativeImputer] Change: 0.16029288594558969, scaled tolerance: 0.014114220441223293 \n",
      "[IterativeImputer] Ending imputation round 11/15, elapsed time 34.05\n",
      "[IterativeImputer] Change: 0.1536821345827736, scaled tolerance: 0.014114220441223293 \n",
      "[IterativeImputer] Ending imputation round 12/15, elapsed time 37.16\n",
      "[IterativeImputer] Change: 0.14699014758363377, scaled tolerance: 0.014114220441223293 \n",
      "[IterativeImputer] Ending imputation round 13/15, elapsed time 40.26\n",
      "[IterativeImputer] Change: 0.14043086873317007, scaled tolerance: 0.014114220441223293 \n",
      "[IterativeImputer] Ending imputation round 14/15, elapsed time 43.55\n",
      "[IterativeImputer] Change: 0.13410665828213897, scaled tolerance: 0.014114220441223293 \n",
      "[IterativeImputer] Ending imputation round 15/15, elapsed time 46.86\n",
      "[IterativeImputer] Change: 0.12805424137047983, scaled tolerance: 0.014114220441223293 \n",
      "[IterativeImputer] Completing matrix with shape (1162, 199)\n",
      "[IterativeImputer] Ending imputation round 1/15, elapsed time 0.06\n",
      "[IterativeImputer] Ending imputation round 2/15, elapsed time 0.13\n",
      "[IterativeImputer] Ending imputation round 3/15, elapsed time 0.19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/loic/Enter/envs/InternetAnalytics/lib/python3.9/site-packages/sklearn/impute/_iterative.py:685: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\"[IterativeImputer] Early stopping criterion not\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[IterativeImputer] Ending imputation round 4/15, elapsed time 0.26\n",
      "[IterativeImputer] Ending imputation round 5/15, elapsed time 0.33\n",
      "[IterativeImputer] Ending imputation round 6/15, elapsed time 0.41\n",
      "[IterativeImputer] Ending imputation round 7/15, elapsed time 0.52\n",
      "[IterativeImputer] Ending imputation round 8/15, elapsed time 0.59\n",
      "[IterativeImputer] Ending imputation round 9/15, elapsed time 0.64\n",
      "[IterativeImputer] Ending imputation round 10/15, elapsed time 0.70\n",
      "[IterativeImputer] Ending imputation round 11/15, elapsed time 0.77\n",
      "[IterativeImputer] Ending imputation round 12/15, elapsed time 0.83\n",
      "[IterativeImputer] Ending imputation round 13/15, elapsed time 0.89\n",
      "[IterativeImputer] Ending imputation round 14/15, elapsed time 0.95\n",
      "[IterativeImputer] Ending imputation round 15/15, elapsed time 1.02\n",
      "[IterativeImputer] Completing matrix with shape (776, 199)\n",
      "[IterativeImputer] Ending imputation round 1/15, elapsed time 0.05\n",
      "[IterativeImputer] Ending imputation round 2/15, elapsed time 0.09\n",
      "[IterativeImputer] Ending imputation round 3/15, elapsed time 0.14\n",
      "[IterativeImputer] Ending imputation round 4/15, elapsed time 0.19\n",
      "[IterativeImputer] Ending imputation round 5/15, elapsed time 0.28\n",
      "[IterativeImputer] Ending imputation round 6/15, elapsed time 0.34\n",
      "[IterativeImputer] Ending imputation round 7/15, elapsed time 0.41\n",
      "[IterativeImputer] Ending imputation round 8/15, elapsed time 0.46\n",
      "[IterativeImputer] Ending imputation round 9/15, elapsed time 0.51\n",
      "[IterativeImputer] Ending imputation round 10/15, elapsed time 0.57\n",
      "[IterativeImputer] Ending imputation round 11/15, elapsed time 0.62\n",
      "[IterativeImputer] Ending imputation round 12/15, elapsed time 0.71\n",
      "[IterativeImputer] Ending imputation round 13/15, elapsed time 0.79\n",
      "[IterativeImputer] Ending imputation round 14/15, elapsed time 0.86\n",
      "[IterativeImputer] Ending imputation round 15/15, elapsed time 0.92\n",
      "Exporting clean data to csv...\n",
      "All done!\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading raw data...\")\n",
    "X_train, y_train, X_test = import_data()\n",
    "\n",
    "print(\"Removing outliers...\")\n",
    "#X_train_no_outliers, y_train_cleaned = remove_outliers(X_train, y_train, method=\"IsolationForest\")\n",
    "#X_train_no_outliers, y_train_cleaned = remove_outliers(X_train_no_outliers, y_train_cleaned, method=\"LocalOutlierFactor\") #Remove outliers in x\n",
    "X_train_no_outliers, y_train_cleaned = remove_outliers(X_train, y_train, method=\"LocalOutlierFactor\") #Remove outliers in x\n",
    "\n",
    "\n",
    "print(\"Scaling data...\")\n",
    "X_train_scaled, X_test_scaled = scale(X_train_no_outliers, X_test)\n",
    "\n",
    "print(\"Selecting features...\")\n",
    "X_train_no_constant_features, X_test_no_constant_features = remove_constant_features(X_train_scaled, X_test_scaled)\n",
    "X_train_no_too_correlated_features, X_test_no_too_correlated_features = remove_too_coorelated_features(X_train_no_constant_features, X_test_no_constant_features, threshold=0.98)\n",
    "\n",
    "#TODO: Clean pipeline\n",
    "# If we want iterative imputation, and don't want to run it for one hour (should try at one point though), need to\n",
    "# do feature selection before we do imputation. Except biggest filter for feature selection is f_regression which\n",
    "# cannot deal with nan values. Therefore, intermediary dumb knn imputation was quickly/dirtily \n",
    "# implemented to see results.\n",
    "# Conclusion: we achieve ~same or better validation scores with a LOT less features, seems like an \n",
    "# interresting way forward (even though test scores are a bit lower once submitted, but to not \n",
    "# overfit on the public ones and fail on the secret ones, should only watch validation score)\n",
    "\n",
    "#We first impute with knn which is cheap and then later on reaply mask and use iterative imputing\n",
    "X_train_mask = X_train_no_too_correlated_features.isna()\n",
    "X_test_mask = X_test_no_too_correlated_features.isna()\n",
    "X_train_knn_imputed, X_test_knn_imputed = impute_values(X_train_no_too_correlated_features, X_test_no_too_correlated_features, method='knn')\n",
    "X_train_selected_features, X_test_selected_features, X_train_mask, X_test_mask = remove_random_features(X_train_knn_imputed, y_train_cleaned, X_test_knn_imputed, X_train_mask, X_test_mask, percentile=25)\n",
    "X_train_no_imputation = X_train_selected_features.mask(np.array(X_train_mask))\n",
    "X_test_no_imputation = X_test_selected_features.mask(np.array(X_test_mask))\n",
    "\n",
    "print(\"Imputing nan values...\")\n",
    "X_train_cleaned, X_test_cleaned = impute_values(X_train_no_imputation, X_test_no_imputation, method='iterative')\n",
    "\n",
    "print(\"Exporting clean data to csv...\")\n",
    "export_to_csv(X_train_cleaned, y_train_cleaned, X_test_cleaned)\n",
    "\n",
    "print(\"All done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "banner-collar",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "[CV 1/5] END .........................alpha=0.1;, score=0.474 total time=   0.0s\n",
      "[CV 2/5] END .........................alpha=0.1;, score=0.435 total time=   0.0s\n",
      "[CV 3/5] END .........................alpha=0.1;, score=0.441 total time=   0.0s\n",
      "[CV 4/5] END .........................alpha=0.1;, score=0.501 total time=   0.0s\n",
      "[CV 5/5] END .........................alpha=0.1;, score=0.402 total time=   0.0s\n",
      "[CV 1/5] END .........alpha=0.11288378916846889;, score=0.474 total time=   0.0s\n",
      "[CV 2/5] END .........alpha=0.11288378916846889;, score=0.434 total time=   0.0s\n",
      "[CV 3/5] END .........alpha=0.11288378916846889;, score=0.440 total time=   0.0s\n",
      "[CV 4/5] END .........alpha=0.11288378916846889;, score=0.497 total time=   0.0s\n",
      "[CV 5/5] END .........alpha=0.11288378916846889;, score=0.400 total time=   0.0s\n",
      "[CV 1/5] END .........alpha=0.12742749857031338;, score=0.474 total time=   0.0s\n",
      "[CV 2/5] END .........alpha=0.12742749857031338;, score=0.432 total time=   0.0s\n",
      "[CV 3/5] END .........alpha=0.12742749857031338;, score=0.439 total time=   0.0s\n",
      "[CV 4/5] END .........alpha=0.12742749857031338;, score=0.495 total time=   0.0s\n",
      "[CV 5/5] END .........alpha=0.12742749857031338;, score=0.398 total time=   0.0s\n",
      "[CV 1/5] END .........alpha=0.14384498882876628;, score=0.473 total time=   0.0s\n",
      "[CV 2/5] END .........alpha=0.14384498882876628;, score=0.429 total time=   0.0s\n",
      "[CV 3/5] END .........alpha=0.14384498882876628;, score=0.437 total time=   0.0s\n",
      "[CV 4/5] END .........alpha=0.14384498882876628;, score=0.492 total time=   0.0s\n",
      "[CV 5/5] END .........alpha=0.14384498882876628;, score=0.395 total time=   0.0s\n",
      "[CV 1/5] END .........alpha=0.16237767391887217;, score=0.472 total time=   0.0s\n",
      "[CV 2/5] END .........alpha=0.16237767391887217;, score=0.426 total time=   0.0s\n",
      "[CV 3/5] END .........alpha=0.16237767391887217;, score=0.435 total time=   0.0s\n",
      "[CV 4/5] END .........alpha=0.16237767391887217;, score=0.491 total time=   0.0s\n",
      "[CV 5/5] END .........alpha=0.16237767391887217;, score=0.392 total time=   0.0s\n",
      "[CV 1/5] END .........alpha=0.18329807108324356;, score=0.470 total time=   0.0s\n",
      "[CV 2/5] END .........alpha=0.18329807108324356;, score=0.424 total time=   0.0s\n",
      "[CV 3/5] END .........alpha=0.18329807108324356;, score=0.432 total time=   0.0s\n",
      "[CV 4/5] END .........alpha=0.18329807108324356;, score=0.488 total time=   0.0s\n",
      "[CV 5/5] END .........alpha=0.18329807108324356;, score=0.390 total time=   0.0s\n",
      "[CV 1/5] END .........alpha=0.20691380811147897;, score=0.468 total time=   0.0s\n",
      "[CV 2/5] END .........alpha=0.20691380811147897;, score=0.420 total time=   0.0s\n",
      "[CV 3/5] END .........alpha=0.20691380811147897;, score=0.428 total time=   0.0s\n",
      "[CV 4/5] END .........alpha=0.20691380811147897;, score=0.484 total time=   0.0s\n",
      "[CV 5/5] END .........alpha=0.20691380811147897;, score=0.389 total time=   0.0s\n",
      "[CV 1/5] END .........alpha=0.23357214690901226;, score=0.465 total time=   0.0s\n",
      "[CV 2/5] END .........alpha=0.23357214690901226;, score=0.416 total time=   0.0s\n",
      "[CV 3/5] END .........alpha=0.23357214690901226;, score=0.424 total time=   0.0s\n",
      "[CV 4/5] END .........alpha=0.23357214690901226;, score=0.481 total time=   0.0s\n",
      "[CV 5/5] END .........alpha=0.23357214690901226;, score=0.387 total time=   0.0s\n",
      "[CV 1/5] END .........alpha=0.26366508987303583;, score=0.462 total time=   0.0s\n",
      "[CV 2/5] END .........alpha=0.26366508987303583;, score=0.411 total time=   0.0s\n",
      "[CV 3/5] END .........alpha=0.26366508987303583;, score=0.420 total time=   0.0s\n",
      "[CV 4/5] END .........alpha=0.26366508987303583;, score=0.477 total time=   0.0s\n",
      "[CV 5/5] END .........alpha=0.26366508987303583;, score=0.385 total time=   0.0s\n",
      "[CV 1/5] END .........alpha=0.29763514416313175;, score=0.461 total time=   0.0s\n",
      "[CV 2/5] END .........alpha=0.29763514416313175;, score=0.403 total time=   0.0s\n",
      "[CV 3/5] END .........alpha=0.29763514416313175;, score=0.415 total time=   0.0s\n",
      "[CV 4/5] END .........alpha=0.29763514416313175;, score=0.471 total time=   0.0s\n",
      "[CV 5/5] END .........alpha=0.29763514416313175;, score=0.381 total time=   0.0s\n",
      "[CV 1/5] END .........alpha=0.33598182862837817;, score=0.460 total time=   0.0s\n",
      "[CV 2/5] END .........alpha=0.33598182862837817;, score=0.394 total time=   0.0s\n",
      "[CV 3/5] END .........alpha=0.33598182862837817;, score=0.409 total time=   0.0s\n",
      "[CV 4/5] END .........alpha=0.33598182862837817;, score=0.462 total time=   0.0s\n",
      "[CV 5/5] END .........alpha=0.33598182862837817;, score=0.377 total time=   0.0s\n",
      "[CV 1/5] END .........alpha=0.37926901907322497;, score=0.460 total time=   0.0s\n",
      "[CV 2/5] END .........alpha=0.37926901907322497;, score=0.387 total time=   0.0s\n",
      "[CV 3/5] END .........alpha=0.37926901907322497;, score=0.405 total time=   0.0s\n",
      "[CV 4/5] END .........alpha=0.37926901907322497;, score=0.456 total time=   0.0s\n",
      "[CV 5/5] END .........alpha=0.37926901907322497;, score=0.374 total time=   0.0s\n",
      "[CV 1/5] END .........alpha=0.42813323987193935;, score=0.459 total time=   0.0s\n",
      "[CV 2/5] END .........alpha=0.42813323987193935;, score=0.382 total time=   0.0s\n",
      "[CV 3/5] END .........alpha=0.42813323987193935;, score=0.401 total time=   0.0s\n",
      "[CV 4/5] END .........alpha=0.42813323987193935;, score=0.450 total time=   0.0s\n",
      "[CV 5/5] END .........alpha=0.42813323987193935;, score=0.371 total time=   0.0s\n",
      "[CV 1/5] END ..........alpha=0.4832930238571752;, score=0.456 total time=   0.0s\n",
      "[CV 2/5] END ..........alpha=0.4832930238571752;, score=0.379 total time=   0.0s\n",
      "[CV 3/5] END ..........alpha=0.4832930238571752;, score=0.396 total time=   0.0s\n",
      "[CV 4/5] END ..........alpha=0.4832930238571752;, score=0.445 total time=   0.0s\n",
      "[CV 5/5] END ..........alpha=0.4832930238571752;, score=0.368 total time=   0.0s\n",
      "[CV 1/5] END ..........alpha=0.5455594781168519;, score=0.452 total time=   0.0s\n",
      "[CV 2/5] END ..........alpha=0.5455594781168519;, score=0.377 total time=   0.0s\n",
      "[CV 3/5] END ..........alpha=0.5455594781168519;, score=0.391 total time=   0.0s\n",
      "[CV 4/5] END ..........alpha=0.5455594781168519;, score=0.441 total time=   0.0s\n",
      "[CV 5/5] END ..........alpha=0.5455594781168519;, score=0.363 total time=   0.0s\n",
      "[CV 1/5] END ..........alpha=0.6158482110660264;, score=0.447 total time=   0.0s\n",
      "[CV 2/5] END ..........alpha=0.6158482110660264;, score=0.375 total time=   0.0s\n",
      "[CV 3/5] END ..........alpha=0.6158482110660264;, score=0.384 total time=   0.0s\n",
      "[CV 4/5] END ..........alpha=0.6158482110660264;, score=0.436 total time=   0.0s\n",
      "[CV 5/5] END ..........alpha=0.6158482110660264;, score=0.358 total time=   0.0s\n",
      "[CV 1/5] END ..........alpha=0.6951927961775606;, score=0.441 total time=   0.0s\n",
      "[CV 2/5] END ..........alpha=0.6951927961775606;, score=0.372 total time=   0.0s\n",
      "[CV 3/5] END ..........alpha=0.6951927961775606;, score=0.376 total time=   0.0s\n",
      "[CV 4/5] END ..........alpha=0.6951927961775606;, score=0.431 total time=   0.0s\n",
      "[CV 5/5] END ..........alpha=0.6951927961775606;, score=0.353 total time=   0.0s\n",
      "[CV 1/5] END ..........alpha=0.7847599703514611;, score=0.433 total time=   0.0s\n",
      "[CV 2/5] END ..........alpha=0.7847599703514611;, score=0.367 total time=   0.0s\n",
      "[CV 3/5] END ..........alpha=0.7847599703514611;, score=0.367 total time=   0.0s\n",
      "[CV 4/5] END ..........alpha=0.7847599703514611;, score=0.424 total time=   0.0s\n",
      "[CV 5/5] END ..........alpha=0.7847599703514611;, score=0.348 total time=   0.0s\n",
      "[CV 1/5] END ..........alpha=0.8858667904100825;, score=0.422 total time=   0.0s\n",
      "[CV 2/5] END ..........alpha=0.8858667904100825;, score=0.360 total time=   0.0s\n",
      "[CV 3/5] END ..........alpha=0.8858667904100825;, score=0.356 total time=   0.0s\n",
      "[CV 4/5] END ..........alpha=0.8858667904100825;, score=0.415 total time=   0.0s\n",
      "[CV 5/5] END ..........alpha=0.8858667904100825;, score=0.341 total time=   0.0s\n",
      "[CV 1/5] END .........................alpha=1.0;, score=0.410 total time=   0.0s\n",
      "[CV 2/5] END .........................alpha=1.0;, score=0.352 total time=   0.0s\n",
      "[CV 3/5] END .........................alpha=1.0;, score=0.343 total time=   0.0s\n",
      "[CV 4/5] END .........................alpha=1.0;, score=0.405 total time=   0.0s\n",
      "[CV 5/5] END .........................alpha=1.0;, score=0.332 total time=   0.0s\n",
      "The best validation score obtained is 0.45075 with\n",
      "\talpha: 0.1\n",
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
      "[CV 1/5] END ..C=0.1, epsilon=0.01, kernel=poly;, score=0.078 total time=   0.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END ..C=0.1, epsilon=0.01, kernel=poly;, score=0.033 total time=   0.1s\n",
      "[CV 3/5] END ..C=0.1, epsilon=0.01, kernel=poly;, score=0.039 total time=   0.1s\n",
      "[CV 4/5] END ..C=0.1, epsilon=0.01, kernel=poly;, score=0.049 total time=   0.1s\n",
      "[CV 5/5] END ..C=0.1, epsilon=0.01, kernel=poly;, score=0.019 total time=   0.1s\n",
      "[CV 1/5] END ...C=0.1, epsilon=0.01, kernel=rbf;, score=0.129 total time=   0.1s\n",
      "[CV 2/5] END ...C=0.1, epsilon=0.01, kernel=rbf;, score=0.109 total time=   0.1s\n",
      "[CV 3/5] END ...C=0.1, epsilon=0.01, kernel=rbf;, score=0.102 total time=   0.1s\n",
      "[CV 4/5] END ...C=0.1, epsilon=0.01, kernel=rbf;, score=0.115 total time=   0.1s\n",
      "[CV 5/5] END ...C=0.1, epsilon=0.01, kernel=rbf;, score=0.110 total time=   0.1s\n",
      "[CV 1/5] END C=0.1, epsilon=0.01, kernel=sigmoid;, score=0.313 total time=   0.1s\n",
      "[CV 2/5] END C=0.1, epsilon=0.01, kernel=sigmoid;, score=0.265 total time=   0.1s\n",
      "[CV 3/5] END C=0.1, epsilon=0.01, kernel=sigmoid;, score=0.239 total time=   0.1s\n",
      "[CV 4/5] END C=0.1, epsilon=0.01, kernel=sigmoid;, score=0.265 total time=   0.1s\n",
      "[CV 5/5] END C=0.1, epsilon=0.01, kernel=sigmoid;, score=0.244 total time=   0.1s\n",
      "[CV 1/5] END C=0.1, epsilon=0.31622776601683794, kernel=poly;, score=0.078 total time=   0.1s\n",
      "[CV 2/5] END C=0.1, epsilon=0.31622776601683794, kernel=poly;, score=0.028 total time=   0.1s\n",
      "[CV 3/5] END C=0.1, epsilon=0.31622776601683794, kernel=poly;, score=0.040 total time=   0.1s\n",
      "[CV 4/5] END C=0.1, epsilon=0.31622776601683794, kernel=poly;, score=0.048 total time=   0.1s\n",
      "[CV 5/5] END C=0.1, epsilon=0.31622776601683794, kernel=poly;, score=0.016 total time=   0.1s\n",
      "[CV 1/5] END C=0.1, epsilon=0.31622776601683794, kernel=rbf;, score=0.128 total time=   0.1s\n",
      "[CV 2/5] END C=0.1, epsilon=0.31622776601683794, kernel=rbf;, score=0.110 total time=   0.1s\n",
      "[CV 3/5] END C=0.1, epsilon=0.31622776601683794, kernel=rbf;, score=0.098 total time=   0.1s\n",
      "[CV 4/5] END C=0.1, epsilon=0.31622776601683794, kernel=rbf;, score=0.116 total time=   0.1s\n",
      "[CV 5/5] END C=0.1, epsilon=0.31622776601683794, kernel=rbf;, score=0.108 total time=   0.1s\n",
      "[CV 1/5] END C=0.1, epsilon=0.31622776601683794, kernel=sigmoid;, score=0.312 total time=   0.1s\n",
      "[CV 2/5] END C=0.1, epsilon=0.31622776601683794, kernel=sigmoid;, score=0.265 total time=   0.1s\n",
      "[CV 3/5] END C=0.1, epsilon=0.31622776601683794, kernel=sigmoid;, score=0.241 total time=   0.1s\n",
      "[CV 4/5] END C=0.1, epsilon=0.31622776601683794, kernel=sigmoid;, score=0.265 total time=   0.1s\n",
      "[CV 5/5] END C=0.1, epsilon=0.31622776601683794, kernel=sigmoid;, score=0.243 total time=   0.1s\n",
      "[CV 1/5] END ..C=0.1, epsilon=10.0, kernel=poly;, score=0.034 total time=   0.0s\n",
      "[CV 2/5] END ..C=0.1, epsilon=10.0, kernel=poly;, score=0.025 total time=   0.0s\n",
      "[CV 3/5] END ..C=0.1, epsilon=10.0, kernel=poly;, score=0.030 total time=   0.0s\n",
      "[CV 4/5] END ..C=0.1, epsilon=10.0, kernel=poly;, score=0.037 total time=   0.0s\n",
      "[CV 5/5] END ..C=0.1, epsilon=10.0, kernel=poly;, score=0.017 total time=   0.0s\n",
      "[CV 1/5] END ...C=0.1, epsilon=10.0, kernel=rbf;, score=0.057 total time=   0.0s\n",
      "[CV 2/5] END ...C=0.1, epsilon=10.0, kernel=rbf;, score=0.063 total time=   0.0s\n",
      "[CV 3/5] END ...C=0.1, epsilon=10.0, kernel=rbf;, score=0.062 total time=   0.0s\n",
      "[CV 4/5] END ...C=0.1, epsilon=10.0, kernel=rbf;, score=0.064 total time=   0.0s\n",
      "[CV 5/5] END ...C=0.1, epsilon=10.0, kernel=rbf;, score=0.069 total time=   0.0s\n",
      "[CV 1/5] END C=0.1, epsilon=10.0, kernel=sigmoid;, score=0.210 total time=   0.0s\n",
      "[CV 2/5] END C=0.1, epsilon=10.0, kernel=sigmoid;, score=0.179 total time=   0.0s\n",
      "[CV 3/5] END C=0.1, epsilon=10.0, kernel=sigmoid;, score=0.183 total time=   0.0s\n",
      "[CV 4/5] END C=0.1, epsilon=10.0, kernel=sigmoid;, score=0.182 total time=   0.0s\n",
      "[CV 5/5] END C=0.1, epsilon=10.0, kernel=sigmoid;, score=0.196 total time=   0.0s\n",
      "[CV 1/5] END C=1.1659144011798317, epsilon=0.01, kernel=poly;, score=0.284 total time=   0.1s\n",
      "[CV 2/5] END C=1.1659144011798317, epsilon=0.01, kernel=poly;, score=0.165 total time=   0.1s\n",
      "[CV 3/5] END C=1.1659144011798317, epsilon=0.01, kernel=poly;, score=0.153 total time=   0.1s\n",
      "[CV 4/5] END C=1.1659144011798317, epsilon=0.01, kernel=poly;, score=0.178 total time=   0.1s\n",
      "[CV 5/5] END C=1.1659144011798317, epsilon=0.01, kernel=poly;, score=0.135 total time=   0.1s\n",
      "[CV 1/5] END C=1.1659144011798317, epsilon=0.01, kernel=rbf;, score=0.463 total time=   0.1s\n",
      "[CV 2/5] END C=1.1659144011798317, epsilon=0.01, kernel=rbf;, score=0.396 total time=   0.1s\n",
      "[CV 3/5] END C=1.1659144011798317, epsilon=0.01, kernel=rbf;, score=0.362 total time=   0.1s\n",
      "[CV 4/5] END C=1.1659144011798317, epsilon=0.01, kernel=rbf;, score=0.406 total time=   0.1s\n",
      "[CV 5/5] END C=1.1659144011798317, epsilon=0.01, kernel=rbf;, score=0.413 total time=   0.1s\n",
      "[CV 1/5] END C=1.1659144011798317, epsilon=0.01, kernel=sigmoid;, score=0.335 total time=   0.1s\n",
      "[CV 2/5] END C=1.1659144011798317, epsilon=0.01, kernel=sigmoid;, score=0.338 total time=   0.1s\n",
      "[CV 3/5] END C=1.1659144011798317, epsilon=0.01, kernel=sigmoid;, score=0.288 total time=   0.1s\n",
      "[CV 4/5] END C=1.1659144011798317, epsilon=0.01, kernel=sigmoid;, score=0.418 total time=   0.1s\n",
      "[CV 5/5] END C=1.1659144011798317, epsilon=0.01, kernel=sigmoid;, score=0.282 total time=   0.1s\n",
      "[CV 1/5] END C=1.1659144011798317, epsilon=0.31622776601683794, kernel=poly;, score=0.284 total time=   0.1s\n",
      "[CV 2/5] END C=1.1659144011798317, epsilon=0.31622776601683794, kernel=poly;, score=0.164 total time=   0.1s\n",
      "[CV 3/5] END C=1.1659144011798317, epsilon=0.31622776601683794, kernel=poly;, score=0.158 total time=   0.1s\n",
      "[CV 4/5] END C=1.1659144011798317, epsilon=0.31622776601683794, kernel=poly;, score=0.179 total time=   0.1s\n",
      "[CV 5/5] END C=1.1659144011798317, epsilon=0.31622776601683794, kernel=poly;, score=0.134 total time=   0.1s\n",
      "[CV 1/5] END C=1.1659144011798317, epsilon=0.31622776601683794, kernel=rbf;, score=0.463 total time=   0.1s\n",
      "[CV 2/5] END C=1.1659144011798317, epsilon=0.31622776601683794, kernel=rbf;, score=0.396 total time=   0.1s\n",
      "[CV 3/5] END C=1.1659144011798317, epsilon=0.31622776601683794, kernel=rbf;, score=0.364 total time=   0.1s\n",
      "[CV 4/5] END C=1.1659144011798317, epsilon=0.31622776601683794, kernel=rbf;, score=0.406 total time=   0.1s\n",
      "[CV 5/5] END C=1.1659144011798317, epsilon=0.31622776601683794, kernel=rbf;, score=0.413 total time=   0.1s\n",
      "[CV 1/5] END C=1.1659144011798317, epsilon=0.31622776601683794, kernel=sigmoid;, score=0.332 total time=   0.1s\n",
      "[CV 2/5] END C=1.1659144011798317, epsilon=0.31622776601683794, kernel=sigmoid;, score=0.335 total time=   0.1s\n",
      "[CV 3/5] END C=1.1659144011798317, epsilon=0.31622776601683794, kernel=sigmoid;, score=0.285 total time=   0.1s\n",
      "[CV 4/5] END C=1.1659144011798317, epsilon=0.31622776601683794, kernel=sigmoid;, score=0.421 total time=   0.1s\n",
      "[CV 5/5] END C=1.1659144011798317, epsilon=0.31622776601683794, kernel=sigmoid;, score=0.278 total time=   0.1s\n",
      "[CV 1/5] END C=1.1659144011798317, epsilon=10.0, kernel=poly;, score=0.164 total time=   0.0s\n",
      "[CV 2/5] END C=1.1659144011798317, epsilon=10.0, kernel=poly;, score=0.103 total time=   0.0s\n",
      "[CV 3/5] END C=1.1659144011798317, epsilon=10.0, kernel=poly;, score=0.101 total time=   0.0s\n",
      "[CV 4/5] END C=1.1659144011798317, epsilon=10.0, kernel=poly;, score=0.110 total time=   0.0s\n",
      "[CV 5/5] END C=1.1659144011798317, epsilon=10.0, kernel=poly;, score=0.063 total time=   0.0s\n",
      "[CV 1/5] END C=1.1659144011798317, epsilon=10.0, kernel=rbf;, score=0.332 total time=   0.0s\n",
      "[CV 2/5] END C=1.1659144011798317, epsilon=10.0, kernel=rbf;, score=0.295 total time=   0.0s\n",
      "[CV 3/5] END C=1.1659144011798317, epsilon=10.0, kernel=rbf;, score=0.294 total time=   0.0s\n",
      "[CV 4/5] END C=1.1659144011798317, epsilon=10.0, kernel=rbf;, score=0.302 total time=   0.0s\n",
      "[CV 5/5] END C=1.1659144011798317, epsilon=10.0, kernel=rbf;, score=0.337 total time=   0.0s\n",
      "[CV 1/5] END C=1.1659144011798317, epsilon=10.0, kernel=sigmoid;, score=0.327 total time=   0.0s\n",
      "[CV 2/5] END C=1.1659144011798317, epsilon=10.0, kernel=sigmoid;, score=0.322 total time=   0.0s\n",
      "[CV 3/5] END C=1.1659144011798317, epsilon=10.0, kernel=sigmoid;, score=0.319 total time=   0.0s\n",
      "[CV 4/5] END C=1.1659144011798317, epsilon=10.0, kernel=sigmoid;, score=0.368 total time=   0.0s\n",
      "[CV 5/5] END C=1.1659144011798317, epsilon=10.0, kernel=sigmoid;, score=0.315 total time=   0.0s\n",
      "[CV 1/5] END C=13.593563908785255, epsilon=0.01, kernel=poly;, score=0.476 total time=   0.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END C=13.593563908785255, epsilon=0.01, kernel=poly;, score=0.391 total time=   0.1s\n",
      "[CV 3/5] END C=13.593563908785255, epsilon=0.01, kernel=poly;, score=0.391 total time=   0.1s\n",
      "[CV 4/5] END C=13.593563908785255, epsilon=0.01, kernel=poly;, score=0.384 total time=   0.1s\n",
      "[CV 5/5] END C=13.593563908785255, epsilon=0.01, kernel=poly;, score=0.360 total time=   0.1s\n",
      "[CV 1/5] END C=13.593563908785255, epsilon=0.01, kernel=rbf;, score=0.618 total time=   0.1s\n",
      "[CV 2/5] END C=13.593563908785255, epsilon=0.01, kernel=rbf;, score=0.542 total time=   0.1s\n",
      "[CV 3/5] END C=13.593563908785255, epsilon=0.01, kernel=rbf;, score=0.510 total time=   0.1s\n",
      "[CV 4/5] END C=13.593563908785255, epsilon=0.01, kernel=rbf;, score=0.556 total time=   0.1s\n",
      "[CV 5/5] END C=13.593563908785255, epsilon=0.01, kernel=rbf;, score=0.575 total time=   0.1s\n",
      "[CV 1/5] END C=13.593563908785255, epsilon=0.01, kernel=sigmoid;, score=-38.302 total time=   0.1s\n",
      "[CV 2/5] END C=13.593563908785255, epsilon=0.01, kernel=sigmoid;, score=-23.980 total time=   0.1s\n",
      "[CV 3/5] END C=13.593563908785255, epsilon=0.01, kernel=sigmoid;, score=-23.735 total time=   0.1s\n",
      "[CV 4/5] END C=13.593563908785255, epsilon=0.01, kernel=sigmoid;, score=-14.345 total time=   0.1s\n",
      "[CV 5/5] END C=13.593563908785255, epsilon=0.01, kernel=sigmoid;, score=-30.839 total time=   0.1s\n",
      "[CV 1/5] END C=13.593563908785255, epsilon=0.31622776601683794, kernel=poly;, score=0.471 total time=   0.1s\n",
      "[CV 2/5] END C=13.593563908785255, epsilon=0.31622776601683794, kernel=poly;, score=0.389 total time=   0.1s\n",
      "[CV 3/5] END C=13.593563908785255, epsilon=0.31622776601683794, kernel=poly;, score=0.389 total time=   0.1s\n",
      "[CV 4/5] END C=13.593563908785255, epsilon=0.31622776601683794, kernel=poly;, score=0.380 total time=   0.1s\n",
      "[CV 5/5] END C=13.593563908785255, epsilon=0.31622776601683794, kernel=poly;, score=0.354 total time=   0.1s\n",
      "[CV 1/5] END C=13.593563908785255, epsilon=0.31622776601683794, kernel=rbf;, score=0.617 total time=   0.1s\n",
      "[CV 2/5] END C=13.593563908785255, epsilon=0.31622776601683794, kernel=rbf;, score=0.541 total time=   0.1s\n",
      "[CV 3/5] END C=13.593563908785255, epsilon=0.31622776601683794, kernel=rbf;, score=0.510 total time=   0.1s\n",
      "[CV 4/5] END C=13.593563908785255, epsilon=0.31622776601683794, kernel=rbf;, score=0.554 total time=   0.1s\n",
      "[CV 5/5] END C=13.593563908785255, epsilon=0.31622776601683794, kernel=rbf;, score=0.575 total time=   0.1s\n",
      "[CV 1/5] END C=13.593563908785255, epsilon=0.31622776601683794, kernel=sigmoid;, score=-38.876 total time=   0.1s\n",
      "[CV 2/5] END C=13.593563908785255, epsilon=0.31622776601683794, kernel=sigmoid;, score=-34.095 total time=   0.1s\n",
      "[CV 3/5] END C=13.593563908785255, epsilon=0.31622776601683794, kernel=sigmoid;, score=-24.307 total time=   0.1s\n",
      "[CV 4/5] END C=13.593563908785255, epsilon=0.31622776601683794, kernel=sigmoid;, score=-14.229 total time=   0.1s\n",
      "[CV 5/5] END C=13.593563908785255, epsilon=0.31622776601683794, kernel=sigmoid;, score=-22.865 total time=   0.1s\n",
      "[CV 1/5] END C=13.593563908785255, epsilon=10.0, kernel=poly;, score=0.259 total time=   0.0s\n",
      "[CV 2/5] END C=13.593563908785255, epsilon=10.0, kernel=poly;, score=0.178 total time=   0.0s\n",
      "[CV 3/5] END C=13.593563908785255, epsilon=10.0, kernel=poly;, score=0.190 total time=   0.0s\n",
      "[CV 4/5] END C=13.593563908785255, epsilon=10.0, kernel=poly;, score=0.158 total time=   0.0s\n",
      "[CV 5/5] END C=13.593563908785255, epsilon=10.0, kernel=poly;, score=0.125 total time=   0.0s\n",
      "[CV 1/5] END C=13.593563908785255, epsilon=10.0, kernel=rbf;, score=0.456 total time=   0.0s\n",
      "[CV 2/5] END C=13.593563908785255, epsilon=10.0, kernel=rbf;, score=0.388 total time=   0.0s\n",
      "[CV 3/5] END C=13.593563908785255, epsilon=10.0, kernel=rbf;, score=0.439 total time=   0.0s\n",
      "[CV 4/5] END C=13.593563908785255, epsilon=10.0, kernel=rbf;, score=0.400 total time=   0.0s\n",
      "[CV 5/5] END C=13.593563908785255, epsilon=10.0, kernel=rbf;, score=0.456 total time=   0.0s\n",
      "[CV 1/5] END C=13.593563908785255, epsilon=10.0, kernel=sigmoid;, score=-24.380 total time=   0.1s\n",
      "[CV 2/5] END C=13.593563908785255, epsilon=10.0, kernel=sigmoid;, score=-25.931 total time=   0.1s\n",
      "[CV 3/5] END C=13.593563908785255, epsilon=10.0, kernel=sigmoid;, score=-18.045 total time=   0.1s\n",
      "[CV 4/5] END C=13.593563908785255, epsilon=10.0, kernel=sigmoid;, score=-11.002 total time=   0.1s\n",
      "[CV 5/5] END C=13.593563908785255, epsilon=10.0, kernel=sigmoid;, score=-21.963 total time=   0.1s\n",
      "[CV 1/5] END C=158.48931924611142, epsilon=0.01, kernel=poly;, score=0.334 total time=   0.2s\n",
      "[CV 2/5] END C=158.48931924611142, epsilon=0.01, kernel=poly;, score=0.194 total time=   0.1s\n",
      "[CV 3/5] END C=158.48931924611142, epsilon=0.01, kernel=poly;, score=0.371 total time=   0.1s\n",
      "[CV 4/5] END C=158.48931924611142, epsilon=0.01, kernel=poly;, score=0.243 total time=   0.2s\n",
      "[CV 5/5] END C=158.48931924611142, epsilon=0.01, kernel=poly;, score=0.282 total time=   0.1s\n",
      "[CV 1/5] END C=158.48931924611142, epsilon=0.01, kernel=rbf;, score=0.574 total time=   0.1s\n",
      "[CV 2/5] END C=158.48931924611142, epsilon=0.01, kernel=rbf;, score=0.534 total time=   0.1s\n",
      "[CV 3/5] END C=158.48931924611142, epsilon=0.01, kernel=rbf;, score=0.521 total time=   0.1s\n",
      "[CV 4/5] END C=158.48931924611142, epsilon=0.01, kernel=rbf;, score=0.548 total time=   0.2s\n",
      "[CV 5/5] END C=158.48931924611142, epsilon=0.01, kernel=rbf;, score=0.592 total time=   0.2s\n",
      "[CV 1/5] END C=158.48931924611142, epsilon=0.01, kernel=sigmoid;, score=-6509.379 total time=   0.1s\n",
      "[CV 2/5] END C=158.48931924611142, epsilon=0.01, kernel=sigmoid;, score=-1244.877 total time=   0.1s\n",
      "[CV 3/5] END C=158.48931924611142, epsilon=0.01, kernel=sigmoid;, score=-3606.710 total time=   0.1s\n",
      "[CV 4/5] END C=158.48931924611142, epsilon=0.01, kernel=sigmoid;, score=-2392.404 total time=   0.1s\n",
      "[CV 5/5] END C=158.48931924611142, epsilon=0.01, kernel=sigmoid;, score=-4367.101 total time=   0.1s\n",
      "[CV 1/5] END C=158.48931924611142, epsilon=0.31622776601683794, kernel=poly;, score=0.340 total time=   0.1s\n",
      "[CV 2/5] END C=158.48931924611142, epsilon=0.31622776601683794, kernel=poly;, score=0.199 total time=   0.1s\n",
      "[CV 3/5] END C=158.48931924611142, epsilon=0.31622776601683794, kernel=poly;, score=0.371 total time=   0.1s\n",
      "[CV 4/5] END C=158.48931924611142, epsilon=0.31622776601683794, kernel=poly;, score=0.244 total time=   0.1s\n",
      "[CV 5/5] END C=158.48931924611142, epsilon=0.31622776601683794, kernel=poly;, score=0.278 total time=   0.1s\n",
      "[CV 1/5] END C=158.48931924611142, epsilon=0.31622776601683794, kernel=rbf;, score=0.572 total time=   0.1s\n",
      "[CV 2/5] END C=158.48931924611142, epsilon=0.31622776601683794, kernel=rbf;, score=0.532 total time=   0.1s\n",
      "[CV 3/5] END C=158.48931924611142, epsilon=0.31622776601683794, kernel=rbf;, score=0.522 total time=   0.1s\n",
      "[CV 4/5] END C=158.48931924611142, epsilon=0.31622776601683794, kernel=rbf;, score=0.547 total time=   0.1s\n",
      "[CV 5/5] END C=158.48931924611142, epsilon=0.31622776601683794, kernel=rbf;, score=0.591 total time=   0.1s\n",
      "[CV 1/5] END C=158.48931924611142, epsilon=0.31622776601683794, kernel=sigmoid;, score=-6652.538 total time=   0.1s\n",
      "[CV 2/5] END C=158.48931924611142, epsilon=0.31622776601683794, kernel=sigmoid;, score=-1278.187 total time=   0.1s\n",
      "[CV 3/5] END C=158.48931924611142, epsilon=0.31622776601683794, kernel=sigmoid;, score=-3634.805 total time=   0.1s\n",
      "[CV 4/5] END C=158.48931924611142, epsilon=0.31622776601683794, kernel=sigmoid;, score=-2312.355 total time=   0.1s\n",
      "[CV 5/5] END C=158.48931924611142, epsilon=0.31622776601683794, kernel=sigmoid;, score=-4481.973 total time=   0.1s\n",
      "[CV 1/5] END C=158.48931924611142, epsilon=10.0, kernel=poly;, score=0.217 total time=   0.0s\n",
      "[CV 2/5] END C=158.48931924611142, epsilon=10.0, kernel=poly;, score=0.137 total time=   0.0s\n",
      "[CV 3/5] END C=158.48931924611142, epsilon=10.0, kernel=poly;, score=0.206 total time=   0.0s\n",
      "[CV 4/5] END C=158.48931924611142, epsilon=10.0, kernel=poly;, score=0.103 total time=   0.0s\n",
      "[CV 5/5] END C=158.48931924611142, epsilon=10.0, kernel=poly;, score=0.107 total time=   0.0s\n",
      "[CV 1/5] END C=158.48931924611142, epsilon=10.0, kernel=rbf;, score=0.427 total time=   0.0s\n",
      "[CV 2/5] END C=158.48931924611142, epsilon=10.0, kernel=rbf;, score=0.378 total time=   0.0s\n",
      "[CV 3/5] END C=158.48931924611142, epsilon=10.0, kernel=rbf;, score=0.438 total time=   0.0s\n",
      "[CV 4/5] END C=158.48931924611142, epsilon=10.0, kernel=rbf;, score=0.386 total time=   0.0s\n",
      "[CV 5/5] END C=158.48931924611142, epsilon=10.0, kernel=rbf;, score=0.449 total time=   0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END C=158.48931924611142, epsilon=10.0, kernel=sigmoid;, score=-6689.595 total time=   0.1s\n",
      "[CV 2/5] END C=158.48931924611142, epsilon=10.0, kernel=sigmoid;, score=-1260.672 total time=   0.1s\n",
      "[CV 3/5] END C=158.48931924611142, epsilon=10.0, kernel=sigmoid;, score=-3596.458 total time=   0.1s\n",
      "[CV 4/5] END C=158.48931924611142, epsilon=10.0, kernel=sigmoid;, score=-2335.250 total time=   0.1s\n",
      "[CV 5/5] END C=158.48931924611142, epsilon=10.0, kernel=sigmoid;, score=-4441.638 total time=   0.1s\n",
      "The best validation score obtained is 0.56008 with\n",
      "    \tkernel: rbf\n",
      "    \tC: 13.593563908785255\n",
      "    \tepsilon: 0.01\n",
      "Fitting 4 folds for each of 1 candidates, totalling 4 fits\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1          86.3646           6.2093            1.72s\n",
      "         2          74.4087           5.5380            1.72s\n",
      "         3          70.2060           5.1252            1.70s\n",
      "         4          66.6197           3.8355            1.68s\n",
      "         5          63.7019           3.6720            1.66s\n",
      "         6          61.3450           2.0721            1.64s\n",
      "         7          53.1905           3.0210            1.62s\n",
      "         8          52.9216           1.9896            1.60s\n",
      "         9          51.5516           1.1313            1.58s\n",
      "        10          47.7659           0.7106            1.57s\n",
      "        20          32.4133           0.7213            1.41s\n",
      "        30          25.5089          -0.1953            1.23s\n",
      "        40          20.8200          -0.1758            1.07s\n",
      "        50          18.3111          -0.1261            0.91s\n",
      "        60          16.5128          -0.1439            0.73s\n",
      "        70          14.5230           0.0027            0.54s\n",
      "        80          12.4052          -0.0651            0.36s\n",
      "        90          11.5106          -0.0571            0.18s\n",
      "       100          10.0588          -0.1236            0.00s\n",
      "[CV] END learning_rate=0.1, loss=ls, max_depth=3, min_samples_leaf=5, min_samples_split=5, n_estimators=100, random_state=0, subsample=0.8, verbose=1; total time=   1.8s\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1          89.2997           5.0031            1.69s\n",
      "         2          79.8066           6.2587            1.67s\n",
      "         3          75.2738           5.7506            1.65s\n",
      "         4          68.8403           5.2701            1.63s\n",
      "         5          64.8668           3.5358            1.61s\n",
      "         6          62.1531           2.2825            1.59s\n",
      "         7          54.5121           2.8554            1.58s\n",
      "         8          51.9974           2.3667            1.57s\n",
      "         9          53.9902           1.4685            1.55s\n",
      "        10          49.2912           1.2614            1.54s\n",
      "        20          33.5997           0.1235            1.45s\n",
      "        30          26.6314          -0.0331            1.28s\n",
      "        40          20.8420          -0.1201            1.09s\n",
      "        50          18.6792          -0.3564            0.92s\n",
      "        60          16.7723          -0.0602            0.73s\n",
      "        70          14.6728          -0.0373            0.55s\n",
      "        80          12.4755          -0.0245            0.37s\n",
      "        90          11.6116          -0.1390            0.19s\n",
      "       100          10.6353          -0.0560            0.00s\n",
      "[CV] END learning_rate=0.1, loss=ls, max_depth=3, min_samples_leaf=5, min_samples_split=5, n_estimators=100, random_state=0, subsample=0.8, verbose=1; total time=   1.9s\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1          82.6387           6.3522            1.82s\n",
      "         2          73.4973           5.7378            1.79s\n",
      "         3          72.4223           4.2758            1.76s\n",
      "         4          62.6568           4.8850            1.72s\n",
      "         5          58.7628           5.0121            1.68s\n",
      "         6          52.9116           3.6151            1.65s\n",
      "         7          51.3879           1.8400            1.63s\n",
      "         8          49.5239           1.4042            1.60s\n",
      "         9          47.3956           1.4694            1.58s\n",
      "        10          45.7199           1.6491            1.56s\n",
      "        20          30.1373          -0.0202            1.47s\n",
      "        30          23.6823           0.0158            1.27s\n",
      "        40          19.3669           0.0225            1.08s\n",
      "        50          16.3911          -0.0012            0.89s\n",
      "        60          14.5431          -0.0944            0.72s\n",
      "        70          12.8314          -0.0720            0.54s\n",
      "        80          10.7528          -0.0807            0.37s\n",
      "        90           9.5765          -0.0777            0.18s\n",
      "       100           8.7499          -0.0378            0.00s\n",
      "[CV] END learning_rate=0.1, loss=ls, max_depth=3, min_samples_leaf=5, min_samples_split=5, n_estimators=100, random_state=0, subsample=0.8, verbose=1; total time=   1.9s\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1          85.3457           5.7203            1.92s\n",
      "         2          75.9547           7.0664            1.90s\n",
      "         3          73.5670           5.1891            1.87s\n",
      "         4          62.0520           4.7843            1.84s\n",
      "         5          60.2574           4.4193            1.83s\n",
      "         6          56.1371           2.8773            1.81s\n",
      "         7          53.3698           2.1412            1.77s\n",
      "         8          50.5964           1.9357            1.73s\n",
      "         9          48.2737           1.2316            1.69s\n",
      "        10          45.8141           1.4489            1.66s\n",
      "        20          31.1197           0.2552            1.44s\n",
      "        30          24.1630          -0.0481            1.30s\n",
      "        40          19.6754          -0.0836            1.10s\n",
      "        50          17.2403          -0.1790            0.92s\n",
      "        60          15.4913          -0.1556            0.74s\n",
      "        70          14.4521          -0.0709            0.56s\n",
      "        80          12.2655          -0.0682            0.38s\n",
      "        90          10.7405          -0.1434            0.19s\n",
      "       100           9.7812          -0.0137            0.00s\n",
      "[CV] END learning_rate=0.1, loss=ls, max_depth=3, min_samples_leaf=5, min_samples_split=5, n_estimators=100, random_state=0, subsample=0.8, verbose=1; total time=   1.9s\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1          86.2947           5.9202            2.95s\n",
      "         2          79.0097           5.9168            2.75s\n",
      "         3          72.3631           5.3187            2.63s\n",
      "         4          66.1513           3.9275            2.55s\n",
      "         5          60.9374           3.4711            2.46s\n",
      "         6          60.0730           2.9058            2.39s\n",
      "         7          55.6042           2.4785            2.34s\n",
      "         8          56.1842           1.5906            2.31s\n",
      "         9          51.3062           2.2811            2.30s\n",
      "        10          47.0090           1.7842            2.28s\n",
      "        20          33.4761           0.4424            2.00s\n",
      "        30          28.0745          -0.2488            1.80s\n",
      "        40          23.8266          -0.0069            1.55s\n",
      "        50          21.2329          -0.1041            1.35s\n",
      "        60          19.0021          -0.2806            1.09s\n",
      "        70          17.8468          -0.0814            0.81s\n",
      "        80          15.0943          -0.0829            0.54s\n",
      "        90          14.4788           0.0657            0.27s\n",
      "       100          12.8189          -0.1066            0.00s\n",
      "The best validation score obtained is 0.56443 with\n",
      "    \tloss: ls\n",
      "    \tlearning_rate: 0.1\n",
      "    \tn_estimators: 100\n",
      "    \tsubsample: 0.8\n",
      "    \tmax_depth: 3\n",
      "    \tmin_samples_split: 5\n",
      "    \tmin_samples_leaf: 5\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "#gs = model(X_train_cleaned, np.array(y_train_cleaned).ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "5e5dd6ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:   15.8s remaining:   23.6s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:   20.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation score obtained is 0.6257 with\n",
      "    \t{'loss': 'ls', 'n_estimators': 250, 'learning_rate': 0.025, 'subsample': 0.75, 'max_depth': 6, 'min_samples_split': 5, 'n_iter_no_change': 100, 'validation_fraction': 0.1, 'random_state': 0, 'verbose': 1}\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1          91.3291           2.8179           16.80s\n",
      "         2          90.5564           2.5407           13.68s\n",
      "         3          87.3021           2.4079           12.35s\n",
      "         4          84.6068           2.0784           11.69s\n",
      "         5          80.9514           1.9908           11.39s\n",
      "         6          77.3235           1.9075           11.13s\n",
      "         7          77.5718           1.8483           10.95s\n",
      "         8          73.2426           2.1333           10.87s\n",
      "         9          72.2031           1.4538           10.74s\n",
      "        10          69.2085           1.6761           10.67s\n",
      "        20          49.0434           1.0633            9.95s\n",
      "        30          36.6699           0.6404            9.43s\n",
      "        40          27.3531           0.3737            8.96s\n",
      "        50          21.2296           0.2226            8.53s\n",
      "        60          16.4651           0.1424            8.12s\n",
      "        70          12.7057           0.1556            7.70s\n",
      "        80          10.1661           0.0655            7.38s\n",
      "        90           8.3588           0.0338            7.05s\n",
      "       100           6.7553          -0.0008            6.63s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-27-aec82dbb1057>:47: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "gs = old_best_gbr(X_train_cleaned, np.array(y_train_cleaned).ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "otherwise-leather",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = pd.DataFrame(gs.predict(X_test_cleaned))\n",
    "SUB_ID = 15 #to modify\n",
    "create_submission(prediction, SUB_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ce5b74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

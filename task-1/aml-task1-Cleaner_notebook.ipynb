{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "several-indian",
   "metadata": {},
   "source": [
    "# AML â€” Task 1\n",
    "## Predict the age of a brain from MRI features\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "wired-extra",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import zscore\n",
    "from pandas_profiling import ProfileReport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "exposed-guarantee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import SimpleImputer, KNNImputer, IterativeImputer\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectPercentile, f_regression\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, cross_val_score\n",
    "from sklearn.ensemble import IsolationForest, GradientBoostingRegressor\n",
    "from sklearn.neighbors import LocalOutlierFactor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "personalized-parameter",
   "metadata": {},
   "outputs": [],
   "source": [
    "from warnings import simplefilter\n",
    "from sklearn.exceptions import ConvergenceWarning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extraordinary-immunology",
   "metadata": {},
   "source": [
    "## Import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "white-institution",
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_data(folder=\"data/\", extension=\"\"):\n",
    "    X_train = pd.read_csv(folder + 'X_train' + extension + '.csv').drop(columns=['id'])\n",
    "    y_train = pd.read_csv(folder + 'y_train' + extension + '.csv').drop(columns=['id'])\n",
    "    X_test = pd.read_csv(folder + 'X_test' + extension + '.csv').drop(columns=['id'])\n",
    "    return X_train, y_train, X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indonesian-palmer",
   "metadata": {},
   "source": [
    "---\n",
    "## Exporting Data as csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "accessible-league",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_to_csv(X_train_cleaned, y_train_cleaned, X_test_cleaned, folder=\"data/\"):\n",
    "    X_train_cleaned.to_csv(folder + 'X_train_cleaned.csv', index=False)\n",
    "    y_train_cleaned.to_csv(folder + 'y_train_cleaned.csv', index=False)\n",
    "    X_test_cleaned.to_csv(folder + 'X_test_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cognitive-excellence",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 1: Outlier detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "wicked-villa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Remove the ouliers from our dataset. Replace temporarily the Nan values by the mean to perform outlier selection\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "X_train : pd.df\n",
    "    The features (what we will use to see the outliers)\n",
    "y_train : pd.df\n",
    "    The labels\n",
    "contamination : int, optional\n",
    "    The percent of outliers found by the isolation forest if it's used.\n",
    "verbose : int, optional\n",
    "    If not set to 0, print messages\n",
    "    \n",
    "Return\n",
    "------\n",
    "(pd.df, pd.df)\n",
    "    The data with the outliers rows removed\n",
    "\"\"\"\n",
    "def remove_outliers(X_train, y_train, contamination='auto', verbose=1, method=\"LocalOutlierFactor\"):\n",
    "    # Save a mask of the imputed values to be able to redo the imputation once the outlier detection is done\n",
    "    X_train_null_mask = X_train.isna()\n",
    "    \n",
    "    # Need to impute nan values for the outlier detection to work (cannot deal with nan)\n",
    "    X_train_imputed = pd.DataFrame(SimpleImputer(strategy=\"median\", verbose=verbose).fit_transform(X_train))\n",
    "    \n",
    "    clf = None\n",
    "    if method==\"LocalOutlierFactor\":\n",
    "        clf = LocalOutlierFactor(contamination=contamination)\n",
    "    elif method==\"IsolationForest\":\n",
    "        clf = IsolationForest(contamination=contamination, random_state=0, verbose=verbose)\n",
    "    else:\n",
    "        raise AttributeError(f\"Unvalid argument for method, must be 'LocalOutlierFactor' or 'IsolationForest', not '{method}'\")\n",
    "        \n",
    "    outliers_mask = pd.Series(clf.fit_predict(X_train_imputed)).map({1:1, -1:0}) #Mask with 0 for outliers and 1 for non outliers\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Detected {(outliers_mask == 0).sum()} outliers with method {method}, out of {outliers_mask.shape[0]} samples ({100 * (outliers_mask == 0).sum() / outliers_mask.shape[0]:.2f}%).\")\n",
    "    \n",
    "    #Replace the Nan values (The outlier detection shouldn't replace NaN values by itself)\n",
    "    X_train = pd.DataFrame(X_train).mask(X_train_null_mask, other=np.NaN, inplace=False)\n",
    "    \n",
    "    # Remove outliers from the training set\n",
    "    X_train = np.array(X_train)[outliers_mask == 1, :]\n",
    "    y_train = np.array(y_train)[outliers_mask == 1, :]\n",
    "    \n",
    "    X_train = pd.DataFrame(X_train)\n",
    "    y_train = pd.DataFrame(y_train)\n",
    "    \n",
    "    return (X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modular-leonard",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 2: Data scaling\n",
    "Done as soon as possible because can have an effect (e.g. on distances for `KNNImputer`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "burning-composition",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale(X_train_no_outliers, X_test):\n",
    "    # Do the scaling, saving the scaler to use it for X_test too. No need imputation, just ignore Nan values\n",
    "    scaler = StandardScaler().fit(X_train)\n",
    "    X_train_scaled = pd.DataFrame(scaler.transform(X_train_no_outliers))\n",
    "    # Cast X_test to np.array to avoid warning of model trained without feature names but X having some.\n",
    "    X_test_scaled = pd.DataFrame(scaler.transform(np.array(X_test)))\n",
    "    return (X_train_scaled, X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "featured-proxy",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 3: impute values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "stretch-surface",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_values(X_train, X_test, method='knn'):\n",
    "    print(f\"For the train dataset, there are {np.array(X_train.isna()).sum().sum()} nan values, out of {X_train.shape[0]*X_train.shape[1]} ({100*np.array(X_train.isna()).sum().sum()/(X_train.shape[0]*X_train.shape[1]):.2f}%).\")\n",
    "    \n",
    "    imputer = None\n",
    "    if method == 'knn':\n",
    "        imputer = KNNImputer(n_neighbors=6, weights='uniform').fit(X_train)\n",
    "    elif method == 'iterative':\n",
    "        # Runs VERY slowly and might cause the kernel to crash due to intenisve use of RAM\n",
    "        imputer = IterativeImputer(random_state=0, max_iter=10, verbose=2).fit(X_train)\n",
    "    else:\n",
    "        raise AttributeError(f\"Unvalid argument for method, must be 'knn' or 'iterative', not '{method}'\")\n",
    "    \n",
    "    X_train_imputed = pd.DataFrame(imputer.transform(X_train))\n",
    "    X_test_imputed = pd.DataFrame(imputer.transform(X_test))\n",
    "    return (X_train_imputed, X_test_imputed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "golden-external",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 4: Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "funny-washer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_features(X_train_imputed, X_test_imputed):\n",
    "    X_train_selected_features, X_test_selected_features = remove_constant_features(X_train_imputed, X_test_imputed)\n",
    "    X_train_selected_features, X_test_selected_features = remove_too_coorelated_features(X_train_selected_features, X_test_selected_features)\n",
    "    X_train_selected_features, X_test_selected_features = remove_random_features(X_train_selected_features, y_train, X_test_selected_features, percentile=80)\n",
    "\n",
    "    return X_train_selected_features, X_test_selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "reliable-holly",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_constant_features(X_train_imputed, X_test_imputed, verbose=1):\n",
    "    X_train_selected_features = X_train_imputed.loc[:, np.logical_and(X_train_imputed != X_train_imputed.iloc[0], X_train_imputed.notna()).any()]\n",
    "    X_test_selected_features = X_test_imputed.loc[:, np.logical_and(X_train_imputed != X_train_imputed.iloc[0], X_train_imputed.notna()).any()]\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"{X_train_imputed.shape[1]-X_train_selected_features.shape[1]} features removed because of constant values ({100*(X_train_imputed.shape[1]-X_train_selected_features.shape[1])/X_train_imputed.shape[1]:.2f}%)\")\n",
    "    \n",
    "    return X_train_selected_features, X_test_selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "solid-consistency",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_too_coorelated_features(X_train, X_test, threshold=0.7, verbose=1):\n",
    "    X_train_corr_ = X_train.corr()\n",
    "\n",
    "    X_train_too_correlated = (X_train_corr_.mask(\n",
    "        np.tril(np.ones([len(X_train_corr_)]*2, dtype=bool))).abs() > threshold).any()\n",
    "    \n",
    "    \n",
    "    X_train_selected_features = X_train.loc[:, (~X_train_too_correlated)]\n",
    "    X_test_selected_features = X_test.loc[:, (~X_train_too_correlated)]\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"{X_train.shape[1]-X_train_selected_features.shape[1]} features removed because of correlation > {threshold} ({100*(X_train.shape[1]-X_train_selected_features.shape[1])/X_train.shape[1]:.2f}%)\")\n",
    "\n",
    "    return X_train_selected_features, X_test_selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aging-account",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_random_features(X_train, y_train, X_test, Xtrm=None, Xtem=None, percentile=80, verbose=1):\n",
    "    selector = SelectPercentile(f_regression, percentile=percentile) # modify here\n",
    "    selector.fit(X_train, np.array(y_train).ravel())\n",
    "    X_train_selected_features = pd.DataFrame(selector.transform(X_train))\n",
    "    X_test_selected_features = pd.DataFrame(selector.transform(X_test))\n",
    "    if Xtrm is not None:\n",
    "        Xtrm = pd.DataFrame(selector.transform(Xtrm))\n",
    "    if Xtem is not None:\n",
    "        Xtem = pd.DataFrame(selector.transform(Xtem))\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"{X_train.shape[1]-X_train_selected_features.shape[1]} features removed because of low correlation with target ({100*(X_train.shape[1]-X_train_selected_features.shape[1])/X_train.shape[1]:.2f}%).\")\n",
    "        \n",
    "    return X_train_selected_features, X_test_selected_features, Xtrm, Xtem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "motivated-contribution",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 5 Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "accompanied-saudi",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train_cleaned, y_train_cleaned):\n",
    "    gs_lasso = best_lasso(X_train_cleaned, y_train_cleaned)\n",
    "    gs_svr = best_svr(X_train_cleaned, y_train_cleaned)\n",
    "    gbr = best_gbr(X_train_cleaned, y_train_cleaned)\n",
    "    gbr_scores = cross_val_score(gbr, X_train_cleaned, y_train_cleaned, n_jobs=-1, verbose=3)\n",
    "    gbr_best_score_ = np.mean(gbr_scores)\n",
    "    \n",
    "    max_score = max(max(gbr_best_score_, gs_lasso.best_score_), gs_svr.best_score_)\n",
    "    \n",
    "    return gs_lasso if gs_lasso.best_score_ == max_score else gs_svr if gs_svr.best_score_ == max_score else gbr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "macro-immunology",
   "metadata": {},
   "source": [
    "### Model 1: Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "circular-addiction",
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_lasso(X_train_cleaned, y_train_cleaned):\n",
    "    lasso = Lasso(max_iter=100000)\n",
    "    gs_lasso_params = {\n",
    "    'alpha': np.logspace(-1, 0, 20),\n",
    "    }\n",
    "    gs_lasso = GridSearchCV(lasso, gs_lasso_params, cv=5, verbose=3)\n",
    "    gs_lasso.fit(X_train_cleaned, y_train_cleaned)\n",
    "    print(f\"The best validation score obtained is {gs_lasso.best_score_:.5f} with\\n\\talpha: {gs_lasso.best_params_['alpha']}\")\n",
    "    return gs_lasso;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "negative-reaction",
   "metadata": {},
   "source": [
    "### Model 2: SVR (SVM for regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "simplified-store",
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_svr(X_train_cleaned, y_train_cleaned):\n",
    "    svr = SVR()\n",
    "    gs_svr_params = {\n",
    "    'kernel': ['poly', 'rbf', 'sigmoid'],\n",
    "    'C': np.logspace(-1, 2.2, 4),\n",
    "    'epsilon': np.logspace(-2, 1, 3),\n",
    "    }\n",
    "    gs_svr = GridSearchCV(svr, gs_svr_params, cv=5, verbose=3)\n",
    "    gs_svr.fit(X_train_cleaned, y_train_cleaned)\n",
    "    print(f\"\"\"The best validation score obtained is {gs_svr.best_score_:.5f} with\n",
    "    \\tkernel: {gs_svr.best_params_['kernel']}\n",
    "    \\tC: {gs_svr.best_params_['C']}\n",
    "    \\tepsilon: {gs_svr.best_params_['epsilon']}\"\"\")\n",
    "    return gs_svr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8cd568",
   "metadata": {},
   "source": [
    "### Model 3: Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5c33ef83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_gbr(X_train, y_train):\n",
    "#     gbr = GradientBoostingRegressor(random_state=0)\n",
    "#     gs_gbr_params = {\n",
    "#         \"learning_rate\": np.logspace(-3, -1, 3),\n",
    "#         \"n_estimators\": np.logspace(1, 3, 3),\n",
    "#         \"subsample\": [0.7, 1],\n",
    "#         \"max_depth\": [3, 4, 5],\n",
    "#     }\n",
    "#     gs_gbr = GridSearchCV(gbr, gs_gbr_params, cv=5, verbose=3, error_score='raise')\n",
    "#     gs_gbr.fit(X_train, y_train)\n",
    "#     return gs_gbr\n",
    "\n",
    "    params = {\n",
    "        \"loss\":\"ls\",\n",
    "        \"n_estimators\": 250,\n",
    "        \"learning_rate\": 0.025,\n",
    "        \"subsample\": 0.75,\n",
    "        \"max_depth\": 6,\n",
    "        \"min_samples_split\": 5,\n",
    "        \"n_iter_no_change\": 100,\n",
    "        \"validation_fraction\": 0.1,\n",
    "        \"random_state\": 0, \n",
    "        \"verbose\": 1,\n",
    "    }\n",
    "\n",
    "    gbr = GradientBoostingRegressor(**params)\n",
    "    \n",
    "    # Get the validation score\n",
    "    gbr_cv_scores = cross_val_score(gbr, X_train, y_train, n_jobs=-1, verbose=3)\n",
    "    print(f\"\"\"Validation score obtained is {np.mean(gbr_cv_scores):.4f} with\n",
    "    \\t{params}\"\"\")\n",
    "    \n",
    "    # Fit model (because previous function does not return fitted model)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
    "    gbr.fit(X_train, y_train)\n",
    "    \n",
    "    test_score = np.zeros(gbr.n_estimators_, dtype=np.float64)\n",
    "    for i, y_pred in enumerate(gbr.staged_predict(X_test)):\n",
    "        test_score[i] = gbr.loss_(y_test, y_pred)\n",
    "\n",
    "    fig = plt.figure(figsize=(6, 6))\n",
    "    plt.subplot(1, 1, 1)\n",
    "    plt.title(\"Deviance\")\n",
    "    plt.plot(\n",
    "        np.arange(gbr.n_estimators_),\n",
    "        gbr.train_score_,\n",
    "        \"b-\",\n",
    "        label=\"Training Set Deviance\",\n",
    "    )\n",
    "    plt.plot(\n",
    "        np.arange(gbr.n_estimators_), test_score, \"r-\", label=\"Test Set Deviance\"\n",
    "    )\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.xlabel(\"Boosting Iterations\")\n",
    "    plt.ylabel(\"Deviance\")\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return gbr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "julian-brunswick",
   "metadata": {},
   "source": [
    "---\n",
    "## Create CSV for submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "actual-ranch",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_submission(prediction, sub_id, basepath = 'submissions/task1-sub'):\n",
    "    result = prediction.copy()\n",
    "    result = result.rename(columns={0: 'y'})\n",
    "    result['id'] = range(0, len(result))\n",
    "    result = result[['id', 'y']]\n",
    "    result.to_csv(basepath+str(sub_id) + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "virgin-major",
   "metadata": {},
   "source": [
    "---\n",
    "## Final Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "79177a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading raw data...\n",
      "Removing outliers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected 5 outliers with method IsolationForest, out of 1212 samples (0.41%).\n",
      "Detected 49 outliers with method LocalOutlierFactor, out of 1207 samples (4.06%).\n",
      "Scaling data...\n",
      "Selecting features...\n",
      "3 features removed because of constant values (0.36%)\n",
      "145 features removed because of correlation > 0.7 (17.49%)\n",
      "For the train dataset, there are 60375 nan values, out of 792072 (7.62%).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/loic/Enter/envs/InternetAnalytics/lib/python3.9/site-packages/sklearn/feature_selection/_univariate_selection.py:302: RuntimeWarning: invalid value encountered in true_divide\n",
      "  corr /= X_norms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "547 features removed because of low correlation with target (79.97%).\n",
      "Imputing nan values...\n",
      "For the train dataset, there are 12008 nan values, out of 158646 (7.57%).\n",
      "[IterativeImputer] Completing matrix with shape (1158, 137)\n",
      "[IterativeImputer] Ending imputation round 1/10, elapsed time 0.99\n",
      "[IterativeImputer] Change: 22.86860557505578, scaled tolerance: 0.014114220441223293 \n",
      "[IterativeImputer] Ending imputation round 2/10, elapsed time 2.06\n",
      "[IterativeImputer] Change: 1.8253574886633708, scaled tolerance: 0.014114220441223293 \n",
      "[IterativeImputer] Ending imputation round 3/10, elapsed time 3.05\n",
      "[IterativeImputer] Change: 0.32551102406025395, scaled tolerance: 0.014114220441223293 \n",
      "[IterativeImputer] Ending imputation round 4/10, elapsed time 4.07\n",
      "[IterativeImputer] Change: 0.15447516978474374, scaled tolerance: 0.014114220441223293 \n",
      "[IterativeImputer] Ending imputation round 5/10, elapsed time 5.09\n",
      "[IterativeImputer] Change: 0.08724944718489042, scaled tolerance: 0.014114220441223293 \n",
      "[IterativeImputer] Ending imputation round 6/10, elapsed time 6.10\n",
      "[IterativeImputer] Change: 0.049362473206929346, scaled tolerance: 0.014114220441223293 \n",
      "[IterativeImputer] Ending imputation round 7/10, elapsed time 7.10\n",
      "[IterativeImputer] Change: 0.027984811636121196, scaled tolerance: 0.014114220441223293 \n",
      "[IterativeImputer] Ending imputation round 8/10, elapsed time 8.09\n",
      "[IterativeImputer] Change: 0.01588151061864201, scaled tolerance: 0.014114220441223293 \n",
      "[IterativeImputer] Ending imputation round 9/10, elapsed time 9.15\n",
      "[IterativeImputer] Change: 0.009017302224264587, scaled tolerance: 0.014114220441223293 \n",
      "[IterativeImputer] Early stopping criterion reached.\n",
      "[IterativeImputer] Completing matrix with shape (1158, 137)\n",
      "[IterativeImputer] Ending imputation round 1/9, elapsed time 0.03\n",
      "[IterativeImputer] Ending imputation round 2/9, elapsed time 0.07\n",
      "[IterativeImputer] Ending imputation round 3/9, elapsed time 0.10\n",
      "[IterativeImputer] Ending imputation round 4/9, elapsed time 0.13\n",
      "[IterativeImputer] Ending imputation round 5/9, elapsed time 0.16\n",
      "[IterativeImputer] Ending imputation round 6/9, elapsed time 0.19\n",
      "[IterativeImputer] Ending imputation round 7/9, elapsed time 0.21\n",
      "[IterativeImputer] Ending imputation round 8/9, elapsed time 0.24\n",
      "[IterativeImputer] Ending imputation round 9/9, elapsed time 0.27\n",
      "[IterativeImputer] Completing matrix with shape (776, 137)\n",
      "[IterativeImputer] Ending imputation round 1/9, elapsed time 0.02\n",
      "[IterativeImputer] Ending imputation round 2/9, elapsed time 0.04\n",
      "[IterativeImputer] Ending imputation round 3/9, elapsed time 0.06\n",
      "[IterativeImputer] Ending imputation round 4/9, elapsed time 0.08\n",
      "[IterativeImputer] Ending imputation round 5/9, elapsed time 0.10\n",
      "[IterativeImputer] Ending imputation round 6/9, elapsed time 0.13\n",
      "[IterativeImputer] Ending imputation round 7/9, elapsed time 0.15\n",
      "[IterativeImputer] Ending imputation round 8/9, elapsed time 0.17\n",
      "[IterativeImputer] Ending imputation round 9/9, elapsed time 0.19\n",
      "Exporting clean data to csv...\n",
      "All done!\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading raw data...\")\n",
    "X_train, y_train, X_test = import_data()\n",
    "\n",
    "print(\"Removing outliers...\")\n",
    "X_train_no_outliers, y_train_cleaned = remove_outliers(X_train, y_train, method=\"IsolationForest\")\n",
    "X_train_no_outliers, y_train_cleaned = remove_outliers(X_train_no_outliers, y_train_cleaned, method=\"LocalOutlierFactor\") #Remove outliers in x\n",
    "\n",
    "\n",
    "print(\"Scaling data...\")\n",
    "X_train_scaled, X_test_scaled = scale(X_train_no_outliers, X_test)\n",
    "\n",
    "print(\"Selecting features...\")\n",
    "X_train_no_constant_features, X_test_no_constant_features = remove_constant_features(X_train_scaled, X_test_scaled)\n",
    "X_train_no_too_correlated_features, X_test_no_too_correlated_features = remove_too_coorelated_features(X_train_no_constant_features, X_test_no_constant_features)\n",
    "\n",
    "#TODO: Clean pipeline\n",
    "# If we want iterative imputation, and don't want to run it for one hour (should try at one point though), need to\n",
    "# do feature selection before we do imputation. Except biggest filter for feature selection is f_regression which\n",
    "# cannot deal with nan values. Therefore, intermediary dumb knn imputation was quickly/dirtily \n",
    "# implemented to see results.\n",
    "# Conclusion: we achieve ~same or better validation scores with a LOT less features, seems like an \n",
    "# interresting way forward (even though test scores are a bit lower once submitted, but to not \n",
    "# overfit on the public ones and fail on the secret ones, should only watch validation score)\n",
    "\n",
    "#We first impute with knn which is cheap and then later on reaply mask and use iterative imputing\n",
    "X_train_mask = X_train_no_too_correlated_features.isna()\n",
    "X_test_mask = X_test_no_too_correlated_features.isna()\n",
    "X_train_knn_imputed, X_test_knn_imputed = impute_values(X_train_no_too_correlated_features, X_test_no_too_correlated_features, method='knn')\n",
    "X_train_selected_features, X_test_selected_features, X_train_mask, X_test_mask = remove_random_features(X_train_knn_imputed, y_train_cleaned, X_test_knn_imputed, X_train_mask, X_test_mask, percentile=20)\n",
    "X_train_no_imputation = X_train_selected_features.mask(np.array(X_train_mask))\n",
    "X_test_no_imputation = X_test_selected_features.mask(np.array(X_test_mask))\n",
    "\n",
    "print(\"Imputing nan values...\")\n",
    "X_train_cleaned, X_test_cleaned = impute_values(X_train_no_imputation, X_test_no_imputation, method='iterative')\n",
    "\n",
    "print(\"Exporting clean data to csv...\")\n",
    "export_to_csv(X_train_cleaned, y_train_cleaned, X_test_cleaned)\n",
    "\n",
    "print(\"All done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "banner-collar",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "[CV 1/5] END .........................alpha=0.1;, score=0.474 total time=   0.0s\n",
      "[CV 2/5] END .........................alpha=0.1;, score=0.435 total time=   0.0s\n",
      "[CV 3/5] END .........................alpha=0.1;, score=0.441 total time=   0.0s\n",
      "[CV 4/5] END .........................alpha=0.1;, score=0.501 total time=   0.0s\n",
      "[CV 5/5] END .........................alpha=0.1;, score=0.402 total time=   0.0s\n",
      "[CV 1/5] END .........alpha=0.11288378916846889;, score=0.474 total time=   0.0s\n",
      "[CV 2/5] END .........alpha=0.11288378916846889;, score=0.434 total time=   0.0s\n",
      "[CV 3/5] END .........alpha=0.11288378916846889;, score=0.440 total time=   0.0s\n",
      "[CV 4/5] END .........alpha=0.11288378916846889;, score=0.497 total time=   0.0s\n",
      "[CV 5/5] END .........alpha=0.11288378916846889;, score=0.400 total time=   0.0s\n",
      "[CV 1/5] END .........alpha=0.12742749857031338;, score=0.474 total time=   0.0s\n",
      "[CV 2/5] END .........alpha=0.12742749857031338;, score=0.432 total time=   0.0s\n",
      "[CV 3/5] END .........alpha=0.12742749857031338;, score=0.439 total time=   0.0s\n",
      "[CV 4/5] END .........alpha=0.12742749857031338;, score=0.495 total time=   0.0s\n",
      "[CV 5/5] END .........alpha=0.12742749857031338;, score=0.398 total time=   0.0s\n",
      "[CV 1/5] END .........alpha=0.14384498882876628;, score=0.473 total time=   0.0s\n",
      "[CV 2/5] END .........alpha=0.14384498882876628;, score=0.429 total time=   0.0s\n",
      "[CV 3/5] END .........alpha=0.14384498882876628;, score=0.437 total time=   0.0s\n",
      "[CV 4/5] END .........alpha=0.14384498882876628;, score=0.492 total time=   0.0s\n",
      "[CV 5/5] END .........alpha=0.14384498882876628;, score=0.395 total time=   0.0s\n",
      "[CV 1/5] END .........alpha=0.16237767391887217;, score=0.472 total time=   0.0s\n",
      "[CV 2/5] END .........alpha=0.16237767391887217;, score=0.426 total time=   0.0s\n",
      "[CV 3/5] END .........alpha=0.16237767391887217;, score=0.435 total time=   0.0s\n",
      "[CV 4/5] END .........alpha=0.16237767391887217;, score=0.491 total time=   0.0s\n",
      "[CV 5/5] END .........alpha=0.16237767391887217;, score=0.392 total time=   0.0s\n",
      "[CV 1/5] END .........alpha=0.18329807108324356;, score=0.470 total time=   0.0s\n",
      "[CV 2/5] END .........alpha=0.18329807108324356;, score=0.424 total time=   0.0s\n",
      "[CV 3/5] END .........alpha=0.18329807108324356;, score=0.432 total time=   0.0s\n",
      "[CV 4/5] END .........alpha=0.18329807108324356;, score=0.488 total time=   0.0s\n",
      "[CV 5/5] END .........alpha=0.18329807108324356;, score=0.390 total time=   0.0s\n",
      "[CV 1/5] END .........alpha=0.20691380811147897;, score=0.468 total time=   0.0s\n",
      "[CV 2/5] END .........alpha=0.20691380811147897;, score=0.420 total time=   0.0s\n",
      "[CV 3/5] END .........alpha=0.20691380811147897;, score=0.428 total time=   0.0s\n",
      "[CV 4/5] END .........alpha=0.20691380811147897;, score=0.484 total time=   0.0s\n",
      "[CV 5/5] END .........alpha=0.20691380811147897;, score=0.389 total time=   0.0s\n",
      "[CV 1/5] END .........alpha=0.23357214690901226;, score=0.465 total time=   0.0s\n",
      "[CV 2/5] END .........alpha=0.23357214690901226;, score=0.416 total time=   0.0s\n",
      "[CV 3/5] END .........alpha=0.23357214690901226;, score=0.424 total time=   0.0s\n",
      "[CV 4/5] END .........alpha=0.23357214690901226;, score=0.481 total time=   0.0s\n",
      "[CV 5/5] END .........alpha=0.23357214690901226;, score=0.387 total time=   0.0s\n",
      "[CV 1/5] END .........alpha=0.26366508987303583;, score=0.462 total time=   0.0s\n",
      "[CV 2/5] END .........alpha=0.26366508987303583;, score=0.411 total time=   0.0s\n",
      "[CV 3/5] END .........alpha=0.26366508987303583;, score=0.420 total time=   0.0s\n",
      "[CV 4/5] END .........alpha=0.26366508987303583;, score=0.477 total time=   0.0s\n",
      "[CV 5/5] END .........alpha=0.26366508987303583;, score=0.385 total time=   0.0s\n",
      "[CV 1/5] END .........alpha=0.29763514416313175;, score=0.461 total time=   0.0s\n",
      "[CV 2/5] END .........alpha=0.29763514416313175;, score=0.403 total time=   0.0s\n",
      "[CV 3/5] END .........alpha=0.29763514416313175;, score=0.415 total time=   0.0s\n",
      "[CV 4/5] END .........alpha=0.29763514416313175;, score=0.471 total time=   0.0s\n",
      "[CV 5/5] END .........alpha=0.29763514416313175;, score=0.381 total time=   0.0s\n",
      "[CV 1/5] END .........alpha=0.33598182862837817;, score=0.460 total time=   0.0s\n",
      "[CV 2/5] END .........alpha=0.33598182862837817;, score=0.394 total time=   0.0s\n",
      "[CV 3/5] END .........alpha=0.33598182862837817;, score=0.409 total time=   0.0s\n",
      "[CV 4/5] END .........alpha=0.33598182862837817;, score=0.462 total time=   0.0s\n",
      "[CV 5/5] END .........alpha=0.33598182862837817;, score=0.377 total time=   0.0s\n",
      "[CV 1/5] END .........alpha=0.37926901907322497;, score=0.460 total time=   0.0s\n",
      "[CV 2/5] END .........alpha=0.37926901907322497;, score=0.387 total time=   0.0s\n",
      "[CV 3/5] END .........alpha=0.37926901907322497;, score=0.405 total time=   0.0s\n",
      "[CV 4/5] END .........alpha=0.37926901907322497;, score=0.456 total time=   0.0s\n",
      "[CV 5/5] END .........alpha=0.37926901907322497;, score=0.374 total time=   0.0s\n",
      "[CV 1/5] END .........alpha=0.42813323987193935;, score=0.459 total time=   0.0s\n",
      "[CV 2/5] END .........alpha=0.42813323987193935;, score=0.382 total time=   0.0s\n",
      "[CV 3/5] END .........alpha=0.42813323987193935;, score=0.401 total time=   0.0s\n",
      "[CV 4/5] END .........alpha=0.42813323987193935;, score=0.450 total time=   0.0s\n",
      "[CV 5/5] END .........alpha=0.42813323987193935;, score=0.371 total time=   0.0s\n",
      "[CV 1/5] END ..........alpha=0.4832930238571752;, score=0.456 total time=   0.0s\n",
      "[CV 2/5] END ..........alpha=0.4832930238571752;, score=0.379 total time=   0.0s\n",
      "[CV 3/5] END ..........alpha=0.4832930238571752;, score=0.396 total time=   0.0s\n",
      "[CV 4/5] END ..........alpha=0.4832930238571752;, score=0.445 total time=   0.0s\n",
      "[CV 5/5] END ..........alpha=0.4832930238571752;, score=0.368 total time=   0.0s\n",
      "[CV 1/5] END ..........alpha=0.5455594781168519;, score=0.452 total time=   0.0s\n",
      "[CV 2/5] END ..........alpha=0.5455594781168519;, score=0.377 total time=   0.0s\n",
      "[CV 3/5] END ..........alpha=0.5455594781168519;, score=0.391 total time=   0.0s\n",
      "[CV 4/5] END ..........alpha=0.5455594781168519;, score=0.441 total time=   0.0s\n",
      "[CV 5/5] END ..........alpha=0.5455594781168519;, score=0.363 total time=   0.0s\n",
      "[CV 1/5] END ..........alpha=0.6158482110660264;, score=0.447 total time=   0.0s\n",
      "[CV 2/5] END ..........alpha=0.6158482110660264;, score=0.375 total time=   0.0s\n",
      "[CV 3/5] END ..........alpha=0.6158482110660264;, score=0.384 total time=   0.0s\n",
      "[CV 4/5] END ..........alpha=0.6158482110660264;, score=0.436 total time=   0.0s\n",
      "[CV 5/5] END ..........alpha=0.6158482110660264;, score=0.358 total time=   0.0s\n",
      "[CV 1/5] END ..........alpha=0.6951927961775606;, score=0.441 total time=   0.0s\n",
      "[CV 2/5] END ..........alpha=0.6951927961775606;, score=0.372 total time=   0.0s\n",
      "[CV 3/5] END ..........alpha=0.6951927961775606;, score=0.376 total time=   0.0s\n",
      "[CV 4/5] END ..........alpha=0.6951927961775606;, score=0.431 total time=   0.0s\n",
      "[CV 5/5] END ..........alpha=0.6951927961775606;, score=0.353 total time=   0.0s\n",
      "[CV 1/5] END ..........alpha=0.7847599703514611;, score=0.433 total time=   0.0s\n",
      "[CV 2/5] END ..........alpha=0.7847599703514611;, score=0.367 total time=   0.0s\n",
      "[CV 3/5] END ..........alpha=0.7847599703514611;, score=0.367 total time=   0.0s\n",
      "[CV 4/5] END ..........alpha=0.7847599703514611;, score=0.424 total time=   0.0s\n",
      "[CV 5/5] END ..........alpha=0.7847599703514611;, score=0.348 total time=   0.0s\n",
      "[CV 1/5] END ..........alpha=0.8858667904100825;, score=0.422 total time=   0.0s\n",
      "[CV 2/5] END ..........alpha=0.8858667904100825;, score=0.360 total time=   0.0s\n",
      "[CV 3/5] END ..........alpha=0.8858667904100825;, score=0.356 total time=   0.0s\n",
      "[CV 4/5] END ..........alpha=0.8858667904100825;, score=0.415 total time=   0.0s\n",
      "[CV 5/5] END ..........alpha=0.8858667904100825;, score=0.341 total time=   0.0s\n",
      "[CV 1/5] END .........................alpha=1.0;, score=0.410 total time=   0.0s\n",
      "[CV 2/5] END .........................alpha=1.0;, score=0.352 total time=   0.0s\n",
      "[CV 3/5] END .........................alpha=1.0;, score=0.343 total time=   0.0s\n",
      "[CV 4/5] END .........................alpha=1.0;, score=0.405 total time=   0.0s\n",
      "[CV 5/5] END .........................alpha=1.0;, score=0.332 total time=   0.0s\n",
      "The best validation score obtained is 0.45075 with\n",
      "\talpha: 0.1\n",
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
      "[CV 1/5] END ..C=0.1, epsilon=0.01, kernel=poly;, score=0.078 total time=   0.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END ..C=0.1, epsilon=0.01, kernel=poly;, score=0.033 total time=   0.1s\n",
      "[CV 3/5] END ..C=0.1, epsilon=0.01, kernel=poly;, score=0.039 total time=   0.1s\n",
      "[CV 4/5] END ..C=0.1, epsilon=0.01, kernel=poly;, score=0.049 total time=   0.1s\n",
      "[CV 5/5] END ..C=0.1, epsilon=0.01, kernel=poly;, score=0.019 total time=   0.1s\n",
      "[CV 1/5] END ...C=0.1, epsilon=0.01, kernel=rbf;, score=0.129 total time=   0.1s\n",
      "[CV 2/5] END ...C=0.1, epsilon=0.01, kernel=rbf;, score=0.109 total time=   0.1s\n",
      "[CV 3/5] END ...C=0.1, epsilon=0.01, kernel=rbf;, score=0.102 total time=   0.1s\n",
      "[CV 4/5] END ...C=0.1, epsilon=0.01, kernel=rbf;, score=0.115 total time=   0.1s\n",
      "[CV 5/5] END ...C=0.1, epsilon=0.01, kernel=rbf;, score=0.110 total time=   0.1s\n",
      "[CV 1/5] END C=0.1, epsilon=0.01, kernel=sigmoid;, score=0.313 total time=   0.1s\n",
      "[CV 2/5] END C=0.1, epsilon=0.01, kernel=sigmoid;, score=0.265 total time=   0.1s\n",
      "[CV 3/5] END C=0.1, epsilon=0.01, kernel=sigmoid;, score=0.239 total time=   0.1s\n",
      "[CV 4/5] END C=0.1, epsilon=0.01, kernel=sigmoid;, score=0.265 total time=   0.1s\n",
      "[CV 5/5] END C=0.1, epsilon=0.01, kernel=sigmoid;, score=0.244 total time=   0.1s\n",
      "[CV 1/5] END C=0.1, epsilon=0.31622776601683794, kernel=poly;, score=0.078 total time=   0.1s\n",
      "[CV 2/5] END C=0.1, epsilon=0.31622776601683794, kernel=poly;, score=0.028 total time=   0.1s\n",
      "[CV 3/5] END C=0.1, epsilon=0.31622776601683794, kernel=poly;, score=0.040 total time=   0.1s\n",
      "[CV 4/5] END C=0.1, epsilon=0.31622776601683794, kernel=poly;, score=0.048 total time=   0.1s\n",
      "[CV 5/5] END C=0.1, epsilon=0.31622776601683794, kernel=poly;, score=0.016 total time=   0.1s\n",
      "[CV 1/5] END C=0.1, epsilon=0.31622776601683794, kernel=rbf;, score=0.128 total time=   0.1s\n",
      "[CV 2/5] END C=0.1, epsilon=0.31622776601683794, kernel=rbf;, score=0.110 total time=   0.1s\n",
      "[CV 3/5] END C=0.1, epsilon=0.31622776601683794, kernel=rbf;, score=0.098 total time=   0.1s\n",
      "[CV 4/5] END C=0.1, epsilon=0.31622776601683794, kernel=rbf;, score=0.116 total time=   0.1s\n",
      "[CV 5/5] END C=0.1, epsilon=0.31622776601683794, kernel=rbf;, score=0.108 total time=   0.1s\n",
      "[CV 1/5] END C=0.1, epsilon=0.31622776601683794, kernel=sigmoid;, score=0.312 total time=   0.1s\n",
      "[CV 2/5] END C=0.1, epsilon=0.31622776601683794, kernel=sigmoid;, score=0.265 total time=   0.1s\n",
      "[CV 3/5] END C=0.1, epsilon=0.31622776601683794, kernel=sigmoid;, score=0.241 total time=   0.1s\n",
      "[CV 4/5] END C=0.1, epsilon=0.31622776601683794, kernel=sigmoid;, score=0.265 total time=   0.1s\n",
      "[CV 5/5] END C=0.1, epsilon=0.31622776601683794, kernel=sigmoid;, score=0.243 total time=   0.1s\n",
      "[CV 1/5] END ..C=0.1, epsilon=10.0, kernel=poly;, score=0.034 total time=   0.0s\n",
      "[CV 2/5] END ..C=0.1, epsilon=10.0, kernel=poly;, score=0.025 total time=   0.0s\n",
      "[CV 3/5] END ..C=0.1, epsilon=10.0, kernel=poly;, score=0.030 total time=   0.0s\n",
      "[CV 4/5] END ..C=0.1, epsilon=10.0, kernel=poly;, score=0.037 total time=   0.0s\n",
      "[CV 5/5] END ..C=0.1, epsilon=10.0, kernel=poly;, score=0.017 total time=   0.0s\n",
      "[CV 1/5] END ...C=0.1, epsilon=10.0, kernel=rbf;, score=0.057 total time=   0.0s\n",
      "[CV 2/5] END ...C=0.1, epsilon=10.0, kernel=rbf;, score=0.063 total time=   0.0s\n",
      "[CV 3/5] END ...C=0.1, epsilon=10.0, kernel=rbf;, score=0.062 total time=   0.0s\n",
      "[CV 4/5] END ...C=0.1, epsilon=10.0, kernel=rbf;, score=0.064 total time=   0.0s\n",
      "[CV 5/5] END ...C=0.1, epsilon=10.0, kernel=rbf;, score=0.069 total time=   0.0s\n",
      "[CV 1/5] END C=0.1, epsilon=10.0, kernel=sigmoid;, score=0.210 total time=   0.0s\n",
      "[CV 2/5] END C=0.1, epsilon=10.0, kernel=sigmoid;, score=0.179 total time=   0.0s\n",
      "[CV 3/5] END C=0.1, epsilon=10.0, kernel=sigmoid;, score=0.183 total time=   0.0s\n",
      "[CV 4/5] END C=0.1, epsilon=10.0, kernel=sigmoid;, score=0.182 total time=   0.0s\n",
      "[CV 5/5] END C=0.1, epsilon=10.0, kernel=sigmoid;, score=0.196 total time=   0.0s\n",
      "[CV 1/5] END C=1.1659144011798317, epsilon=0.01, kernel=poly;, score=0.284 total time=   0.1s\n",
      "[CV 2/5] END C=1.1659144011798317, epsilon=0.01, kernel=poly;, score=0.165 total time=   0.1s\n",
      "[CV 3/5] END C=1.1659144011798317, epsilon=0.01, kernel=poly;, score=0.153 total time=   0.1s\n",
      "[CV 4/5] END C=1.1659144011798317, epsilon=0.01, kernel=poly;, score=0.178 total time=   0.1s\n",
      "[CV 5/5] END C=1.1659144011798317, epsilon=0.01, kernel=poly;, score=0.135 total time=   0.1s\n",
      "[CV 1/5] END C=1.1659144011798317, epsilon=0.01, kernel=rbf;, score=0.463 total time=   0.1s\n",
      "[CV 2/5] END C=1.1659144011798317, epsilon=0.01, kernel=rbf;, score=0.396 total time=   0.1s\n",
      "[CV 3/5] END C=1.1659144011798317, epsilon=0.01, kernel=rbf;, score=0.362 total time=   0.1s\n",
      "[CV 4/5] END C=1.1659144011798317, epsilon=0.01, kernel=rbf;, score=0.406 total time=   0.1s\n",
      "[CV 5/5] END C=1.1659144011798317, epsilon=0.01, kernel=rbf;, score=0.413 total time=   0.1s\n",
      "[CV 1/5] END C=1.1659144011798317, epsilon=0.01, kernel=sigmoid;, score=0.335 total time=   0.1s\n",
      "[CV 2/5] END C=1.1659144011798317, epsilon=0.01, kernel=sigmoid;, score=0.338 total time=   0.1s\n",
      "[CV 3/5] END C=1.1659144011798317, epsilon=0.01, kernel=sigmoid;, score=0.288 total time=   0.1s\n",
      "[CV 4/5] END C=1.1659144011798317, epsilon=0.01, kernel=sigmoid;, score=0.418 total time=   0.1s\n",
      "[CV 5/5] END C=1.1659144011798317, epsilon=0.01, kernel=sigmoid;, score=0.282 total time=   0.1s\n",
      "[CV 1/5] END C=1.1659144011798317, epsilon=0.31622776601683794, kernel=poly;, score=0.284 total time=   0.1s\n",
      "[CV 2/5] END C=1.1659144011798317, epsilon=0.31622776601683794, kernel=poly;, score=0.164 total time=   0.1s\n",
      "[CV 3/5] END C=1.1659144011798317, epsilon=0.31622776601683794, kernel=poly;, score=0.158 total time=   0.1s\n",
      "[CV 4/5] END C=1.1659144011798317, epsilon=0.31622776601683794, kernel=poly;, score=0.179 total time=   0.1s\n",
      "[CV 5/5] END C=1.1659144011798317, epsilon=0.31622776601683794, kernel=poly;, score=0.134 total time=   0.1s\n",
      "[CV 1/5] END C=1.1659144011798317, epsilon=0.31622776601683794, kernel=rbf;, score=0.463 total time=   0.1s\n",
      "[CV 2/5] END C=1.1659144011798317, epsilon=0.31622776601683794, kernel=rbf;, score=0.396 total time=   0.1s\n",
      "[CV 3/5] END C=1.1659144011798317, epsilon=0.31622776601683794, kernel=rbf;, score=0.364 total time=   0.1s\n",
      "[CV 4/5] END C=1.1659144011798317, epsilon=0.31622776601683794, kernel=rbf;, score=0.406 total time=   0.1s\n",
      "[CV 5/5] END C=1.1659144011798317, epsilon=0.31622776601683794, kernel=rbf;, score=0.413 total time=   0.1s\n",
      "[CV 1/5] END C=1.1659144011798317, epsilon=0.31622776601683794, kernel=sigmoid;, score=0.332 total time=   0.1s\n",
      "[CV 2/5] END C=1.1659144011798317, epsilon=0.31622776601683794, kernel=sigmoid;, score=0.335 total time=   0.1s\n",
      "[CV 3/5] END C=1.1659144011798317, epsilon=0.31622776601683794, kernel=sigmoid;, score=0.285 total time=   0.1s\n",
      "[CV 4/5] END C=1.1659144011798317, epsilon=0.31622776601683794, kernel=sigmoid;, score=0.421 total time=   0.1s\n",
      "[CV 5/5] END C=1.1659144011798317, epsilon=0.31622776601683794, kernel=sigmoid;, score=0.278 total time=   0.1s\n",
      "[CV 1/5] END C=1.1659144011798317, epsilon=10.0, kernel=poly;, score=0.164 total time=   0.0s\n",
      "[CV 2/5] END C=1.1659144011798317, epsilon=10.0, kernel=poly;, score=0.103 total time=   0.0s\n",
      "[CV 3/5] END C=1.1659144011798317, epsilon=10.0, kernel=poly;, score=0.101 total time=   0.0s\n",
      "[CV 4/5] END C=1.1659144011798317, epsilon=10.0, kernel=poly;, score=0.110 total time=   0.0s\n",
      "[CV 5/5] END C=1.1659144011798317, epsilon=10.0, kernel=poly;, score=0.063 total time=   0.0s\n",
      "[CV 1/5] END C=1.1659144011798317, epsilon=10.0, kernel=rbf;, score=0.332 total time=   0.0s\n",
      "[CV 2/5] END C=1.1659144011798317, epsilon=10.0, kernel=rbf;, score=0.295 total time=   0.0s\n",
      "[CV 3/5] END C=1.1659144011798317, epsilon=10.0, kernel=rbf;, score=0.294 total time=   0.0s\n",
      "[CV 4/5] END C=1.1659144011798317, epsilon=10.0, kernel=rbf;, score=0.302 total time=   0.0s\n",
      "[CV 5/5] END C=1.1659144011798317, epsilon=10.0, kernel=rbf;, score=0.337 total time=   0.0s\n",
      "[CV 1/5] END C=1.1659144011798317, epsilon=10.0, kernel=sigmoid;, score=0.327 total time=   0.0s\n",
      "[CV 2/5] END C=1.1659144011798317, epsilon=10.0, kernel=sigmoid;, score=0.322 total time=   0.0s\n",
      "[CV 3/5] END C=1.1659144011798317, epsilon=10.0, kernel=sigmoid;, score=0.319 total time=   0.0s\n",
      "[CV 4/5] END C=1.1659144011798317, epsilon=10.0, kernel=sigmoid;, score=0.368 total time=   0.0s\n",
      "[CV 5/5] END C=1.1659144011798317, epsilon=10.0, kernel=sigmoid;, score=0.315 total time=   0.0s\n",
      "[CV 1/5] END C=13.593563908785255, epsilon=0.01, kernel=poly;, score=0.476 total time=   0.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END C=13.593563908785255, epsilon=0.01, kernel=poly;, score=0.391 total time=   0.1s\n",
      "[CV 3/5] END C=13.593563908785255, epsilon=0.01, kernel=poly;, score=0.391 total time=   0.1s\n",
      "[CV 4/5] END C=13.593563908785255, epsilon=0.01, kernel=poly;, score=0.384 total time=   0.1s\n",
      "[CV 5/5] END C=13.593563908785255, epsilon=0.01, kernel=poly;, score=0.360 total time=   0.1s\n",
      "[CV 1/5] END C=13.593563908785255, epsilon=0.01, kernel=rbf;, score=0.618 total time=   0.1s\n",
      "[CV 2/5] END C=13.593563908785255, epsilon=0.01, kernel=rbf;, score=0.542 total time=   0.1s\n",
      "[CV 3/5] END C=13.593563908785255, epsilon=0.01, kernel=rbf;, score=0.510 total time=   0.1s\n",
      "[CV 4/5] END C=13.593563908785255, epsilon=0.01, kernel=rbf;, score=0.556 total time=   0.1s\n",
      "[CV 5/5] END C=13.593563908785255, epsilon=0.01, kernel=rbf;, score=0.575 total time=   0.1s\n",
      "[CV 1/5] END C=13.593563908785255, epsilon=0.01, kernel=sigmoid;, score=-38.302 total time=   0.1s\n",
      "[CV 2/5] END C=13.593563908785255, epsilon=0.01, kernel=sigmoid;, score=-23.980 total time=   0.1s\n",
      "[CV 3/5] END C=13.593563908785255, epsilon=0.01, kernel=sigmoid;, score=-23.735 total time=   0.1s\n",
      "[CV 4/5] END C=13.593563908785255, epsilon=0.01, kernel=sigmoid;, score=-14.345 total time=   0.1s\n",
      "[CV 5/5] END C=13.593563908785255, epsilon=0.01, kernel=sigmoid;, score=-30.839 total time=   0.1s\n",
      "[CV 1/5] END C=13.593563908785255, epsilon=0.31622776601683794, kernel=poly;, score=0.471 total time=   0.1s\n",
      "[CV 2/5] END C=13.593563908785255, epsilon=0.31622776601683794, kernel=poly;, score=0.389 total time=   0.1s\n",
      "[CV 3/5] END C=13.593563908785255, epsilon=0.31622776601683794, kernel=poly;, score=0.389 total time=   0.1s\n",
      "[CV 4/5] END C=13.593563908785255, epsilon=0.31622776601683794, kernel=poly;, score=0.380 total time=   0.1s\n",
      "[CV 5/5] END C=13.593563908785255, epsilon=0.31622776601683794, kernel=poly;, score=0.354 total time=   0.1s\n",
      "[CV 1/5] END C=13.593563908785255, epsilon=0.31622776601683794, kernel=rbf;, score=0.617 total time=   0.1s\n",
      "[CV 2/5] END C=13.593563908785255, epsilon=0.31622776601683794, kernel=rbf;, score=0.541 total time=   0.1s\n",
      "[CV 3/5] END C=13.593563908785255, epsilon=0.31622776601683794, kernel=rbf;, score=0.510 total time=   0.1s\n",
      "[CV 4/5] END C=13.593563908785255, epsilon=0.31622776601683794, kernel=rbf;, score=0.554 total time=   0.1s\n",
      "[CV 5/5] END C=13.593563908785255, epsilon=0.31622776601683794, kernel=rbf;, score=0.575 total time=   0.1s\n",
      "[CV 1/5] END C=13.593563908785255, epsilon=0.31622776601683794, kernel=sigmoid;, score=-38.876 total time=   0.1s\n",
      "[CV 2/5] END C=13.593563908785255, epsilon=0.31622776601683794, kernel=sigmoid;, score=-34.095 total time=   0.1s\n",
      "[CV 3/5] END C=13.593563908785255, epsilon=0.31622776601683794, kernel=sigmoid;, score=-24.307 total time=   0.1s\n",
      "[CV 4/5] END C=13.593563908785255, epsilon=0.31622776601683794, kernel=sigmoid;, score=-14.229 total time=   0.1s\n",
      "[CV 5/5] END C=13.593563908785255, epsilon=0.31622776601683794, kernel=sigmoid;, score=-22.865 total time=   0.1s\n",
      "[CV 1/5] END C=13.593563908785255, epsilon=10.0, kernel=poly;, score=0.259 total time=   0.0s\n",
      "[CV 2/5] END C=13.593563908785255, epsilon=10.0, kernel=poly;, score=0.178 total time=   0.0s\n",
      "[CV 3/5] END C=13.593563908785255, epsilon=10.0, kernel=poly;, score=0.190 total time=   0.0s\n",
      "[CV 4/5] END C=13.593563908785255, epsilon=10.0, kernel=poly;, score=0.158 total time=   0.0s\n",
      "[CV 5/5] END C=13.593563908785255, epsilon=10.0, kernel=poly;, score=0.125 total time=   0.0s\n",
      "[CV 1/5] END C=13.593563908785255, epsilon=10.0, kernel=rbf;, score=0.456 total time=   0.0s\n",
      "[CV 2/5] END C=13.593563908785255, epsilon=10.0, kernel=rbf;, score=0.388 total time=   0.0s\n",
      "[CV 3/5] END C=13.593563908785255, epsilon=10.0, kernel=rbf;, score=0.439 total time=   0.0s\n",
      "[CV 4/5] END C=13.593563908785255, epsilon=10.0, kernel=rbf;, score=0.400 total time=   0.0s\n",
      "[CV 5/5] END C=13.593563908785255, epsilon=10.0, kernel=rbf;, score=0.456 total time=   0.0s\n",
      "[CV 1/5] END C=13.593563908785255, epsilon=10.0, kernel=sigmoid;, score=-24.380 total time=   0.1s\n",
      "[CV 2/5] END C=13.593563908785255, epsilon=10.0, kernel=sigmoid;, score=-25.931 total time=   0.1s\n",
      "[CV 3/5] END C=13.593563908785255, epsilon=10.0, kernel=sigmoid;, score=-18.045 total time=   0.1s\n",
      "[CV 4/5] END C=13.593563908785255, epsilon=10.0, kernel=sigmoid;, score=-11.002 total time=   0.1s\n",
      "[CV 5/5] END C=13.593563908785255, epsilon=10.0, kernel=sigmoid;, score=-21.963 total time=   0.1s\n",
      "[CV 1/5] END C=158.48931924611142, epsilon=0.01, kernel=poly;, score=0.334 total time=   0.2s\n",
      "[CV 2/5] END C=158.48931924611142, epsilon=0.01, kernel=poly;, score=0.194 total time=   0.1s\n",
      "[CV 3/5] END C=158.48931924611142, epsilon=0.01, kernel=poly;, score=0.371 total time=   0.1s\n",
      "[CV 4/5] END C=158.48931924611142, epsilon=0.01, kernel=poly;, score=0.243 total time=   0.2s\n",
      "[CV 5/5] END C=158.48931924611142, epsilon=0.01, kernel=poly;, score=0.282 total time=   0.1s\n",
      "[CV 1/5] END C=158.48931924611142, epsilon=0.01, kernel=rbf;, score=0.574 total time=   0.2s\n",
      "[CV 2/5] END C=158.48931924611142, epsilon=0.01, kernel=rbf;, score=0.534 total time=   0.2s\n",
      "[CV 3/5] END C=158.48931924611142, epsilon=0.01, kernel=rbf;, score=0.521 total time=   0.1s\n",
      "[CV 4/5] END C=158.48931924611142, epsilon=0.01, kernel=rbf;, score=0.548 total time=   0.1s\n",
      "[CV 5/5] END C=158.48931924611142, epsilon=0.01, kernel=rbf;, score=0.592 total time=   0.1s\n",
      "[CV 1/5] END C=158.48931924611142, epsilon=0.01, kernel=sigmoid;, score=-6509.379 total time=   0.1s\n",
      "[CV 2/5] END C=158.48931924611142, epsilon=0.01, kernel=sigmoid;, score=-1244.877 total time=   0.1s\n",
      "[CV 3/5] END C=158.48931924611142, epsilon=0.01, kernel=sigmoid;, score=-3606.710 total time=   0.1s\n",
      "[CV 4/5] END C=158.48931924611142, epsilon=0.01, kernel=sigmoid;, score=-2392.404 total time=   0.1s\n",
      "[CV 5/5] END C=158.48931924611142, epsilon=0.01, kernel=sigmoid;, score=-4367.101 total time=   0.1s\n",
      "[CV 1/5] END C=158.48931924611142, epsilon=0.31622776601683794, kernel=poly;, score=0.340 total time=   0.1s\n",
      "[CV 2/5] END C=158.48931924611142, epsilon=0.31622776601683794, kernel=poly;, score=0.199 total time=   0.1s\n",
      "[CV 3/5] END C=158.48931924611142, epsilon=0.31622776601683794, kernel=poly;, score=0.371 total time=   0.1s\n",
      "[CV 4/5] END C=158.48931924611142, epsilon=0.31622776601683794, kernel=poly;, score=0.244 total time=   0.2s\n",
      "[CV 5/5] END C=158.48931924611142, epsilon=0.31622776601683794, kernel=poly;, score=0.278 total time=   0.1s\n",
      "[CV 1/5] END C=158.48931924611142, epsilon=0.31622776601683794, kernel=rbf;, score=0.572 total time=   0.1s\n",
      "[CV 2/5] END C=158.48931924611142, epsilon=0.31622776601683794, kernel=rbf;, score=0.532 total time=   0.1s\n",
      "[CV 3/5] END C=158.48931924611142, epsilon=0.31622776601683794, kernel=rbf;, score=0.522 total time=   0.1s\n",
      "[CV 4/5] END C=158.48931924611142, epsilon=0.31622776601683794, kernel=rbf;, score=0.547 total time=   0.1s\n",
      "[CV 5/5] END C=158.48931924611142, epsilon=0.31622776601683794, kernel=rbf;, score=0.591 total time=   0.1s\n",
      "[CV 1/5] END C=158.48931924611142, epsilon=0.31622776601683794, kernel=sigmoid;, score=-6652.538 total time=   0.1s\n",
      "[CV 2/5] END C=158.48931924611142, epsilon=0.31622776601683794, kernel=sigmoid;, score=-1278.187 total time=   0.1s\n",
      "[CV 3/5] END C=158.48931924611142, epsilon=0.31622776601683794, kernel=sigmoid;, score=-3634.805 total time=   0.1s\n",
      "[CV 4/5] END C=158.48931924611142, epsilon=0.31622776601683794, kernel=sigmoid;, score=-2312.355 total time=   0.1s\n",
      "[CV 5/5] END C=158.48931924611142, epsilon=0.31622776601683794, kernel=sigmoid;, score=-4481.973 total time=   0.1s\n",
      "[CV 1/5] END C=158.48931924611142, epsilon=10.0, kernel=poly;, score=0.217 total time=   0.0s\n",
      "[CV 2/5] END C=158.48931924611142, epsilon=10.0, kernel=poly;, score=0.137 total time=   0.0s\n",
      "[CV 3/5] END C=158.48931924611142, epsilon=10.0, kernel=poly;, score=0.206 total time=   0.0s\n",
      "[CV 4/5] END C=158.48931924611142, epsilon=10.0, kernel=poly;, score=0.103 total time=   0.0s\n",
      "[CV 5/5] END C=158.48931924611142, epsilon=10.0, kernel=poly;, score=0.107 total time=   0.0s\n",
      "[CV 1/5] END C=158.48931924611142, epsilon=10.0, kernel=rbf;, score=0.427 total time=   0.0s\n",
      "[CV 2/5] END C=158.48931924611142, epsilon=10.0, kernel=rbf;, score=0.378 total time=   0.0s\n",
      "[CV 3/5] END C=158.48931924611142, epsilon=10.0, kernel=rbf;, score=0.438 total time=   0.0s\n",
      "[CV 4/5] END C=158.48931924611142, epsilon=10.0, kernel=rbf;, score=0.386 total time=   0.0s\n",
      "[CV 5/5] END C=158.48931924611142, epsilon=10.0, kernel=rbf;, score=0.449 total time=   0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END C=158.48931924611142, epsilon=10.0, kernel=sigmoid;, score=-6689.595 total time=   0.1s\n",
      "[CV 2/5] END C=158.48931924611142, epsilon=10.0, kernel=sigmoid;, score=-1260.672 total time=   0.1s\n",
      "[CV 3/5] END C=158.48931924611142, epsilon=10.0, kernel=sigmoid;, score=-3596.458 total time=   0.1s\n",
      "[CV 4/5] END C=158.48931924611142, epsilon=10.0, kernel=sigmoid;, score=-2335.250 total time=   0.1s\n",
      "[CV 5/5] END C=158.48931924611142, epsilon=10.0, kernel=sigmoid;, score=-4441.638 total time=   0.1s\n",
      "The best validation score obtained is 0.56008 with\n",
      "    \tkernel: rbf\n",
      "    \tC: 13.593563908785255\n",
      "    \tepsilon: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:   12.9s remaining:   19.4s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:   15.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation score obtained is 0.5911 with\n",
      "    \t{'loss': 'ls', 'n_estimators': 250, 'learning_rate': 0.025, 'subsample': 0.75, 'max_depth': 6, 'min_samples_split': 5, 'n_iter_no_change': 100, 'validation_fraction': 0.1, 'random_state': 0, 'verbose': 1}\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1          94.2287           1.9422            8.18s\n",
      "         2          90.9186           2.0753            7.53s\n",
      "         3          84.7186           1.6382            7.21s\n",
      "         4          83.6806           2.4806            7.24s\n",
      "         5          80.7916           1.7114            7.19s\n",
      "         6          76.7468           1.4421            7.11s\n",
      "         7          74.3124           1.3496            7.10s\n",
      "         8          72.5802           1.2955            7.10s\n",
      "         9          67.5527           1.4972            7.06s\n",
      "        10          70.2389           1.5653            7.02s\n",
      "        20          50.7553           0.6170            6.74s\n",
      "        30          37.0051           0.3434            6.44s\n",
      "        40          28.7143           0.2254            6.14s\n",
      "        50          22.1906           0.1570            5.85s\n",
      "        60          16.8876           0.1750            5.59s\n",
      "        70          12.8043           0.0694            5.31s\n",
      "        80          10.5193           0.0620            5.02s\n",
      "        90           8.7576           0.0273            4.72s\n",
      "       100           7.3454           0.0465            4.43s\n",
      "       200           1.4566          -0.0041            1.49s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-16-4d2dfc667113>:57: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    }
   ],
   "source": [
    "gs = model(X_train_cleaned, np.array(y_train_cleaned).ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "otherwise-leather",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = pd.DataFrame(gs.predict(X_test_cleaned))\n",
    "SUB_ID = 6 #to modify\n",
    "create_submission(prediction, SUB_ID)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

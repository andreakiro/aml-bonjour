{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "suburban-concrete",
   "metadata": {},
   "source": [
    "This Pipeline is based on the computer vision lab where the functions were modified for the purpose of this lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "supposed-juice",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from scipy.stats import zscore\n",
    "from sklearn.metrics import r2_score\n",
    "#from sklearn.model_selection import train_test_split\n",
    "from numpy.random import RandomState\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "handmade-bumper",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED= 123456789\n",
    "BATCH_SIZE = 10\n",
    "NUM_WORKERS = 4\n",
    "NUM_EPOCHS = 100\n",
    "VALIDATION_INDEX = 750 #We will take the VALIDATION_INDEX firts samples as training and the remaining as validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "excess-aircraft",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updating mean class for loss aggregation.\n",
    "class UpdatingMean():\n",
    "    def __init__(self):\n",
    "        self.sum = 0\n",
    "        self.n = 0\n",
    "\n",
    "    def mean(self):\n",
    "        return self.sum / self.n\n",
    "\n",
    "    def add(self, loss):\n",
    "        self.sum += loss\n",
    "        self.n += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "powered-springfield",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MRIDataset(Dataset):\n",
    "    def __init__(self, train=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        X_train_test = pd.read_csv('data/X_train_cleaned.csv')\n",
    "        y_train_test = pd.read_csv('data/y_train_cleaned.csv')\n",
    "        \n",
    "        #Split in test and validation:\n",
    "        prng = RandomState(SEED)\n",
    "        train_mask = prng.rand(len(X_train_test)) < 0.8 #~80% of training and 20% of validation\n",
    "                \n",
    "        if(train):\n",
    "            self.samples = X_train_test[train_mask].to_numpy()\n",
    "            self.annotations= y_train_test[train_mask].to_numpy()\n",
    "        else:\n",
    "            self.samples = X_train_test[~train_mask].to_numpy()\n",
    "            self.annotations= y_train_test[~train_mask].to_numpy()\n",
    "                                        \n",
    "    def __len__(self):\n",
    "        # Returns the number of samples in the dataset.\n",
    "        return self.samples.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Returns the sample and annotation with index idx.\n",
    "        sample = self.samples[idx]\n",
    "        annotation = self.annotations[idx]\n",
    "                \n",
    "        # Convert to tensor.\n",
    "        return {\n",
    "            'input': torch.from_numpy(sample).float(),\n",
    "            'annotation': torch.from_numpy(annotation).float()\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "adjustable-missile",
   "metadata": {},
   "outputs": [],
   "source": [
    "def r2_loss(output, target):\n",
    "    target_mean = torch.mean(target)\n",
    "    ss_tot = torch.sum((target - target_mean) ** 2)\n",
    "    ss_res = torch.sum((target - output) ** 2)\n",
    "    r2 = 1 - ss_res / ss_tot if ss_tot != 0 else torch.zeros(1) #What to do when divided by zero?\n",
    "    return r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "systematic-superintendent",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training_epoch(net, optimizer, dataloader):\n",
    "    loss_aggregator = UpdatingMean()\n",
    "    # Put the network in training mode.\n",
    "    net.train()\n",
    "    # Loop over batches.\n",
    "    for batch in dataloader:\n",
    "        # Reset gradients.\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass.\n",
    "        X = batch[\"input\"]\n",
    "        y = batch[\"annotation\"]\n",
    "        output = net(X)\n",
    "\n",
    "        # Compute the loss - MAE\n",
    "        loss = nn.L1Loss()\n",
    "        output_loss = loss(output, y)\n",
    "\n",
    "        # Backwards pass.\n",
    "        output_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Save loss value in the aggregator.\n",
    "        loss_aggregator.add(output_loss.detach().numpy())\n",
    "    return loss_aggregator.mean()\n",
    "\n",
    "\n",
    "def compute_accuracy(output, labels):\n",
    "    return r2_loss(output, labels)\n",
    "\n",
    "\n",
    "def run_validation_epoch(net, dataloader):\n",
    "    accuracy_aggregator = UpdatingMean()\n",
    "    # Put the network in evaluation mode.\n",
    "    net.eval()\n",
    "    # Loop over batches.\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        # Forward pass only.\n",
    "        output = net(batch['input'])\n",
    "\n",
    "        # Compute the accuracy using compute_accuracy.\n",
    "        accuracy = compute_accuracy(output.detach().numpy(), batch['annotation'])\n",
    "\n",
    "        # Save accuracy value in the aggregator.\n",
    "        accuracy_aggregator.add(accuracy.item())\n",
    "    return accuracy_aggregator.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "preliminary-shower",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPTest(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.codename = 'mlp'\n",
    "\n",
    "        # Define the network layers in order.\n",
    "        # Input is 726D.\n",
    "        # Output is a single value.\n",
    "        # Multiple linear layers each followed by a LeakyReLU non-linearity.\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(726, 100),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(100, 150),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(150, 1),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "    \n",
    "    def forward(self, batch):\n",
    "        # Process batch using the layers.\n",
    "        x = self.layers(batch)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "informational-penetration",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 01] Loss: 34.8019\n",
      "[Epoch 01] r2_score.: -15.3883\n",
      "[Epoch 02] Loss: 22.0019\n",
      "[Epoch 02] r2_score.: -14.0344\n",
      "[Epoch 03] Loss: 16.7106\n",
      "[Epoch 03] r2_score.: -9.1285\n",
      "[Epoch 04] Loss: 14.1468\n",
      "[Epoch 04] r2_score.: -8.6176\n",
      "[Epoch 05] Loss: 11.5266\n",
      "[Epoch 05] r2_score.: -7.9749\n",
      "[Epoch 06] Loss: 10.8761\n",
      "[Epoch 06] r2_score.: -6.8809\n",
      "[Epoch 07] Loss: 9.5303\n",
      "[Epoch 07] r2_score.: -6.4303\n",
      "[Epoch 08] Loss: 9.7316\n",
      "[Epoch 08] r2_score.: -7.7254\n",
      "[Epoch 09] Loss: 9.9943\n",
      "[Epoch 09] r2_score.: -8.0056\n",
      "[Epoch 10] Loss: 9.0102\n",
      "[Epoch 10] r2_score.: -7.2296\n",
      "[Epoch 11] Loss: 8.0100\n",
      "[Epoch 11] r2_score.: -6.9948\n",
      "[Epoch 12] Loss: 7.9675\n",
      "[Epoch 12] r2_score.: -7.3746\n",
      "[Epoch 13] Loss: 7.9124\n",
      "[Epoch 13] r2_score.: -5.8719\n",
      "[Epoch 14] Loss: 7.2796\n",
      "[Epoch 14] r2_score.: -6.3524\n",
      "[Epoch 15] Loss: 7.3535\n",
      "[Epoch 15] r2_score.: -7.1675\n",
      "[Epoch 16] Loss: 6.8215\n",
      "[Epoch 16] r2_score.: -5.3834\n",
      "[Epoch 17] Loss: 6.7409\n",
      "[Epoch 17] r2_score.: -5.9551\n",
      "[Epoch 18] Loss: 6.7115\n",
      "[Epoch 18] r2_score.: -8.4022\n",
      "[Epoch 19] Loss: 6.9097\n",
      "[Epoch 19] r2_score.: -7.2632\n",
      "[Epoch 20] Loss: 5.8770\n",
      "[Epoch 20] r2_score.: -8.2364\n",
      "[Epoch 21] Loss: 6.4420\n",
      "[Epoch 21] r2_score.: -6.6146\n",
      "[Epoch 22] Loss: 6.3410\n",
      "[Epoch 22] r2_score.: -7.8521\n",
      "[Epoch 23] Loss: 5.6834\n",
      "[Epoch 23] r2_score.: -6.5350\n",
      "[Epoch 24] Loss: 5.9275\n",
      "[Epoch 24] r2_score.: -6.6314\n",
      "[Epoch 25] Loss: 6.4547\n",
      "[Epoch 25] r2_score.: -8.0506\n",
      "[Epoch 26] Loss: 6.7148\n",
      "[Epoch 26] r2_score.: -5.5966\n",
      "[Epoch 27] Loss: 5.7904\n",
      "[Epoch 27] r2_score.: -7.8160\n",
      "[Epoch 28] Loss: 5.6924\n",
      "[Epoch 28] r2_score.: -7.3280\n",
      "[Epoch 29] Loss: 5.3796\n",
      "[Epoch 29] r2_score.: -7.0391\n",
      "[Epoch 30] Loss: 5.2715\n",
      "[Epoch 30] r2_score.: -7.0336\n",
      "[Epoch 31] Loss: 5.4548\n",
      "[Epoch 31] r2_score.: -7.0896\n",
      "[Epoch 32] Loss: 5.3940\n",
      "[Epoch 32] r2_score.: -5.6130\n",
      "[Epoch 33] Loss: 5.2886\n",
      "[Epoch 33] r2_score.: -6.7010\n",
      "[Epoch 34] Loss: 4.8859\n",
      "[Epoch 34] r2_score.: -8.4135\n",
      "[Epoch 35] Loss: 5.0681\n",
      "[Epoch 35] r2_score.: -7.5850\n",
      "[Epoch 36] Loss: 5.0145\n",
      "[Epoch 36] r2_score.: -6.2591\n",
      "[Epoch 37] Loss: 5.0672\n",
      "[Epoch 37] r2_score.: -6.0650\n",
      "[Epoch 38] Loss: 5.1967\n",
      "[Epoch 38] r2_score.: -5.5991\n",
      "[Epoch 39] Loss: 4.9478\n",
      "[Epoch 39] r2_score.: -8.3181\n",
      "[Epoch 40] Loss: 4.9586\n",
      "[Epoch 40] r2_score.: -6.7312\n",
      "[Epoch 41] Loss: 4.6150\n",
      "[Epoch 41] r2_score.: -6.4365\n",
      "[Epoch 42] Loss: 4.6376\n",
      "[Epoch 42] r2_score.: -5.3951\n",
      "[Epoch 43] Loss: 4.4571\n",
      "[Epoch 43] r2_score.: -6.6157\n",
      "[Epoch 44] Loss: 4.3447\n",
      "[Epoch 44] r2_score.: -6.7233\n",
      "[Epoch 45] Loss: 4.4585\n",
      "[Epoch 45] r2_score.: -5.7134\n",
      "[Epoch 46] Loss: 4.5557\n",
      "[Epoch 46] r2_score.: -7.4289\n",
      "[Epoch 47] Loss: 4.1600\n",
      "[Epoch 47] r2_score.: -7.2557\n",
      "[Epoch 48] Loss: 3.9767\n",
      "[Epoch 48] r2_score.: -7.4379\n",
      "[Epoch 49] Loss: 4.2690\n",
      "[Epoch 49] r2_score.: -6.0073\n",
      "[Epoch 50] Loss: 3.9732\n",
      "[Epoch 50] r2_score.: -7.2476\n",
      "[Epoch 51] Loss: 4.0959\n",
      "[Epoch 51] r2_score.: -5.9350\n",
      "[Epoch 52] Loss: 3.8248\n",
      "[Epoch 52] r2_score.: -6.2817\n",
      "[Epoch 53] Loss: 3.9667\n",
      "[Epoch 53] r2_score.: -6.9966\n",
      "[Epoch 54] Loss: 3.9166\n",
      "[Epoch 54] r2_score.: -7.7624\n",
      "[Epoch 55] Loss: 3.7082\n",
      "[Epoch 55] r2_score.: -6.6912\n",
      "[Epoch 56] Loss: 3.9096\n",
      "[Epoch 56] r2_score.: -7.2194\n",
      "[Epoch 57] Loss: 3.7978\n",
      "[Epoch 57] r2_score.: -6.1778\n",
      "[Epoch 58] Loss: 3.9048\n",
      "[Epoch 58] r2_score.: -8.1129\n",
      "[Epoch 59] Loss: 3.6401\n",
      "[Epoch 59] r2_score.: -6.5012\n",
      "[Epoch 60] Loss: 3.4453\n",
      "[Epoch 60] r2_score.: -6.7568\n",
      "[Epoch 61] Loss: 3.5823\n",
      "[Epoch 61] r2_score.: -6.6763\n",
      "[Epoch 62] Loss: 3.6081\n",
      "[Epoch 62] r2_score.: -6.5282\n",
      "[Epoch 63] Loss: 3.5277\n",
      "[Epoch 63] r2_score.: -5.5919\n",
      "[Epoch 64] Loss: 3.6503\n",
      "[Epoch 64] r2_score.: -6.1779\n",
      "[Epoch 65] Loss: 3.6723\n",
      "[Epoch 65] r2_score.: -6.1484\n",
      "[Epoch 66] Loss: 3.6414\n",
      "[Epoch 66] r2_score.: -7.5081\n",
      "[Epoch 67] Loss: 3.4024\n",
      "[Epoch 67] r2_score.: -7.7352\n",
      "[Epoch 68] Loss: 3.3096\n",
      "[Epoch 68] r2_score.: -6.3509\n",
      "[Epoch 69] Loss: 3.4757\n",
      "[Epoch 69] r2_score.: -6.6860\n",
      "[Epoch 70] Loss: 3.3670\n",
      "[Epoch 70] r2_score.: -6.9805\n",
      "[Epoch 71] Loss: 3.3648\n",
      "[Epoch 71] r2_score.: -5.4861\n",
      "[Epoch 72] Loss: 3.5614\n",
      "[Epoch 72] r2_score.: -5.6985\n",
      "[Epoch 73] Loss: 3.5048\n",
      "[Epoch 73] r2_score.: -6.2272\n",
      "[Epoch 74] Loss: 3.4724\n",
      "[Epoch 74] r2_score.: -6.4810\n",
      "[Epoch 75] Loss: 3.5413\n",
      "[Epoch 75] r2_score.: -7.5408\n",
      "[Epoch 76] Loss: 3.3208\n",
      "[Epoch 76] r2_score.: -6.4422\n",
      "[Epoch 77] Loss: 3.2769\n",
      "[Epoch 77] r2_score.: -6.4613\n",
      "[Epoch 78] Loss: 3.3326\n",
      "[Epoch 78] r2_score.: -6.2351\n",
      "[Epoch 79] Loss: 3.4375\n",
      "[Epoch 79] r2_score.: -6.5392\n",
      "[Epoch 80] Loss: 3.1194\n",
      "[Epoch 80] r2_score.: -6.4977\n",
      "[Epoch 81] Loss: 3.0737\n",
      "[Epoch 81] r2_score.: -6.0718\n",
      "[Epoch 82] Loss: 3.2877\n",
      "[Epoch 82] r2_score.: -6.5980\n",
      "[Epoch 83] Loss: 3.2597\n",
      "[Epoch 83] r2_score.: -6.3262\n",
      "[Epoch 84] Loss: 3.1856\n",
      "[Epoch 84] r2_score.: -6.1407\n",
      "[Epoch 85] Loss: 3.1470\n",
      "[Epoch 85] r2_score.: -6.8169\n",
      "[Epoch 86] Loss: 3.2259\n",
      "[Epoch 86] r2_score.: -6.0892\n",
      "[Epoch 87] Loss: 3.1551\n",
      "[Epoch 87] r2_score.: -6.8944\n",
      "[Epoch 88] Loss: 3.2603\n",
      "[Epoch 88] r2_score.: -7.1591\n",
      "[Epoch 89] Loss: 3.1105\n",
      "[Epoch 89] r2_score.: -6.1643\n",
      "[Epoch 90] Loss: 2.8843\n",
      "[Epoch 90] r2_score.: -6.3656\n",
      "[Epoch 91] Loss: 2.9506\n",
      "[Epoch 91] r2_score.: -6.4518\n",
      "[Epoch 92] Loss: 2.9524\n",
      "[Epoch 92] r2_score.: -6.2136\n",
      "[Epoch 93] Loss: 2.8996\n",
      "[Epoch 93] r2_score.: -5.8870\n",
      "[Epoch 94] Loss: 3.0465\n",
      "[Epoch 94] r2_score.: -5.9658\n",
      "[Epoch 95] Loss: 2.7693\n",
      "[Epoch 95] r2_score.: -6.4604\n",
      "[Epoch 96] Loss: 2.9352\n",
      "[Epoch 96] r2_score.: -6.6833\n",
      "[Epoch 97] Loss: 3.0627\n",
      "[Epoch 97] r2_score.: -6.7540\n",
      "[Epoch 98] Loss: 2.8448\n",
      "[Epoch 98] r2_score.: -5.9030\n",
      "[Epoch 99] Loss: 2.8260\n",
      "[Epoch 99] r2_score.: -5.8105\n",
      "[Epoch 100] Loss: 2.7635\n",
      "[Epoch 100] r2_score.: -6.4100\n"
     ]
    }
   ],
   "source": [
    "# Create the training dataset and dataloader.\n",
    "train_dataset = MRIDataset(train=True)\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    shuffle=True\n",
    ")\n",
    "# Create the validation dataset and dataloader.\n",
    "valid_dataset = MRIDataset(train=False)\n",
    "valid_dataloader = DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=NUM_WORKERS\n",
    ")\n",
    "\n",
    "# Create the network.\n",
    "net = MLPTest()\n",
    "\n",
    "# Create the optimizer.\n",
    "optimizer = Adam(net.parameters(), lr=0.005)\n",
    "\n",
    "# Main training loop.\n",
    "best_accuracy = 0\n",
    "for epoch_idx in range(NUM_EPOCHS):\n",
    "    # Training code.\n",
    "    loss = run_training_epoch(net, optimizer, train_dataloader)\n",
    "    print('[Epoch %02d] Loss: %.4f' % (epoch_idx + 1, loss))\n",
    "\n",
    "    # Validation code.\n",
    "    acc = run_validation_epoch(net, valid_dataloader)\n",
    "    print('[Epoch %02d] r2_score.: %.4f' % (epoch_idx + 1, acc))\n",
    "\n",
    "    # Save checkpoint if accuracy is the best so far.\n",
    "    checkpoint = {\n",
    "        'epoch_idx': epoch_idx,\n",
    "        'net': net.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "    }\n",
    "    if acc > best_accuracy:\n",
    "        best_accuracy = acc\n",
    "        torch.save(checkpoint, f'best-{net.codename}.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hungarian-express",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

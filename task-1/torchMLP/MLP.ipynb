{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "suburban-concrete",
   "metadata": {},
   "source": [
    "This Pipeline is based on the computer vision lab where the functions were modified for the purpose of this lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "supposed-juice",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from scipy.stats import zscore\n",
    "from sklearn.metrics import r2_score\n",
    "#from sklearn.model_selection import train_test_split\n",
    "from numpy.random import RandomState\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "handmade-bumper",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED= 123456789\n",
    "BATCH_SIZE = 10\n",
    "NUM_WORKERS = 4\n",
    "NUM_EPOCHS = 100\n",
    "VALIDATION_INDEX = 750 #We will take the VALIDATION_INDEX firts samples as training and the remaining as validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "excess-aircraft",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updating mean class for loss aggregation.\n",
    "class UpdatingMean():\n",
    "    def __init__(self):\n",
    "        self.sum = 0\n",
    "        self.n = 0\n",
    "\n",
    "    def mean(self):\n",
    "        return self.sum / self.n\n",
    "\n",
    "    def add(self, loss):\n",
    "        self.sum += loss\n",
    "        self.n += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "powered-springfield",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MRIDataset(Dataset):\n",
    "    def __init__(self, train=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        X_train_test = pd.read_csv('data/X_train_cleaned.csv')\n",
    "        y_train_test = pd.read_csv('data/y_train_cleaned.csv')\n",
    "        \n",
    "        #Split in test and validation:\n",
    "        prng = RandomState(SEED)\n",
    "        train_mask = prng.rand(len(X_train_test)) < 0.8 #~80% of training and 20% of validation\n",
    "                \n",
    "        if(train):\n",
    "            self.samples = X_train_test[train_mask].to_numpy()\n",
    "            self.annotations= y_train_test[train_mask].to_numpy()\n",
    "        else:\n",
    "            self.samples = X_train_test[~train_mask].to_numpy()\n",
    "            self.annotations= y_train_test[~train_mask].to_numpy()\n",
    "                                        \n",
    "    def __len__(self):\n",
    "        # Returns the number of samples in the dataset.\n",
    "        return self.samples.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Returns the sample and annotation with index idx.\n",
    "        sample = self.samples[idx]\n",
    "        annotation = self.annotations[idx]\n",
    "                \n",
    "        # Convert to tensor.\n",
    "        return {\n",
    "            'input': torch.from_numpy(sample).float(),\n",
    "            'annotation': torch.from_numpy(annotation).float()\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "adjustable-missile",
   "metadata": {},
   "outputs": [],
   "source": [
    "def r2_loss(output, target):\n",
    "    target_mean = torch.mean(target)\n",
    "    ss_tot = torch.sum((target - target_mean) ** 2)\n",
    "    ss_res = torch.sum((target - output) ** 2)\n",
    "    r2 = 1 - ss_res / ss_tot if ss_tot != 0 else torch.zeros(1) #What to do when divided by zero?\n",
    "    return r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "systematic-superintendent",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training_epoch(net, optimizer, dataloader):\n",
    "    loss_aggregator = UpdatingMean()\n",
    "    # Put the network in training mode.\n",
    "    net.train()\n",
    "    # Loop over batches.\n",
    "    for batch in dataloader:\n",
    "        # Reset gradients.\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass.\n",
    "        X = batch[\"input\"]\n",
    "        y = batch[\"annotation\"]\n",
    "        output = net(X)\n",
    "\n",
    "        # Compute the loss - MAE\n",
    "        loss = nn.MSELoss()\n",
    "        output_loss = loss(output, y)\n",
    "\n",
    "        # Backwards pass.\n",
    "        output_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Save loss value in the aggregator.\n",
    "        loss_aggregator.add(output_loss)\n",
    "    return loss_aggregator.mean()\n",
    "\n",
    "\n",
    "def compute_accuracy(output, labels):\n",
    "    return r2_loss(output, labels)\n",
    "\n",
    "\n",
    "def run_validation_epoch(net, dataloader):\n",
    "    accuracy_aggregator = UpdatingMean()\n",
    "    # Put the network in evaluation mode.\n",
    "    net.eval()\n",
    "    # Loop over batches.\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        # Forward pass only.\n",
    "        output = net(batch['input'])\n",
    "\n",
    "        # Compute the accuracy using compute_accuracy.\n",
    "        accuracy = compute_accuracy(output.detach().numpy(), batch['annotation'])\n",
    "\n",
    "        # Save accuracy value in the aggregator.\n",
    "        accuracy_aggregator.add(accuracy.item())\n",
    "    return accuracy_aggregator.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "preliminary-shower",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPTest(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.codename = 'mlp'\n",
    "\n",
    "        # Define the network layers in order.\n",
    "        # Input is 199D.\n",
    "        # Output is a single value.\n",
    "        # Multiple linear layers each followed by a LeakyReLU non-linearity.\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(199, 4),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(4, 10),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(10, 1),\n",
    "            #nn.LeakyReLU(),\n",
    "            #nn.Linear(5, 1),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "    \n",
    "    def forward(self, batch):\n",
    "        # Process batch using the layers.\n",
    "        x = self.layers(batch)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "informational-penetration",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 01] Loss: 4771.1499\n",
      "[Epoch 01] r2_score.: -68.4152\n",
      "[Epoch 02] Loss: 3982.5259\n",
      "[Epoch 02] r2_score.: -47.7271\n",
      "[Epoch 03] Loss: 2578.4617\n",
      "[Epoch 03] r2_score.: -29.3501\n",
      "[Epoch 04] Loss: 2031.9648\n",
      "[Epoch 04] r2_score.: -26.1725\n",
      "[Epoch 05] Loss: 1865.7903\n",
      "[Epoch 05] r2_score.: -25.1557\n",
      "[Epoch 06] Loss: 1782.5873\n",
      "[Epoch 06] r2_score.: -24.4645\n",
      "[Epoch 07] Loss: 1747.7057\n",
      "[Epoch 07] r2_score.: -23.8998\n",
      "[Epoch 08] Loss: 1755.4540\n",
      "[Epoch 08] r2_score.: -23.3756\n",
      "[Epoch 09] Loss: 1648.3805\n",
      "[Epoch 09] r2_score.: -22.8410\n",
      "[Epoch 10] Loss: 1560.6420\n",
      "[Epoch 10] r2_score.: -22.1052\n",
      "[Epoch 11] Loss: 1496.1311\n",
      "[Epoch 11] r2_score.: -21.5625\n",
      "[Epoch 12] Loss: 1438.6906\n",
      "[Epoch 12] r2_score.: -20.8191\n",
      "[Epoch 13] Loss: 1392.0734\n",
      "[Epoch 13] r2_score.: -19.8008\n",
      "[Epoch 14] Loss: 1303.6826\n",
      "[Epoch 14] r2_score.: -18.2815\n",
      "[Epoch 15] Loss: 1210.5154\n",
      "[Epoch 15] r2_score.: -16.7962\n",
      "[Epoch 16] Loss: 1092.9053\n",
      "[Epoch 16] r2_score.: -15.0745\n",
      "[Epoch 17] Loss: 992.2244\n",
      "[Epoch 17] r2_score.: -13.6112\n",
      "[Epoch 18] Loss: 907.0212\n",
      "[Epoch 18] r2_score.: -12.7857\n",
      "[Epoch 19] Loss: 835.1978\n",
      "[Epoch 19] r2_score.: -11.6440\n",
      "[Epoch 20] Loss: 778.7461\n",
      "[Epoch 20] r2_score.: -10.8161\n",
      "[Epoch 21] Loss: 727.3785\n",
      "[Epoch 21] r2_score.: -10.4953\n",
      "[Epoch 22] Loss: 688.3226\n",
      "[Epoch 22] r2_score.: -10.0028\n",
      "[Epoch 23] Loss: 651.8980\n",
      "[Epoch 23] r2_score.: -9.4847\n",
      "[Epoch 24] Loss: 611.5111\n",
      "[Epoch 24] r2_score.: -9.3543\n",
      "[Epoch 25] Loss: 582.5140\n",
      "[Epoch 25] r2_score.: -8.6120\n",
      "[Epoch 26] Loss: 547.0587\n",
      "[Epoch 26] r2_score.: -8.1102\n",
      "[Epoch 27] Loss: 518.4721\n",
      "[Epoch 27] r2_score.: -7.7318\n",
      "[Epoch 28] Loss: 482.3761\n",
      "[Epoch 28] r2_score.: -7.3189\n",
      "[Epoch 29] Loss: 460.3778\n",
      "[Epoch 29] r2_score.: -6.9441\n",
      "[Epoch 30] Loss: 416.5680\n",
      "[Epoch 30] r2_score.: -6.4736\n",
      "[Epoch 31] Loss: 390.0941\n",
      "[Epoch 31] r2_score.: -6.0423\n",
      "[Epoch 32] Loss: 358.1850\n",
      "[Epoch 32] r2_score.: -5.4774\n",
      "[Epoch 33] Loss: 326.9623\n",
      "[Epoch 33] r2_score.: -5.1769\n",
      "[Epoch 34] Loss: 296.6298\n",
      "[Epoch 34] r2_score.: -4.7572\n",
      "[Epoch 35] Loss: 268.7200\n",
      "[Epoch 35] r2_score.: -4.3424\n",
      "[Epoch 36] Loss: 246.9850\n",
      "[Epoch 36] r2_score.: -3.9545\n",
      "[Epoch 37] Loss: 227.5588\n",
      "[Epoch 37] r2_score.: -3.7122\n",
      "[Epoch 38] Loss: 202.5351\n",
      "[Epoch 38] r2_score.: -3.0963\n",
      "[Epoch 39] Loss: 194.4944\n",
      "[Epoch 39] r2_score.: -2.7016\n",
      "[Epoch 40] Loss: 168.8640\n",
      "[Epoch 40] r2_score.: -2.2595\n",
      "[Epoch 41] Loss: 156.1200\n",
      "[Epoch 41] r2_score.: -2.0554\n",
      "[Epoch 42] Loss: 141.0960\n",
      "[Epoch 42] r2_score.: -1.7556\n",
      "[Epoch 43] Loss: 129.3568\n",
      "[Epoch 43] r2_score.: -1.6217\n",
      "[Epoch 44] Loss: 118.5163\n",
      "[Epoch 44] r2_score.: -1.3417\n",
      "[Epoch 45] Loss: 107.9418\n",
      "[Epoch 45] r2_score.: -1.0370\n",
      "[Epoch 46] Loss: 100.0374\n",
      "[Epoch 46] r2_score.: -0.8849\n",
      "[Epoch 47] Loss: 88.6151\n",
      "[Epoch 47] r2_score.: -0.9298\n",
      "[Epoch 48] Loss: 83.3042\n",
      "[Epoch 48] r2_score.: -0.6477\n",
      "[Epoch 49] Loss: 76.4845\n",
      "[Epoch 49] r2_score.: -0.4055\n",
      "[Epoch 50] Loss: 70.6669\n",
      "[Epoch 50] r2_score.: -0.3631\n",
      "[Epoch 51] Loss: 66.8774\n",
      "[Epoch 51] r2_score.: -0.3059\n",
      "[Epoch 52] Loss: 63.1756\n",
      "[Epoch 52] r2_score.: -0.1548\n",
      "[Epoch 53] Loss: 59.4577\n",
      "[Epoch 53] r2_score.: -0.0969\n",
      "[Epoch 54] Loss: 57.1789\n",
      "[Epoch 54] r2_score.: -0.0650\n",
      "[Epoch 55] Loss: 53.7190\n",
      "[Epoch 55] r2_score.: -0.0531\n",
      "[Epoch 56] Loss: 50.9563\n",
      "[Epoch 56] r2_score.: 0.0025\n",
      "[Epoch 57] Loss: 48.7883\n",
      "[Epoch 57] r2_score.: 0.0429\n",
      "[Epoch 58] Loss: 46.3601\n",
      "[Epoch 58] r2_score.: 0.0008\n",
      "[Epoch 59] Loss: 46.0127\n",
      "[Epoch 59] r2_score.: 0.0364\n",
      "[Epoch 60] Loss: 45.1836\n",
      "[Epoch 60] r2_score.: 0.0650\n",
      "[Epoch 61] Loss: 42.6307\n",
      "[Epoch 61] r2_score.: 0.0415\n",
      "[Epoch 62] Loss: 42.3878\n",
      "[Epoch 62] r2_score.: 0.0694\n",
      "[Epoch 63] Loss: 42.7026\n",
      "[Epoch 63] r2_score.: 0.0795\n",
      "[Epoch 64] Loss: 40.1599\n",
      "[Epoch 64] r2_score.: 0.1186\n",
      "[Epoch 65] Loss: 39.8961\n",
      "[Epoch 65] r2_score.: 0.1317\n",
      "[Epoch 66] Loss: 39.5996\n",
      "[Epoch 66] r2_score.: 0.0491\n",
      "[Epoch 67] Loss: 39.6954\n",
      "[Epoch 67] r2_score.: 0.1340\n",
      "[Epoch 68] Loss: 38.2050\n",
      "[Epoch 68] r2_score.: 0.1645\n",
      "[Epoch 69] Loss: 37.7265\n",
      "[Epoch 69] r2_score.: 0.1297\n",
      "[Epoch 70] Loss: 36.7783\n",
      "[Epoch 70] r2_score.: 0.1706\n",
      "[Epoch 71] Loss: 35.9368\n",
      "[Epoch 71] r2_score.: 0.1840\n",
      "[Epoch 72] Loss: 37.4414\n",
      "[Epoch 72] r2_score.: 0.1613\n",
      "[Epoch 73] Loss: 36.9674\n",
      "[Epoch 73] r2_score.: 0.2017\n",
      "[Epoch 74] Loss: 36.1565\n",
      "[Epoch 74] r2_score.: 0.2068\n",
      "[Epoch 75] Loss: 36.3209\n",
      "[Epoch 75] r2_score.: 0.1886\n",
      "[Epoch 76] Loss: 35.6468\n",
      "[Epoch 76] r2_score.: 0.1803\n",
      "[Epoch 77] Loss: 35.6352\n",
      "[Epoch 77] r2_score.: 0.1689\n",
      "[Epoch 78] Loss: 35.6561\n",
      "[Epoch 78] r2_score.: 0.1788\n",
      "[Epoch 79] Loss: 34.0818\n",
      "[Epoch 79] r2_score.: 0.2236\n",
      "[Epoch 80] Loss: 35.3176\n",
      "[Epoch 80] r2_score.: 0.2018\n",
      "[Epoch 81] Loss: 34.2444\n",
      "[Epoch 81] r2_score.: 0.1401\n",
      "[Epoch 82] Loss: 35.2662\n",
      "[Epoch 82] r2_score.: 0.2121\n",
      "[Epoch 83] Loss: 33.2084\n",
      "[Epoch 83] r2_score.: 0.2325\n",
      "[Epoch 84] Loss: 33.0043\n",
      "[Epoch 84] r2_score.: 0.1718\n",
      "[Epoch 85] Loss: 34.0591\n",
      "[Epoch 85] r2_score.: 0.1924\n",
      "[Epoch 86] Loss: 33.7881\n",
      "[Epoch 86] r2_score.: 0.2139\n",
      "[Epoch 87] Loss: 32.3566\n",
      "[Epoch 87] r2_score.: 0.1694\n",
      "[Epoch 88] Loss: 32.7699\n",
      "[Epoch 88] r2_score.: 0.1885\n",
      "[Epoch 89] Loss: 33.9167\n",
      "[Epoch 89] r2_score.: 0.2246\n",
      "[Epoch 90] Loss: 32.5553\n",
      "[Epoch 90] r2_score.: 0.2112\n",
      "[Epoch 91] Loss: 32.6826\n",
      "[Epoch 91] r2_score.: 0.1614\n",
      "[Epoch 92] Loss: 32.0021\n",
      "[Epoch 92] r2_score.: 0.1722\n",
      "[Epoch 93] Loss: 34.2544\n",
      "[Epoch 93] r2_score.: 0.2505\n",
      "[Epoch 94] Loss: 31.9089\n",
      "[Epoch 94] r2_score.: 0.2396\n",
      "[Epoch 95] Loss: 31.1498\n",
      "[Epoch 95] r2_score.: 0.2407\n",
      "[Epoch 96] Loss: 32.0240\n",
      "[Epoch 96] r2_score.: 0.2132\n",
      "[Epoch 97] Loss: 32.7155\n",
      "[Epoch 97] r2_score.: 0.1992\n",
      "[Epoch 98] Loss: 30.2555\n",
      "[Epoch 98] r2_score.: 0.2201\n",
      "[Epoch 99] Loss: 31.7106\n",
      "[Epoch 99] r2_score.: 0.2232\n",
      "[Epoch 100] Loss: 32.0835\n",
      "[Epoch 100] r2_score.: 0.1875\n"
     ]
    }
   ],
   "source": [
    "# Create the training dataset and dataloader.\n",
    "train_dataset = MRIDataset(train=True)\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    shuffle=True\n",
    ")\n",
    "# Create the validation dataset and dataloader.\n",
    "valid_dataset = MRIDataset(train=False)\n",
    "valid_dataloader = DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=NUM_WORKERS\n",
    ")\n",
    "\n",
    "# Create the network.\n",
    "net = MLPTest()\n",
    "\n",
    "# Create the optimizer., lr=0.005\n",
    "optimizer = Adam(net.parameters())\n",
    "\n",
    "# Main training loop.\n",
    "best_accuracy = 0\n",
    "for epoch_idx in range(NUM_EPOCHS):\n",
    "    # Training code.\n",
    "    loss = run_training_epoch(net, optimizer, train_dataloader)\n",
    "    print('[Epoch %02d] Loss: %.4f' % (epoch_idx + 1, loss))\n",
    "\n",
    "    # Validation code.\n",
    "    acc = run_validation_epoch(net, valid_dataloader)\n",
    "    print('[Epoch %02d] r2_score.: %.4f' % (epoch_idx + 1, acc))\n",
    "\n",
    "    # Save checkpoint if accuracy is the best so far.\n",
    "    checkpoint = {\n",
    "        'epoch_idx': epoch_idx,\n",
    "        'net': net.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "    }\n",
    "    if acc > best_accuracy:\n",
    "        best_accuracy = acc\n",
    "        torch.save(checkpoint, f'best-{net.codename}.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "hungarian-express",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.35426631645776574"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_test = pd.read_csv('data/X_train_cleaned.csv')\n",
    "y_train_test = pd.read_csv('data/y_train_cleaned.csv')\n",
    "\n",
    "prng = RandomState(SEED)\n",
    "train_mask = prng.rand(len(X_train_test)) < 0.8 #~80% of training and 20% of validation\n",
    "X_validation = X_train_test[~train_mask].to_numpy()\n",
    "y_validation = y_train_test[~train_mask].to_numpy()\n",
    "\n",
    "#output = net(torch.from_numpy(X_validation).float())\n",
    "output = torch.ceil(net(torch.from_numpy(X_validation).float())) #Gives better result cause our NN slightly underestimate our result\n",
    "r2_score(output.detach().numpy(),  y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "removable-award",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
